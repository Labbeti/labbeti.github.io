<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>Automated Audio Captioning state-of-the-art scores</title>
    <style>
      html, body {
        box-sizing: border-box;
        height: 100%;
        margin: 0;
        padding: 0;
      }
    </style>
    <script type="text/javascript" src="https://cdn.bokeh.org/bokeh/release/bokeh-3.1.1.min.js"></script>
    <script type="text/javascript">
        Bokeh.set_log_level("info");
    </script>
  </head>
  <body>
    <center>
    <h1>SPIDEr as the function of the number of parameters for the AudioCaps (AC) dataset.</h1>
    <div id="a3461d19-6c75-4588-b576-a78362f42f3b" data-root-id="p247860" style="display: contents;"></div>

    <script type="application/json" id="p262213">
      {"afc68e10-35eb-4963-9506-c561e93de049":{"version":"3.1.1","title":"Bokeh Application","defs":[],"roots":[{"type":"object","name":"Figure","id":"p247860","attributes":{"width":500,"height":500,"x_range":{"type":"object","name":"DataRange1d","id":"p247861","attributes":{"renderers":[{"type":"object","name":"GlyphRenderer","id":"p247915","attributes":{"data_source":{"type":"object","name":"ColumnDataSource","id":"p247906","attributes":{"selected":{"type":"object","name":"Selection","id":"p247907","attributes":{"indices":[],"line_indices":[]}},"selection_policy":{"type":"object","name":"UnionRenderers","id":"p247908"},"data":{"type":"map","entries":[["#",["2","2","3","6","8","9","11","12","13","15","20","21","31","32","33"]],["Paper name",["WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research","WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research","Rethinking Transfer and Auxiliary Learning for Improving Audio Captioning Transformer","Exploring Train and Test-Time Augmentations for Audio-Language Learning","Automated audio captioning by fine-tuning bart with audioset tags","AudioCaps: Generating Captions for Audios in The Wild","AUDIO CAPTIONING TRANSFORMER","IMPROVING AUDIO CAPTIONING USING SEMANTIC SIMILARITY METRICS","Visually-Aware Audio Captioning With Adaptive Audio-Visual Attention","Leveraging Pre-trained BERT for Audio Captioning","INVESTIGATING LOCAL AND GLOBAL INFORMATION FOR AUTOMATED AUDIO CAPTIONING WITH TRANSFER LEARNING","CONTINUAL LEARNING FOR AUTOMATED AUDIO CAPTIONING USING THE LEARNING WITHOUT FORGETTING APPROACH","WEAKLY-SUPERVISED AUTOMATED AUDIO CAPTIONING VIA TEXT ONLY TRAINING","TRAINING AUDIO CAPTIONING MODELS WITHOUT AUDIO","PREFIX TUNING FOR AUTOMATED AUDIO CAPTIONING"]],["Link",["https://arxiv.org/pdf/2303.17395.pdf","https://arxiv.org/pdf/2303.17395.pdf","https://www.isca-speech.org/archive/pdfs/interspeech_2023/shin23_interspeech.pdf","https://arxiv.org/pdf/2210.17143.pdf","https://inria.hal.science/hal-03522488/document","https://aclanthology.org/N19-1011.pdf","https://dcase.community/documents/workshop2021/proceedings/DCASE2021Workshop_Mei_68.pdf","https://arxiv.org/pdf/2210.16470.pdf","https://arxiv.org/pdf/2210.16428.pdf","https://arxiv.org/pdf/2203.02838.pdf","https://arxiv.org/pdf/2102.11457.pdf","https://arxiv.org/pdf/2107.08028v1.pdf","https://arxiv.org/pdf/2309.12242.pdf","https://arxiv.org/pdf/2309.07372.pdf","https://arxiv.org/pdf/2303.17489.pdf"]],["First Author",["Xinhao Mei","Xinhao Mei","Wooseok Shin","Eungbeom Kim","F\u00e9lix Gontier","Chris Dongjoo Kim","Xinhao Mei","Rehana Mahfuz","Xubo Liu","Xubo Liu","Xuenan Xu","Jan Berg","Theodoros Kouzelis","Soham Deshmukh","Minkyu Kim"]],["Nb params",[171.0,171.0,105.0,108.0,494.0,150.0,117.0,14.0,117.0,25.0,10.0,4.0,380.0,312.0,205.0]],["SPIDEr",[0.444,0.485,0.472,0.475,0.465,0.369,0.42,0.37,0.442,0.419,0.414,0.318,0.403,0.455,0.455]],["Train data",["AC","AC+WC","AC","AC","AC","AC","AC","AC","AC","AC","AC","AC","AC+WC","AC+WC","AC"]],["Test data",["AC","AC","AC","AC","AC","AC","AC","AC","AC","AC","AC","AC","AC","AC","AC"]],["Pretrained",["audio+text","audio+text","audio","audio","audio+text","audio","audio","audio","audio","audio+text","audio","none","audio","audio+text","audio+text"]],["Nb mod\u00e8les",[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]],["RL",[false,false,false,false,false,false,false,false,false,false,false,false,false,false,false]],["Year",["2023","2023","2023","2022","2021","2019","2021","2022","2023","2022","2021","2021","2023","2023","2023"]],["Archi",["trans-trans","trans-trans","trans-trans","trans-trans","cnn-trans","cnn-rnn","trans-trans","cnn-trans","trans-trans","cnn-trans","cnn-rnn","trans-trans","cnn-trans","trans-trans","cnn-trans"]],["Bibtex",["","","","","@inproceedings{Gontier2021, title = {Automated Audio Captioning by Fine-Tuning BART with AudioSet Tags}, author = {Gontier, Felix and Serizel, Romain and Cerisara, Christophe}, year = 2021, month = {November}, booktitle = {Proceedings of the 6th Detection and Classification of Acoustic Scenes and Events 2021 Workshop (DCASE2021)}, address = {Barcelona, Spain}, pages = {170--174}, isbn = {978-84-09-36072-7}, abstract = {Automated audio captioning is the multimodal task of describing environmental audio recordings with fluent natural language. Most current methods utilize pre-trained analysis models to extract relevant semantic content from the audio input. However, prior information on language modeling is rarely introduced, and corresponding architectures are limited in capacity due to data scarcity. In this paper, we present a method leveraging the linguistic information contained in BART, a large-scale conditional language model with general purpose pre-training. The caption generation is conditioned on sequences of textual AudioSet tags. This input is enriched with temporally aligned audio embeddings that allows the model to improve the sound event recognition. The full BART architecture is fine-tuned with few additional parameters. Experimental results demonstrate that, beyond the scaling properties of the architecture, language-only pre-training improves the text quality in the multimodal setting of audio captioning. The best model achieves state-of-the-art performance on AudioCaps with 46.5 SPIDEr.}, doi. = {10.5281/zenodo.5770113} }","","","","","","","","","",""]],["Optim",["Adam","Adam","Adam","AdamW","AdamW","Adam","Adam","?","Adam","Adam","Adam","Adam","Adam","Adam","AdamW"]],["ensemble",["Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single"]],["extra_train",[false,true,false,false,false,false,false,false,false,false,false,false,true,true,false]],["color",["#70b1ff","#ff7270","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#ff7270","#ff7270","#70b1ff"]],["marker",["circle","circle","diamond","diamond","circle","diamond","diamond","diamond","diamond","circle","diamond","triangle","diamond","circle","circle"]],["is_pareto",[false,true,true,true,false,false,false,false,false,true,true,true,false,false,false]],["pareto_idx",["none",1,2,3,"none","none","none","none","none",4,5,6,"none","none","none"]],["size",[13.076696830622021,13.076696830622021,10.246950765959598,10.392304845413264,22.22611077089287,12.24744871391589,10.816653826391969,3.7416573867739413,10.816653826391969,5.0,3.1622776601683795,2.0,19.493588689617926,17.663521732655695,14.317821063276353]],["params_display",["171M","171M","105M","108M","494M","150M","117M","14M","117M","25M","10M","4M","380M","312M","205M"]],["params_xoff",[-28.53834841531101,-28.53834841531101,-27.123475382979798,-27.196152422706632,-33.11305538544644,-28.123724356957943,-27.408326913195985,-23.870828693386972,-27.408326913195985,-24.5,-23.58113883008419,-23.0,-31.74679434480896,-30.831760866327848,-29.158910531638178]],["params_yoff",[-11.53834841531101,-11.53834841531101,-10.123475382979798,-10.196152422706632,-16.113055385446437,-11.123724356957945,-10.408326913195985,-6.87082869338697,-10.408326913195985,-7.5,-6.58113883008419,-6.0,-14.746794344808963,-13.831760866327848,-12.158910531638178]]]}}},"view":{"type":"object","name":"CDSView","id":"p247916","attributes":{"filter":{"type":"object","name":"AllIndices","id":"p247917"}}},"glyph":{"type":"object","name":"Scatter","id":"p247912","attributes":{"x":{"type":"field","field":"Nb params"},"y":{"type":"field","field":"SPIDEr"},"size":{"type":"value","value":18.0},"fill_color":{"type":"field","field":"color"},"hatch_color":{"type":"field","field":"color"},"marker":{"type":"field","field":"marker"}}},"nonselection_glyph":{"type":"object","name":"Scatter","id":"p247913","attributes":{"x":{"type":"field","field":"Nb params"},"y":{"type":"field","field":"SPIDEr"},"size":{"type":"value","value":18.0},"line_alpha":{"type":"value","value":0.1},"fill_color":{"type":"field","field":"color"},"fill_alpha":{"type":"value","value":0.1},"hatch_color":{"type":"field","field":"color"},"hatch_alpha":{"type":"value","value":0.1},"marker":{"type":"field","field":"marker"}}},"muted_glyph":{"type":"object","name":"Scatter","id":"p247914","attributes":{"x":{"type":"field","field":"Nb params"},"y":{"type":"field","field":"SPIDEr"},"size":{"type":"value","value":18.0},"line_alpha":{"type":"value","value":0.2},"fill_color":{"type":"field","field":"color"},"fill_alpha":{"type":"value","value":0.2},"hatch_color":{"type":"field","field":"color"},"hatch_alpha":{"type":"value","value":0.2},"marker":{"type":"field","field":"marker"}}}}}]}},"y_range":{"type":"object","name":"Range1d","id":"p247971","attributes":{"start":0.30528,"end":0.5044}},"x_scale":{"type":"object","name":"LogScale","id":"p247873"},"y_scale":{"type":"object","name":"LinearScale","id":"p247875"},"title":null,"outline_line_color":null,"renderers":[{"id":"p247915"},{"type":"object","name":"GlyphRenderer","id":"p247925","attributes":{"visible":false,"data_source":{"type":"object","name":"ColumnDataSource","id":"p247919","attributes":{"selected":{"type":"object","name":"Selection","id":"p247920","attributes":{"indices":[],"line_indices":[]}},"selection_policy":{"type":"object","name":"UnionRenderers","id":"p247921"},"data":{"type":"map","entries":[["x",[0]],["y",[0]]]}}},"view":{"type":"object","name":"CDSView","id":"p247926","attributes":{"filter":{"type":"object","name":"AllIndices","id":"p247927"}}},"glyph":{"type":"object","name":"Scatter","id":"p247922","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"fill_color":{"type":"value","value":"#ff7270"},"hatch_color":{"type":"value","value":"#ff7270"}}},"nonselection_glyph":{"type":"object","name":"Scatter","id":"p247923","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"line_alpha":{"type":"value","value":0.1},"fill_color":{"type":"value","value":"#ff7270"},"fill_alpha":{"type":"value","value":0.1},"hatch_color":{"type":"value","value":"#ff7270"},"hatch_alpha":{"type":"value","value":0.1}}},"muted_glyph":{"type":"object","name":"Scatter","id":"p247924","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"line_alpha":{"type":"value","value":0.2},"fill_color":{"type":"value","value":"#ff7270"},"fill_alpha":{"type":"value","value":0.2},"hatch_color":{"type":"value","value":"#ff7270"},"hatch_alpha":{"type":"value","value":0.2}}}}},{"type":"object","name":"GlyphRenderer","id":"p247935","attributes":{"visible":false,"data_source":{"type":"object","name":"ColumnDataSource","id":"p247929","attributes":{"selected":{"type":"object","name":"Selection","id":"p247930","attributes":{"indices":[],"line_indices":[]}},"selection_policy":{"type":"object","name":"UnionRenderers","id":"p247931"},"data":{"type":"map","entries":[["x",[0]],["y",[0]]]}}},"view":{"type":"object","name":"CDSView","id":"p247936","attributes":{"filter":{"type":"object","name":"AllIndices","id":"p247937"}}},"glyph":{"type":"object","name":"Scatter","id":"p247932","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"fill_color":{"type":"value","value":"#70b1ff"},"hatch_color":{"type":"value","value":"#70b1ff"}}},"nonselection_glyph":{"type":"object","name":"Scatter","id":"p247933","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"line_alpha":{"type":"value","value":0.1},"fill_color":{"type":"value","value":"#70b1ff"},"fill_alpha":{"type":"value","value":0.1},"hatch_color":{"type":"value","value":"#70b1ff"},"hatch_alpha":{"type":"value","value":0.1}}},"muted_glyph":{"type":"object","name":"Scatter","id":"p247934","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"line_alpha":{"type":"value","value":0.2},"fill_color":{"type":"value","value":"#70b1ff"},"fill_alpha":{"type":"value","value":0.2},"hatch_color":{"type":"value","value":"#70b1ff"},"hatch_alpha":{"type":"value","value":0.2}}}}},{"type":"object","name":"GlyphRenderer","id":"p247946","attributes":{"visible":false,"data_source":{"type":"object","name":"ColumnDataSource","id":"p247940","attributes":{"selected":{"type":"object","name":"Selection","id":"p247941","attributes":{"indices":[],"line_indices":[]}},"selection_policy":{"type":"object","name":"UnionRenderers","id":"p247942"},"data":{"type":"map","entries":[["x",[0]],["y",[0]]]}}},"view":{"type":"object","name":"CDSView","id":"p247947","attributes":{"filter":{"type":"object","name":"AllIndices","id":"p247948"}}},"glyph":{"type":"object","name":"Scatter","id":"p247943","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"fill_color":{"type":"value","value":"grey"},"hatch_color":{"type":"value","value":"grey"}}},"nonselection_glyph":{"type":"object","name":"Scatter","id":"p247944","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"line_alpha":{"type":"value","value":0.1},"fill_color":{"type":"value","value":"grey"},"fill_alpha":{"type":"value","value":0.1},"hatch_color":{"type":"value","value":"grey"},"hatch_alpha":{"type":"value","value":0.1}}},"muted_glyph":{"type":"object","name":"Scatter","id":"p247945","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"line_alpha":{"type":"value","value":0.2},"fill_color":{"type":"value","value":"grey"},"fill_alpha":{"type":"value","value":0.2},"hatch_color":{"type":"value","value":"grey"},"hatch_alpha":{"type":"value","value":0.2}}}}},{"type":"object","name":"GlyphRenderer","id":"p247956","attributes":{"visible":false,"data_source":{"type":"object","name":"ColumnDataSource","id":"p247950","attributes":{"selected":{"type":"object","name":"Selection","id":"p247951","attributes":{"indices":[],"line_indices":[]}},"selection_policy":{"type":"object","name":"UnionRenderers","id":"p247952"},"data":{"type":"map","entries":[["x",[0]],["y",[0]]]}}},"view":{"type":"object","name":"CDSView","id":"p247957","attributes":{"filter":{"type":"object","name":"AllIndices","id":"p247958"}}},"glyph":{"type":"object","name":"Scatter","id":"p247953","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"fill_color":{"type":"value","value":"grey"},"hatch_color":{"type":"value","value":"grey"},"marker":{"type":"value","value":"diamond"}}},"nonselection_glyph":{"type":"object","name":"Scatter","id":"p247954","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"line_alpha":{"type":"value","value":0.1},"fill_color":{"type":"value","value":"grey"},"fill_alpha":{"type":"value","value":0.1},"hatch_color":{"type":"value","value":"grey"},"hatch_alpha":{"type":"value","value":0.1},"marker":{"type":"value","value":"diamond"}}},"muted_glyph":{"type":"object","name":"Scatter","id":"p247955","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"line_alpha":{"type":"value","value":0.2},"fill_color":{"type":"value","value":"grey"},"fill_alpha":{"type":"value","value":0.2},"hatch_color":{"type":"value","value":"grey"},"hatch_alpha":{"type":"value","value":0.2},"marker":{"type":"value","value":"diamond"}}}}},{"type":"object","name":"GlyphRenderer","id":"p247966","attributes":{"visible":false,"data_source":{"type":"object","name":"ColumnDataSource","id":"p247960","attributes":{"selected":{"type":"object","name":"Selection","id":"p247961","attributes":{"indices":[],"line_indices":[]}},"selection_policy":{"type":"object","name":"UnionRenderers","id":"p247962"},"data":{"type":"map","entries":[["x",[0]],["y",[0]]]}}},"view":{"type":"object","name":"CDSView","id":"p247967","attributes":{"filter":{"type":"object","name":"AllIndices","id":"p247968"}}},"glyph":{"type":"object","name":"Scatter","id":"p247963","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"fill_color":{"type":"value","value":"grey"},"hatch_color":{"type":"value","value":"grey"},"marker":{"type":"value","value":"triangle"}}},"nonselection_glyph":{"type":"object","name":"Scatter","id":"p247964","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"line_alpha":{"type":"value","value":0.1},"fill_color":{"type":"value","value":"grey"},"fill_alpha":{"type":"value","value":0.1},"hatch_color":{"type":"value","value":"grey"},"hatch_alpha":{"type":"value","value":0.1},"marker":{"type":"value","value":"triangle"}}},"muted_glyph":{"type":"object","name":"Scatter","id":"p247965","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"line_alpha":{"type":"value","value":0.2},"fill_color":{"type":"value","value":"grey"},"fill_alpha":{"type":"value","value":0.2},"hatch_color":{"type":"value","value":"grey"},"hatch_alpha":{"type":"value","value":0.2},"marker":{"type":"value","value":"triangle"}}}}}],"toolbar":{"type":"object","name":"Toolbar","id":"p247866","attributes":{"tools":[{"type":"object","name":"PanTool","id":"p247891"},{"type":"object","name":"WheelZoomTool","id":"p247892"},{"type":"object","name":"BoxZoomTool","id":"p247893","attributes":{"overlay":{"type":"object","name":"BoxAnnotation","id":"p247894","attributes":{"syncable":false,"level":"overlay","visible":false,"left_units":"canvas","right_units":"canvas","bottom_units":"canvas","top_units":"canvas","line_color":"black","line_alpha":1.0,"line_width":2,"line_dash":[4,4],"fill_color":"lightgrey","fill_alpha":0.5}}}},{"type":"object","name":"SaveTool","id":"p247895"},{"type":"object","name":"ResetTool","id":"p247896"},{"type":"object","name":"HelpTool","id":"p247897"},{"type":"object","name":"HoverTool","id":"p243513","attributes":{"renderers":"auto","tooltips":[["Title","@{Paper name}"],["Nb params","@{Nb params}"],["SPIDEr","@{SPIDEr}"],["Train data","@{Train data}"],["Pretrained","@{Pretrained}"]]}}]}},"left":[{"type":"object","name":"LinearAxis","id":"p247884","attributes":{"ticker":{"type":"object","name":"BasicTicker","id":"p247887","attributes":{"mantissas":[1,2,5]}},"formatter":{"type":"object","name":"BasicTickFormatter","id":"p247886"},"axis_label":"SPIDEr","axis_label_text_font_size":"20px","major_label_policy":{"type":"object","name":"AllLabels","id":"p247885"},"major_label_text_font_size":"20px"}}],"below":[{"type":"object","name":"LogAxis","id":"p247877","attributes":{"ticker":{"type":"object","name":"LogTicker","id":"p247880","attributes":{"num_minor_ticks":10,"mantissas":[1,5]}},"formatter":{"type":"object","name":"LogTickFormatter","id":"p247879"},"axis_label":"Parameters (M)","axis_label_text_font_size":"20px","major_label_policy":{"type":"object","name":"AllLabels","id":"p247878"},"major_label_text_font_size":"20px"}}],"center":[{"type":"object","name":"Grid","id":"p247883","attributes":{"visible":false,"axis":{"id":"p247877"}}},{"type":"object","name":"Grid","id":"p247890","attributes":{"visible":false,"dimension":1,"axis":{"id":"p247884"}}},{"type":"object","name":"Legend","id":"p247939","attributes":{"location":[285,115],"title":"External data","title_text_font_size":"14px","label_text_font_size":"14px","padding":5,"items":[{"type":"object","name":"LegendItem","id":"p247928","attributes":{"label":{"type":"value","value":"yes"},"renderers":[{"id":"p247925"}],"index":1}},{"type":"object","name":"LegendItem","id":"p247938","attributes":{"label":{"type":"value","value":"no"},"renderers":[{"id":"p247935"}],"index":0}}]}},{"type":"object","name":"Legend","id":"p247970","attributes":{"location":"bottom_right","title":"Pretrained","title_text_font_size":"14px","label_text_font_size":"14px","padding":5,"items":[{"type":"object","name":"LegendItem","id":"p247949","attributes":{"label":{"type":"value","value":"audio+text"},"renderers":[{"id":"p247946"}],"index":0}},{"type":"object","name":"LegendItem","id":"p247959","attributes":{"label":{"type":"value","value":"audio"},"renderers":[{"id":"p247956"}],"index":2}},{"type":"object","name":"LegendItem","id":"p247969","attributes":{"label":{"type":"value","value":"none"},"renderers":[{"id":"p247966"}],"index":11}}]}}],"background_fill_color":null,"border_fill_color":null,"output_backend":"svg"}}],"callbacks":{"type":"map"}}}
    </script>
    <script type="text/javascript">
      (function() {
        const fn = function() {
          Bokeh.safely(function() {
            (function(root) {
              function embed_document(root) {
              const docs_json = document.getElementById('p262213').textContent;
              const render_items = [{"docid":"afc68e10-35eb-4963-9506-c561e93de049","roots":{"p247860":"a3461d19-6c75-4588-b576-a78362f42f3b"},"root_ids":["p247860"]}];
              root.Bokeh.embed.embed_items(docs_json, render_items);
              }
              if (root.Bokeh !== undefined) {
                embed_document(root);
              } else {
                let attempts = 0;
                const timer = setInterval(function(root) {
                  if (root.Bokeh !== undefined) {
                    clearInterval(timer);
                    embed_document(root);
                  } else {
                    attempts++;
                    if (attempts > 100) {
                      clearInterval(timer);
                      console.log("Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing");
                    }
                  }
                }, 10, root)
              }
            })(window);
          });
        };
        if (document.readyState != "loading") fn();
        else document.addEventListener("DOMContentLoaded", fn);
      })();
    </script>

    <h1>SPIDEr as the function of the number of parameters for the Clotho (CL) dataset.</h1>
    <div id="b400c5b8-5d80-489b-89ba-a1622a2526d8" data-root-id="p243517" style="display: contents;"></div>
  
    <script type="application/json" id="p261755">
      {"cd6f2a4c-bcfe-415b-bc7f-e2e6ab59de8b":{"version":"3.1.1","title":"Bokeh Application","defs":[],"roots":[{"type":"object","name":"Figure","id":"p243517","attributes":{"width":500,"height":500,"x_range":{"type":"object","name":"DataRange1d","id":"p243518","attributes":{"renderers":[{"type":"object","name":"GlyphRenderer","id":"p243572","attributes":{"data_source":{"type":"object","name":"ColumnDataSource","id":"p243563","attributes":{"selected":{"type":"object","name":"Selection","id":"p243564","attributes":{"indices":[],"line_indices":[]}},"selection_policy":{"type":"object","name":"UnionRenderers","id":"p243565"},"data":{"type":"map","entries":[["#",["1","1","2","2","5","10","10","12","14","18","20","22","23","28","29","31","32","33","35","36","36","39","40","41","43","44","44","44","46","46","46","46"]],["Paper name",["BEATS-BASED AUDIO CAPTIONING MODEL WITH INSTRUCTOR EMBEDDING SUPERVISION AND CHATGPT MIX-UP","BEATS-BASED AUDIO CAPTIONING MODEL WITH INSTRUCTOR EMBEDDING SUPERVISION AND CHATGPT MIX-UP","WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research","WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research","CAU SUBMISSION TO DCASE 2021 TASK6: TRANSFORMER FOLLOWED BY TRANSFER LEARNING FOR AUDIO CAPTIONING","THE NTT DCASE2020 CHALLENGE TASK 6 SYSTEM: AUTOMATED AUDIO CAPTIONING WITH KEYWORDS AND SENTENCE LENGTH ESTIMATION","THE NTT DCASE2020 CHALLENGE TASK 6 SYSTEM: AUTOMATED AUDIO CAPTIONING WITH KEYWORDS AND SENTENCE LENGTH ESTIMATION","IMPROVING AUDIO CAPTIONING USING SEMANTIC SIMILARITY METRICS","AUTOMATED AUDIO CAPTIONING WITH KEYWORDS GUIDANCE","EFFICIENT AUDIO CAPTIONING TRANSFORMER WITH PATCHOUT AND TEXT GUIDANCE","INVESTIGATING LOCAL AND GLOBAL INFORMATION FOR AUTOMATED AUDIO CAPTIONING WITH TRANSFER LEARNING","WaveTransformer: A Novel Architecture for Audio Captioning Based on Learning Temporal and Time-Frequency Information","Automated Audio Captioning With Topic Modeling","AUTOMATED AUDIO CAPTIONING USING TRANSFER LEARNING AND RECONSTRUCTION LATENT SPACE SIMILARITY REGULARIZATION","IMPROVING THE PERFORMANCE OF AUTOMATED AUDIO CAPTIONING VIA INTEGRATING THE ACOUSTIC AND SEMANTIC INFORMATION","WEAKLY-SUPERVISED AUTOMATED AUDIO CAPTIONING VIA TEXT ONLY TRAINING","TRAINING AUDIO CAPTIONING MODELS WITHOUT AUDIO","PREFIX TUNING FOR AUTOMATED AUDIO CAPTIONING","AUTOMATED AUDIO CAPTIONING WITH MULTI-TASK LEARNING","CP-JKU\u2019S SUBMISSION TO TASK 6A OF THE DCASE2022 CHALLENGE: A BART ENCODER-DECODER FOR AUTOMATIC AUDIO CAPTIONING TRAINED VIA THE REINFORCE ALGORITHM AND TRANSFER LEARNING","CP-JKU\u2019S SUBMISSION TO TASK 6A OF THE DCASE2022 CHALLENGE: A BART ENCODER-DECODER FOR AUTOMATIC AUDIO CAPTIONING TRAINED VIA THE REINFORCE ALGORITHM AND TRANSFER LEARNING","PEACS: PREFIX ENCODING FOR AUDITORY CAPTION SYNTHESIS","AUDIO CAPTIONING BASED ON TRANSFORMER AND PRE-TRAINING FOR 2020 DCASE AUDIO CAPTIONING CHALLENGE","AUTOMATED AUDIO CAPTIONING WITH TEMPORAL ATTENTION","THE DCASE2021 CHALLENGE TASK 6 SYSTEM : AUTOMATED AUDIO CAPTION","A WHISPER TRANSFORMER FOR AUDIO CAPTIONING TRAINED WITH SYNTHETIC CAPTIONS AND TRANSFER LEARNING","A WHISPER TRANSFORMER FOR AUDIO CAPTIONING TRAINED WITH SYNTHETIC CAPTIONS AND TRANSFER LEARNING","A WHISPER TRANSFORMER FOR AUDIO CAPTIONING TRAINED WITH SYNTHETIC CAPTIONS AND TRANSFER LEARNING","EXPLORING AUDIO CAPTIONING WITH KEYWORD-GUIDED TEXT GENERATION","EXPLORING AUDIO CAPTIONING WITH KEYWORD-GUIDED TEXT GENERATION","EXPLORING AUDIO CAPTIONING WITH KEYWORD-GUIDED TEXT GENERATION","EXPLORING AUDIO CAPTIONING WITH KEYWORD-GUIDED TEXT GENERATION"]],["Link",["https://dcase.community/documents/challenge2023/technical_reports/DCASE2023_Wu_31_t6a.pdf","https://dcase.community/documents/challenge2023/technical_reports/DCASE2023_Wu_31_t6a.pdf","https://arxiv.org/pdf/2303.17395.pdf","https://arxiv.org/pdf/2303.17395.pdf","https://dcase.community/documents/challenge2021/technical_reports/DCASE2021_Won_103_t6.pdf","https://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Koizumi_63_t6.pdf","https://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Koizumi_63_t6.pdf","https://arxiv.org/pdf/2210.16470.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Mei_117_t6a.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Kouzelis_60_t6a.pdf","https://arxiv.org/pdf/2102.11457.pdf","https://arxiv.org/pdf/2010.11098.pdf","https://www.researchgate.net/publication/367063417_Automated_Audio_Captioning_with_Topic_Modeling/fulltext/63c003f33fcb6855ce7dc192/Automated-Audio-Captioning-With-Topic-Modeling.pdf?origin=publicationDetail&amp;_sg%5B0%5D=dkU_nXJBP2xbhG7xq1Kpptu2LS2Uiq-Fo5RM6TaN7kL5JHW8WO8EUF0hcE4lf-4L7ZQKXRdEVd61EQPaMhGQtw.S1AXevlPqorLIQ_q4bs-6w49K76AG2QOJO7tgbPDXZqZzD4m4H_wzSHQ8AcgNnJngfn4funEYEqMGo4AnRtz2w&amp;_sg%5B1%5D=reAe0vy9g4yvBWpyUKSyvVlseW88gNcN8Dx8LZgBRXPgb8dervQNWpA1cZAZNCUzRlKWai2NkbcrTfj76stgeMdXhgB76EPmstMSZGyRzigi.S1AXevlPqorLIQ_q4bs-6w49K76AG2QOJO7tgbPDXZqZzD4m4H_wzSHQ8AcgNnJngfn4funEYEqMGo4AnRtz2w&amp;_iepl=&amp;_rtd=eyJjb250ZW50SW50ZW50IjoibWFpbkl0ZW0ifQ%3D%3D&amp;_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6Il9kaXJlY3QiLCJwYWdlIjoiX2RpcmVjdCIsInBvc2l0aW9uIjoicGFnZUhlYWRlciJ9fQ","https://arxiv.org/pdf/2108.04692.pdf","https://arxiv.org/pdf/2110.06100.pdf","https://arxiv.org/pdf/2309.12242.pdf","https://arxiv.org/pdf/2309.07372.pdf","https://arxiv.org/pdf/2303.17489.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Zou_37_t6a.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Primus_97_t6a.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Primus_97_t6a.pdf","https://dcase.community/documents/challenge2023/technical_reports/DCASE2023_Schaumloeffel_107_t6a.pdf","https://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Wu_136_t6.pdf","https://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Wang_5_t6.pdf","https://dcase.community/documents/challenge2021/technical_reports/DCASE2021_Yang_76_t6.pdf","https://dcase.community/documents/challenge2023/technical_reports/DCASE2023_Kadlcik_68_t6a.pdf","https://dcase.community/documents/challenge2023/technical_reports/DCASE2023_Kadlcik_68_t6a.pdf","https://dcase.community/documents/challenge2023/technical_reports/DCASE2023_Kadlcik_68_t6a.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Kicinski_115_t6a.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Kicinski_115_t6a.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Kicinski_115_t6a.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Kicinski_115_t6a.pdf"]],["First Author",["Shih-Lun Wu","Shih-Lun Wu","Xinhao Mei","Xinhao Mei","Hyejin Won","Yuma Koizumi","Yuma Koizumi","Rehana Mahfuz","Xinhao Mei","Thodoris Kouzelis","Xuenan Xu","An Tran","Xinhao Mei","Andrew Koh","Zhongjie ye","Theodoros Kouzelis","Soham Deshmukh","Minkyu Kim","Zhongjie Ye","Paul Primus","Paul Primus","Timothy Schauml\u00a8offel","Yusong Wu","Helin Wang","Liu Yang","Marek Kadlc\u0131k","Marek Kadlc\u0131k","Marek Kadlc\u0131k","Dawid Kici\u00b4nski","Dawid Kici\u00b4nski","Dawid Kici\u00b4nski","Dawid Kici\u00b4nski"]],["Nb params",[1600.0,3500.0,220.0,220.0,88.0,82.5,1.0,14.0,8.0,560.0,10.0,4.0,222.0,10.0,80.0,380.0,312.0,205.0,86.0,130.0,130.0,248.0,8.0,12.0,3.0,39.0,244.0,1000.0,104.0,207.0,104.0,207.0]],["SPIDEr",[0.326,0.336,0.255,0.31,0.285,0.207,0.196,0.208,0.266,0.296,0.246,0.182,0.264,0.246,0.269,0.247,0.261,0.255,0.286,0.264,0.284,0.292,0.227,0.172,0.166,0.224,0.269,0.279,0.279,0.255,0.26,0.249]],["Train data",["AC+CL","AC+CL","CL","CL+WC","CL","CL","CL","CL","CL","AC+CL+MA","CL","CL","CL","CL","CL","CL+WC","CL+WC","CL","CL","CL","AC+CL","AC+CL+MA+SD+WT","CL","CL","CL","AC+AS+CL","AC+AS+CL","AC+AS+CL","AC+CL+FSD","AC+CL+FSD","AC+CL+FSD","AC+CL+FSD"]],["Test data",["CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL"]],["Pretrained",["audio+text","audio+text","audio+text","audio+text","audio","none","none","audio","audio","audio","audio","none","audio+text","audio","audio","audio","audio+text","audio+text","audio","audio+text","audio+text","audio+text","none","none","none","audio+text","audio+text","audio+text","audio","audio+text","audio","audio+text"]],["Nb mod\u00e8les",[1,20,1,1,1,50,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]],["RL",[false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false]],["Year",["2023","2023","2023","2023","2021","2020","2020","2022","2022","2022","2021","2020","2023","2021","2021","2023","2023","2023","2022","2022","2022","2023","2020","2020","2021","2023","2023","2023","2022","2022","2022","2022"]],["Archi",["trans-trans","trans-trans","cnn-trans","cnn-trans","cnn-trans","cnn-rnn","cnn-rnn","cnn-trans","cnn-trans","trans-trans","cnn-rnn","trans-trans","cnn-trans","cnn-trans","cnn-rnn","cnn-trans","trans-trans","cnn-trans","cnn-rnn","cnn-trans","cnn-trans","cnn-trans","cnn-trans","cnn-rnn","cnn-trans","trans-trans","trans-trans","trans-trans","cnn-trans","cnn-trans","cnn-trans","cnn-trans"]],["Bibtex",["","","","","","@techreport{koizumi2020_t1, title = {The {NTT} {DCASE2020} Challenge Task 6 System: Automated Audio Captioning With Keywords and Sentence Length Estimation}, author = {Koizumi, Yuma and Takeuchi, Daiki and Ohishi, Yasunori and Harada, Noboru and Kashino, Kunio}, year = 2020, month = {June}, institution = {DCASE2020 Challenge}, abstract = {This technical report describes the system participating to the Detection and Classification of Acoustic Scenes and Events (DCASE) 2020 Challenge, Task 6: automated audio captioning. Our submission focuses on solving two indeterminacy problems in automated audio captioning: word selection indeterminacy and sentence length indeterminacy. We simultaneously solve the main caption generation and sub indeterminacy problems by estimating keywords and sentence length through multi-task learning. We tested a simplified model of our submission using the development-testing dataset. Our model achieved 20.7 SPIDEr score where that of the baseline system was 5.4.} }","@techreport{koizumi2020_t1, title = {The {NTT} {DCASE2020} Challenge Task 6 System: Automated Audio Captioning With Keywords and Sentence Length Estimation}, author = {Koizumi, Yuma and Takeuchi, Daiki and Ohishi, Yasunori and Harada, Noboru and Kashino, Kunio}, year = 2020, month = {June}, institution = {DCASE2020 Challenge}, abstract = {This technical report describes the system participating to the Detection and Classification of Acoustic Scenes and Events (DCASE) 2020 Challenge, Task 6: automated audio captioning. Our submission focuses on solving two indeterminacy problems in automated audio captioning: word selection indeterminacy and sentence length indeterminacy. We simultaneously solve the main caption generation and sub indeterminacy problems by estimating keywords and sentence length through multi-task learning. We tested a simplified model of our submission using the development-testing dataset. Our model achieved 20.7 SPIDEr score where that of the baseline system was 5.4.} }","","","","","","","","","","","","","","","@techreport{schaumloeffel2023_t6a, title = {PEACS: Prefix encoding for auditory caption synthesis}, author = {Schauml\u00f6ffel, Timothy and Vilas, Martina G. and Roig, Gemma}, year = 2023, month = {May}, institution = {DCASE2023 Challenge}, abstract = {This technical report describes an Automated Audio Captioning system for the Detection and Classification of Acoustic Scenes and Events (DCASE) 2023 Challenge, Task 6a (automated audio captioning). Our approach employs an encoder-decoder architecture, with the encoder utilizing a large contrastive pre-trained HTS-AT capable of handling variable-length audio segments. The decoder is based on the GPT2 model. To incorporate audio into the decoding process, we employ a light mapping network that translates audio representations into a prefix, effectively guiding the decoder\u2019s generation process. Given the limited data availability, we pre-train our model on various audio captioning datasets and fine-tune it on Clotho. We reach a SPIDERr-FL score of 29.3 on the evaluation split of the Clotho-v2 dataset.} }","","","","","","","@techreport{kicinski2022_t6a, title = {Exploring audio captioning with keyword-guided text generation}, author = {Kicinski, Dawid and de Gail, Teodor Lamort and Bujnowski, Pawel}, year = 2022, month = {July}, institution = {DCASE2022 Challenge}, abstract = {This technical report describes our submission to the DCASE 2022 challenge, Task 6 A: automated audio captioning. In our system, we explore the use of pre-trained language models for the audio captioning task. The proposed system is an encoder-decoder architecture consisting of a pre-trained PANN encoder and a GPT2 decoder. Audio embeddings are encoded to language model prompts using a simple mapping network. We further develop our system by employing strategies of guiding the decoder with textual information. We prompt the decoder with keywords extracted from semantically similar audios and use them to choose the best matching caption by their occurrence.} }","@techreport{kicinski2022_t6a, title = {Exploring audio captioning with keyword-guided text generation}, author = {Kicinski, Dawid and de Gail, Teodor Lamort and Bujnowski, Pawel}, year = 2022, month = {July}, institution = {DCASE2022 Challenge}, abstract = {This technical report describes our submission to the DCASE 2022 challenge, Task 6 A: automated audio captioning. In our system, we explore the use of pre-trained language models for the audio captioning task. The proposed system is an encoder-decoder architecture consisting of a pre-trained PANN encoder and a GPT2 decoder. Audio embeddings are encoded to language model prompts using a simple mapping network. We further develop our system by employing strategies of guiding the decoder with textual information. We prompt the decoder with keywords extracted from semantically similar audios and use them to choose the best matching caption by their occurrence.} }","@techreport{kicinski2022_t6a, title = {Exploring audio captioning with keyword-guided text generation}, author = {Kicinski, Dawid and de Gail, Teodor Lamort and Bujnowski, Pawel}, year = 2022, month = {July}, institution = {DCASE2022 Challenge}, abstract = {This technical report describes our submission to the DCASE 2022 challenge, Task 6 A: automated audio captioning. In our system, we explore the use of pre-trained language models for the audio captioning task. The proposed system is an encoder-decoder architecture consisting of a pre-trained PANN encoder and a GPT2 decoder. Audio embeddings are encoded to language model prompts using a simple mapping network. We further develop our system by employing strategies of guiding the decoder with textual information. We prompt the decoder with keywords extracted from semantically similar audios and use them to choose the best matching caption by their occurrence.} }","@techreport{kicinski2022_t6a, title = {Exploring audio captioning with keyword-guided text generation}, author = {Kicinski, Dawid and de Gail, Teodor Lamort and Bujnowski, Pawel}, year = 2022, month = {July}, institution = {DCASE2022 Challenge}, abstract = {This technical report describes our submission to the DCASE 2022 challenge, Task 6 A: automated audio captioning. In our system, we explore the use of pre-trained language models for the audio captioning task. The proposed system is an encoder-decoder architecture consisting of a pre-trained PANN encoder and a GPT2 decoder. Audio embeddings are encoded to language model prompts using a simple mapping network. We further develop our system by employing strategies of guiding the decoder with textual information. We prompt the decoder with keywords extracted from semantically similar audios and use them to choose the best matching caption by their occurrence.} }"]],["Optim",["AdamW","AdamW","Adam","Adam","Adam","AdamW","AdamW","?","?","?","Adam","Adam","AdamW","?","Adam","Adam","Adam","AdamW","Adam","Adam","Adam","AdamW","?","Adam","Adam","?","?","?","AdamW","AdamW","AdamW","AdamW"]],["ensemble",["Single","Ensemble","Single","Single","Single","Ensemble","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single"]],["extra_train",[true,true,false,true,false,false,false,false,false,true,false,false,false,false,false,true,true,false,false,false,true,true,false,false,false,true,true,true,true,true,true,true]],["color",["#ff7270","#ff7270","#70b1ff","#ff7270","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#ff7270","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#ff7270","#ff7270","#70b1ff","#70b1ff","#70b1ff","#ff7270","#ff7270","#70b1ff","#70b1ff","#70b1ff","#ff7270","#ff7270","#ff7270","#ff7270","#ff7270","#ff7270","#ff7270"]],["marker",["circle","circle","circle","circle","diamond","triangle","triangle","diamond","diamond","diamond","diamond","triangle","circle","diamond","diamond","diamond","circle","circle","diamond","circle","circle","circle","triangle","triangle","triangle","circle","circle","circle","diamond","circle","diamond","circle"]],["is_pareto",[true,true,false,true,false,false,true,false,true,false,false,false,false,false,true,false,false,false,true,false,false,false,false,false,false,false,false,false,false,false,false,false]],["pareto_idx",[1,1,"none",2,"none","none",3,"none",4,"none","none","none","none","none",5,"none","none","none",6,"none","none","none","none","none","none","none","none","none","none","none","none","none"]],["size",[40.0,59.16079783099616,14.832396974191326,14.832396974191326,9.38083151964686,9.082951062292475,1.0,3.7416573867739413,2.8284271247461903,23.664319132398465,3.1622776601683795,2.0,14.89966442575134,3.1622776601683795,8.94427190999916,19.493588689617926,17.663521732655695,14.317821063276353,9.273618495495704,11.40175425099138,11.40175425099138,15.748015748023622,2.8284271247461903,3.4641016151377544,1.7320508075688772,6.244997998398398,15.620499351813308,31.622776601683793,10.198039027185569,14.38749456993816,10.198039027185569,14.38749456993816]],["params_display",["1600M","3500M","220M","220M","88M","82M","1M","14M","8M","560M","10M","4M","222M","10M","80M","380M","312M","205M","86M","130M","130M","248M","8M","12M","3M","39M","244M","1000M","104M","207M","104M","207M"]],["params_xoff",[-42.0,-51.58039891549808,-29.416198487095663,-29.416198487095663,-26.69041575982343,-26.541475531146236,-22.5,-23.870828693386972,-23.414213562373096,-33.83215956619923,-23.58113883008419,-23.0,-29.44983221287567,-23.58113883008419,-26.47213595499958,-31.74679434480896,-30.831760866327848,-29.158910531638178,-26.636809247747852,-27.70087712549569,-27.70087712549569,-29.87400787401181,-23.414213562373096,-23.73205080756888,-22.866025403784437,-25.1224989991992,-29.810249675906654,-37.8113883008419,-27.099019513592786,-29.193747284969078,-27.099019513592786,-29.193747284969078]],["params_yoff",[-25.0,-34.58039891549808,-12.416198487095663,-12.416198487095663,-9.69041575982343,-9.541475531146236,-5.5,-6.87082869338697,-6.414213562373095,-16.83215956619923,-6.58113883008419,-6.0,-12.44983221287567,-6.58113883008419,-9.47213595499958,-14.746794344808963,-13.831760866327848,-12.158910531638178,-9.636809247747852,-10.70087712549569,-10.70087712549569,-12.874007874011811,-6.414213562373095,-6.732050807568877,-5.866025403784438,-8.122498999199198,-12.810249675906654,-20.811388300841898,-10.099019513592784,-12.19374728496908,-10.099019513592784,-12.19374728496908]]]}}},"view":{"type":"object","name":"CDSView","id":"p243573","attributes":{"filter":{"type":"object","name":"AllIndices","id":"p243574"}}},"glyph":{"type":"object","name":"Scatter","id":"p243569","attributes":{"x":{"type":"field","field":"Nb params"},"y":{"type":"field","field":"SPIDEr"},"size":{"type":"value","value":18.0},"fill_color":{"type":"field","field":"color"},"hatch_color":{"type":"field","field":"color"},"marker":{"type":"field","field":"marker"}}},"nonselection_glyph":{"type":"object","name":"Scatter","id":"p243570","attributes":{"x":{"type":"field","field":"Nb params"},"y":{"type":"field","field":"SPIDEr"},"size":{"type":"value","value":18.0},"line_alpha":{"type":"value","value":0.1},"fill_color":{"type":"field","field":"color"},"fill_alpha":{"type":"value","value":0.1},"hatch_color":{"type":"field","field":"color"},"hatch_alpha":{"type":"value","value":0.1},"marker":{"type":"field","field":"marker"}}},"muted_glyph":{"type":"object","name":"Scatter","id":"p243571","attributes":{"x":{"type":"field","field":"Nb params"},"y":{"type":"field","field":"SPIDEr"},"size":{"type":"value","value":18.0},"line_alpha":{"type":"value","value":0.2},"fill_color":{"type":"field","field":"color"},"fill_alpha":{"type":"value","value":0.2},"hatch_color":{"type":"field","field":"color"},"hatch_alpha":{"type":"value","value":0.2},"marker":{"type":"field","field":"marker"}}}}}]}},"y_range":{"type":"object","name":"Range1d","id":"p243628","attributes":{"start":0.15604,"end":0.35616000000000003}},"x_scale":{"type":"object","name":"LogScale","id":"p243530"},"y_scale":{"type":"object","name":"LinearScale","id":"p243532"},"title":null,"outline_line_color":null,"renderers":[{"id":"p243572"},{"type":"object","name":"GlyphRenderer","id":"p243582","attributes":{"visible":false,"data_source":{"type":"object","name":"ColumnDataSource","id":"p243576","attributes":{"selected":{"type":"object","name":"Selection","id":"p243577","attributes":{"indices":[],"line_indices":[]}},"selection_policy":{"type":"object","name":"UnionRenderers","id":"p243578"},"data":{"type":"map","entries":[["x",[0]],["y",[0]]]}}},"view":{"type":"object","name":"CDSView","id":"p243583","attributes":{"filter":{"type":"object","name":"AllIndices","id":"p243584"}}},"glyph":{"type":"object","name":"Scatter","id":"p243579","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"fill_color":{"type":"value","value":"#ff7270"},"hatch_color":{"type":"value","value":"#ff7270"}}},"nonselection_glyph":{"type":"object","name":"Scatter","id":"p243580","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"line_alpha":{"type":"value","value":0.1},"fill_color":{"type":"value","value":"#ff7270"},"fill_alpha":{"type":"value","value":0.1},"hatch_color":{"type":"value","value":"#ff7270"},"hatch_alpha":{"type":"value","value":0.1}}},"muted_glyph":{"type":"object","name":"Scatter","id":"p243581","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"line_alpha":{"type":"value","value":0.2},"fill_color":{"type":"value","value":"#ff7270"},"fill_alpha":{"type":"value","value":0.2},"hatch_color":{"type":"value","value":"#ff7270"},"hatch_alpha":{"type":"value","value":0.2}}}}},{"type":"object","name":"GlyphRenderer","id":"p243592","attributes":{"visible":false,"data_source":{"type":"object","name":"ColumnDataSource","id":"p243586","attributes":{"selected":{"type":"object","name":"Selection","id":"p243587","attributes":{"indices":[],"line_indices":[]}},"selection_policy":{"type":"object","name":"UnionRenderers","id":"p243588"},"data":{"type":"map","entries":[["x",[0]],["y",[0]]]}}},"view":{"type":"object","name":"CDSView","id":"p243593","attributes":{"filter":{"type":"object","name":"AllIndices","id":"p243594"}}},"glyph":{"type":"object","name":"Scatter","id":"p243589","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"fill_color":{"type":"value","value":"#70b1ff"},"hatch_color":{"type":"value","value":"#70b1ff"}}},"nonselection_glyph":{"type":"object","name":"Scatter","id":"p243590","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"line_alpha":{"type":"value","value":0.1},"fill_color":{"type":"value","value":"#70b1ff"},"fill_alpha":{"type":"value","value":0.1},"hatch_color":{"type":"value","value":"#70b1ff"},"hatch_alpha":{"type":"value","value":0.1}}},"muted_glyph":{"type":"object","name":"Scatter","id":"p243591","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"line_alpha":{"type":"value","value":0.2},"fill_color":{"type":"value","value":"#70b1ff"},"fill_alpha":{"type":"value","value":0.2},"hatch_color":{"type":"value","value":"#70b1ff"},"hatch_alpha":{"type":"value","value":0.2}}}}},{"type":"object","name":"GlyphRenderer","id":"p243603","attributes":{"visible":false,"data_source":{"type":"object","name":"ColumnDataSource","id":"p243597","attributes":{"selected":{"type":"object","name":"Selection","id":"p243598","attributes":{"indices":[],"line_indices":[]}},"selection_policy":{"type":"object","name":"UnionRenderers","id":"p243599"},"data":{"type":"map","entries":[["x",[0]],["y",[0]]]}}},"view":{"type":"object","name":"CDSView","id":"p243604","attributes":{"filter":{"type":"object","name":"AllIndices","id":"p243605"}}},"glyph":{"type":"object","name":"Scatter","id":"p243600","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"fill_color":{"type":"value","value":"grey"},"hatch_color":{"type":"value","value":"grey"}}},"nonselection_glyph":{"type":"object","name":"Scatter","id":"p243601","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"line_alpha":{"type":"value","value":0.1},"fill_color":{"type":"value","value":"grey"},"fill_alpha":{"type":"value","value":0.1},"hatch_color":{"type":"value","value":"grey"},"hatch_alpha":{"type":"value","value":0.1}}},"muted_glyph":{"type":"object","name":"Scatter","id":"p243602","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"line_alpha":{"type":"value","value":0.2},"fill_color":{"type":"value","value":"grey"},"fill_alpha":{"type":"value","value":0.2},"hatch_color":{"type":"value","value":"grey"},"hatch_alpha":{"type":"value","value":0.2}}}}},{"type":"object","name":"GlyphRenderer","id":"p243613","attributes":{"visible":false,"data_source":{"type":"object","name":"ColumnDataSource","id":"p243607","attributes":{"selected":{"type":"object","name":"Selection","id":"p243608","attributes":{"indices":[],"line_indices":[]}},"selection_policy":{"type":"object","name":"UnionRenderers","id":"p243609"},"data":{"type":"map","entries":[["x",[0]],["y",[0]]]}}},"view":{"type":"object","name":"CDSView","id":"p243614","attributes":{"filter":{"type":"object","name":"AllIndices","id":"p243615"}}},"glyph":{"type":"object","name":"Scatter","id":"p243610","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"fill_color":{"type":"value","value":"grey"},"hatch_color":{"type":"value","value":"grey"},"marker":{"type":"value","value":"diamond"}}},"nonselection_glyph":{"type":"object","name":"Scatter","id":"p243611","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"line_alpha":{"type":"value","value":0.1},"fill_color":{"type":"value","value":"grey"},"fill_alpha":{"type":"value","value":0.1},"hatch_color":{"type":"value","value":"grey"},"hatch_alpha":{"type":"value","value":0.1},"marker":{"type":"value","value":"diamond"}}},"muted_glyph":{"type":"object","name":"Scatter","id":"p243612","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"line_alpha":{"type":"value","value":0.2},"fill_color":{"type":"value","value":"grey"},"fill_alpha":{"type":"value","value":0.2},"hatch_color":{"type":"value","value":"grey"},"hatch_alpha":{"type":"value","value":0.2},"marker":{"type":"value","value":"diamond"}}}}},{"type":"object","name":"GlyphRenderer","id":"p243623","attributes":{"visible":false,"data_source":{"type":"object","name":"ColumnDataSource","id":"p243617","attributes":{"selected":{"type":"object","name":"Selection","id":"p243618","attributes":{"indices":[],"line_indices":[]}},"selection_policy":{"type":"object","name":"UnionRenderers","id":"p243619"},"data":{"type":"map","entries":[["x",[0]],["y",[0]]]}}},"view":{"type":"object","name":"CDSView","id":"p243624","attributes":{"filter":{"type":"object","name":"AllIndices","id":"p243625"}}},"glyph":{"type":"object","name":"Scatter","id":"p243620","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"fill_color":{"type":"value","value":"grey"},"hatch_color":{"type":"value","value":"grey"},"marker":{"type":"value","value":"triangle"}}},"nonselection_glyph":{"type":"object","name":"Scatter","id":"p243621","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"line_alpha":{"type":"value","value":0.1},"fill_color":{"type":"value","value":"grey"},"fill_alpha":{"type":"value","value":0.1},"hatch_color":{"type":"value","value":"grey"},"hatch_alpha":{"type":"value","value":0.1},"marker":{"type":"value","value":"triangle"}}},"muted_glyph":{"type":"object","name":"Scatter","id":"p243622","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"line_alpha":{"type":"value","value":0.2},"fill_color":{"type":"value","value":"grey"},"fill_alpha":{"type":"value","value":0.2},"hatch_color":{"type":"value","value":"grey"},"hatch_alpha":{"type":"value","value":0.2},"marker":{"type":"value","value":"triangle"}}}}}],"toolbar":{"type":"object","name":"Toolbar","id":"p243523","attributes":{"tools":[{"type":"object","name":"PanTool","id":"p243548"},{"type":"object","name":"WheelZoomTool","id":"p243549"},{"type":"object","name":"BoxZoomTool","id":"p243550","attributes":{"overlay":{"type":"object","name":"BoxAnnotation","id":"p243551","attributes":{"syncable":false,"level":"overlay","visible":false,"left_units":"canvas","right_units":"canvas","bottom_units":"canvas","top_units":"canvas","line_color":"black","line_alpha":1.0,"line_width":2,"line_dash":[4,4],"fill_color":"lightgrey","fill_alpha":0.5}}}},{"type":"object","name":"SaveTool","id":"p243552"},{"type":"object","name":"ResetTool","id":"p243553"},{"type":"object","name":"HelpTool","id":"p243554"},{"type":"object","name":"HoverTool","id":"p243513","attributes":{"renderers":"auto","tooltips":[["Title","@{Paper name}"],["Nb params","@{Nb params}"],["SPIDEr","@{SPIDEr}"],["Train data","@{Train data}"],["Pretrained","@{Pretrained}"]]}}]}},"left":[{"type":"object","name":"LinearAxis","id":"p243541","attributes":{"ticker":{"type":"object","name":"BasicTicker","id":"p243544","attributes":{"mantissas":[1,2,5]}},"formatter":{"type":"object","name":"BasicTickFormatter","id":"p243543"},"axis_label":"SPIDEr","axis_label_text_font_size":"20px","major_label_policy":{"type":"object","name":"AllLabels","id":"p243542"},"major_label_text_font_size":"20px"}}],"below":[{"type":"object","name":"LogAxis","id":"p243534","attributes":{"ticker":{"type":"object","name":"LogTicker","id":"p243537","attributes":{"num_minor_ticks":10,"mantissas":[1,5]}},"formatter":{"type":"object","name":"LogTickFormatter","id":"p243536"},"axis_label":"Parameters (M)","axis_label_text_font_size":"20px","major_label_policy":{"type":"object","name":"AllLabels","id":"p243535"},"major_label_text_font_size":"20px"}}],"center":[{"type":"object","name":"Grid","id":"p243540","attributes":{"visible":false,"axis":{"id":"p243534"}}},{"type":"object","name":"Grid","id":"p243547","attributes":{"visible":false,"dimension":1,"axis":{"id":"p243541"}}},{"type":"object","name":"Legend","id":"p243596","attributes":{"location":[285,115],"title":"External data","title_text_font_size":"14px","label_text_font_size":"14px","padding":5,"items":[{"type":"object","name":"LegendItem","id":"p243585","attributes":{"label":{"type":"value","value":"yes"},"renderers":[{"id":"p243582"}],"index":0}},{"type":"object","name":"LegendItem","id":"p243595","attributes":{"label":{"type":"value","value":"no"},"renderers":[{"id":"p243592"}],"index":2}}]}},{"type":"object","name":"Legend","id":"p243627","attributes":{"location":"bottom_right","title":"Pretrained","title_text_font_size":"14px","label_text_font_size":"14px","padding":5,"items":[{"type":"object","name":"LegendItem","id":"p243606","attributes":{"label":{"type":"value","value":"audio+text"},"renderers":[{"id":"p243603"}],"index":0}},{"type":"object","name":"LegendItem","id":"p243616","attributes":{"label":{"type":"value","value":"audio"},"renderers":[{"id":"p243613"}],"index":4}},{"type":"object","name":"LegendItem","id":"p243626","attributes":{"label":{"type":"value","value":"none"},"renderers":[{"id":"p243623"}],"index":5}}]}}],"background_fill_color":null,"border_fill_color":null,"output_backend":"svg"}}],"callbacks":{"type":"map"}}}
    </script>
    <script type="text/javascript">
      (function() {
        const fn = function() {
          Bokeh.safely(function() {
            (function(root) {
              function embed_document(root) {
              const docs_json = document.getElementById('p261755').textContent;
              const render_items = [{"docid":"cd6f2a4c-bcfe-415b-bc7f-e2e6ab59de8b","roots":{"p243517":"b400c5b8-5d80-489b-89ba-a1622a2526d8"},"root_ids":["p243517"]}];
              root.Bokeh.embed.embed_items(docs_json, render_items);
              }
              if (root.Bokeh !== undefined) {
                embed_document(root);
              } else {
                let attempts = 0;
                const timer = setInterval(function(root) {
                  if (root.Bokeh !== undefined) {
                    clearInterval(timer);
                    embed_document(root);
                  } else {
                    attempts++;
                    if (attempts > 100) {
                      clearInterval(timer);
                      console.log("Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing");
                    }
                  }
                }, 10, root)
              }
            })(window);
          });
        };
        if (document.readyState != "loading") fn();
        else document.addEventListener("DOMContentLoaded", fn);
      })();
    </script>

    <h1>SPIDEr as the function of the years for the AudioCaps (AC) dataset.</h1>
    <div id="d1c8c8a7-74fd-4ef4-aee3-4cdb0ca0a7a1" data-root-id="p256765" style="display: contents;"></div>

    <script type="application/json" id="p263267">
    {"8d5f837b-1f08-411f-94ab-546c1da821f1":{"version":"3.1.1","title":"Bokeh Application","defs":[],"roots":[{"type":"object","name":"Figure","id":"p256765","attributes":{"width":500,"height":500,"x_range":{"type":"object","name":"Range1d","id":"p256898","attributes":{"start":2018.1924000000001,"end":2023.8092}},"y_range":{"type":"object","name":"Range1d","id":"p256899","attributes":{"start":0.30528,"end":0.5044}},"x_scale":{"type":"object","name":"LinearScale","id":"p256778"},"y_scale":{"type":"object","name":"LinearScale","id":"p256780"},"title":null,"outline_line_color":null,"renderers":[{"type":"object","name":"GlyphRenderer","id":"p256833","attributes":{"data_source":{"type":"object","name":"ColumnDataSource","id":"p256824","attributes":{"selected":{"type":"object","name":"Selection","id":"p256825","attributes":{"indices":[],"line_indices":[]}},"selection_policy":{"type":"object","name":"UnionRenderers","id":"p256826"},"data":{"type":"map","entries":[["#",["2","2","3","6","8","9","11","12","13","15","20","21","31","32","33"]],["Paper name",["WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research","WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research","Rethinking Transfer and Auxiliary Learning for Improving Audio Captioning Transformer","Exploring Train and Test-Time Augmentations for Audio-Language Learning","Automated audio captioning by fine-tuning bart with audioset tags","AudioCaps: Generating Captions for Audios in The Wild","AUDIO CAPTIONING TRANSFORMER","IMPROVING AUDIO CAPTIONING USING SEMANTIC SIMILARITY METRICS","Visually-Aware Audio Captioning With Adaptive Audio-Visual Attention","Leveraging Pre-trained BERT for Audio Captioning","INVESTIGATING LOCAL AND GLOBAL INFORMATION FOR AUTOMATED AUDIO CAPTIONING WITH TRANSFER LEARNING","CONTINUAL LEARNING FOR AUTOMATED AUDIO CAPTIONING USING THE LEARNING WITHOUT FORGETTING APPROACH","WEAKLY-SUPERVISED AUTOMATED AUDIO CAPTIONING VIA TEXT ONLY TRAINING","TRAINING AUDIO CAPTIONING MODELS WITHOUT AUDIO","PREFIX TUNING FOR AUTOMATED AUDIO CAPTIONING"]],["Link",["https://arxiv.org/pdf/2303.17395.pdf","https://arxiv.org/pdf/2303.17395.pdf","https://www.isca-speech.org/archive/pdfs/interspeech_2023/shin23_interspeech.pdf","https://arxiv.org/pdf/2210.17143.pdf","https://inria.hal.science/hal-03522488/document","https://aclanthology.org/N19-1011.pdf","https://dcase.community/documents/workshop2021/proceedings/DCASE2021Workshop_Mei_68.pdf","https://arxiv.org/pdf/2210.16470.pdf","https://arxiv.org/pdf/2210.16428.pdf","https://arxiv.org/pdf/2203.02838.pdf","https://arxiv.org/pdf/2102.11457.pdf","https://arxiv.org/pdf/2107.08028v1.pdf","https://arxiv.org/pdf/2309.12242.pdf","https://arxiv.org/pdf/2309.07372.pdf","https://arxiv.org/pdf/2303.17489.pdf"]],["First Author",["Xinhao Mei","Xinhao Mei","Wooseok Shin","Eungbeom Kim","F\u00e9lix Gontier","Chris Dongjoo Kim","Xinhao Mei","Rehana Mahfuz","Xubo Liu","Xubo Liu","Xuenan Xu","Jan Berg","Theodoros Kouzelis","Soham Deshmukh","Minkyu Kim"]],["Nb params",[171.0,171.0,105.0,108.0,494.0,150.0,117.0,14.0,117.0,25.0,10.0,4.0,380.0,312.0,205.0]],["SPIDEr",[0.444,0.485,0.472,0.475,0.465,0.369,0.42,0.37,0.442,0.419,0.414,0.318,0.403,0.455,0.455]],["Train data",["AC","AC+WC","AC","AC","AC","AC","AC","AC","AC","AC","AC","AC","AC+WC","AC+WC","AC"]],["Test data",["AC","AC","AC","AC","AC","AC","AC","AC","AC","AC","AC","AC","AC","AC","AC"]],["Pretrained",["audio+text","audio+text","audio","audio","audio+text","audio","audio","audio","audio","audio+text","audio","none","audio","audio+text","audio+text"]],["Nb mod\u00e8les",[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]],["RL",[false,false,false,false,false,false,false,false,false,false,false,false,false,false,false]],["Year",["2023","2023","2023","2022","2021","2019","2021","2022","2023","2022","2021","2021","2023","2023","2023"]],["Archi",["trans-trans","trans-trans","trans-trans","trans-trans","cnn-trans","cnn-rnn","trans-trans","cnn-trans","trans-trans","cnn-trans","cnn-rnn","trans-trans","cnn-trans","trans-trans","cnn-trans"]],["Bibtex",["","","","","@inproceedings{Gontier2021, title = {Automated Audio Captioning by Fine-Tuning BART with AudioSet Tags}, author = {Gontier, Felix and Serizel, Romain and Cerisara, Christophe}, year = 2021, month = {November}, booktitle = {Proceedings of the 6th Detection and Classification of Acoustic Scenes and Events 2021 Workshop (DCASE2021)}, address = {Barcelona, Spain}, pages = {170--174}, isbn = {978-84-09-36072-7}, abstract = {Automated audio captioning is the multimodal task of describing environmental audio recordings with fluent natural language. Most current methods utilize pre-trained analysis models to extract relevant semantic content from the audio input. However, prior information on language modeling is rarely introduced, and corresponding architectures are limited in capacity due to data scarcity. In this paper, we present a method leveraging the linguistic information contained in BART, a large-scale conditional language model with general purpose pre-training. The caption generation is conditioned on sequences of textual AudioSet tags. This input is enriched with temporally aligned audio embeddings that allows the model to improve the sound event recognition. The full BART architecture is fine-tuned with few additional parameters. Experimental results demonstrate that, beyond the scaling properties of the architecture, language-only pre-training improves the text quality in the multimodal setting of audio captioning. The best model achieves state-of-the-art performance on AudioCaps with 46.5 SPIDEr.}, doi. = {10.5281/zenodo.5770113} }","","","","","","","","","",""]],["Optim",["Adam","Adam","Adam","AdamW","AdamW","Adam","Adam","?","Adam","Adam","Adam","Adam","Adam","Adam","AdamW"]],["ensemble",["Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single"]],["extra_train",[false,true,false,false,false,false,false,false,false,false,false,false,true,true,false]],["color",["#70b1ff","#ff7270","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#ff7270","#ff7270","#70b1ff"]],["marker",["circle","circle","diamond","diamond","circle","diamond","diamond","diamond","diamond","circle","diamond","triangle","diamond","circle","circle"]],["is_pareto",[false,true,true,true,false,false,false,false,false,true,true,true,false,false,false]],["pareto_idx",["none",1,2,3,"none","none","none","none","none",4,5,6,"none","none","none"]],["size",[13.076696830622021,13.076696830622021,10.246950765959598,10.392304845413264,22.22611077089287,12.24744871391589,10.816653826391969,3.7416573867739413,10.816653826391969,5.0,3.1622776601683795,2.0,19.493588689617926,17.663521732655695,14.317821063276353]],["params_display",["171M","171M","105M","108M","494M","150M","117M","14M","117M","25M","10M","4M","380M","312M","205M"]],["params_xoff",[-28.53834841531101,-28.53834841531101,-27.123475382979798,-27.196152422706632,-33.11305538544644,-28.123724356957943,-27.408326913195985,-23.870828693386972,-27.408326913195985,-24.5,-23.58113883008419,-23.0,-31.74679434480896,-30.831760866327848,-29.158910531638178]],["params_yoff",[-11.53834841531101,-11.53834841531101,-10.123475382979798,-10.196152422706632,-16.113055385446437,-11.123724356957945,-10.408326913195985,-6.87082869338697,-10.408326913195985,-7.5,-6.58113883008419,-6.0,-14.746794344808963,-13.831760866327848,-12.158910531638178]]]}}},"view":{"type":"object","name":"CDSView","id":"p256813","attributes":{"filter":{"type":"object","name":"IndexFilter","id":"p256812","attributes":{"indices":[6,10,11,7,9,0,8,14]}}}},"glyph":{"type":"object","name":"Scatter","id":"p256830","attributes":{"x":{"type":"field","field":"Year"},"y":{"type":"field","field":"SPIDEr"},"size":{"type":"field","field":"size"},"line_alpha":{"type":"value","value":0.15},"fill_color":{"type":"value","value":"#70b1ff"},"fill_alpha":{"type":"value","value":0.15},"hatch_color":{"type":"value","value":"#70b1ff"},"marker":{"type":"field","field":"marker"}}},"nonselection_glyph":{"type":"object","name":"Scatter","id":"p256831","attributes":{"x":{"type":"field","field":"Year"},"y":{"type":"field","field":"SPIDEr"},"size":{"type":"field","field":"size"},"line_alpha":{"type":"value","value":0.1},"fill_color":{"type":"value","value":"#70b1ff"},"fill_alpha":{"type":"value","value":0.1},"hatch_color":{"type":"value","value":"#70b1ff"},"hatch_alpha":{"type":"value","value":0.1},"marker":{"type":"field","field":"marker"}}},"muted_glyph":{"type":"object","name":"Scatter","id":"p256832","attributes":{"x":{"type":"field","field":"Year"},"y":{"type":"field","field":"SPIDEr"},"size":{"type":"field","field":"size"},"line_alpha":{"type":"value","value":0.2},"fill_color":{"type":"value","value":"#70b1ff"},"fill_alpha":{"type":"value","value":0.2},"hatch_color":{"type":"value","value":"#70b1ff"},"hatch_alpha":{"type":"value","value":0.2},"marker":{"type":"field","field":"marker"}}}}},{"type":"object","name":"GlyphRenderer","id":"p256845","attributes":{"data_source":{"type":"object","name":"ColumnDataSource","id":"p256836","attributes":{"selected":{"type":"object","name":"Selection","id":"p256837","attributes":{"indices":[],"line_indices":[]}},"selection_policy":{"type":"object","name":"UnionRenderers","id":"p256838"},"data":{"type":"map","entries":[["#",["2","2","3","6","8","9","11","12","13","15","20","21","31","32","33"]],["Paper name",["WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research","WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research","Rethinking Transfer and Auxiliary Learning for Improving Audio Captioning Transformer","Exploring Train and Test-Time Augmentations for Audio-Language Learning","Automated audio captioning by fine-tuning bart with audioset tags","AudioCaps: Generating Captions for Audios in The Wild","AUDIO CAPTIONING TRANSFORMER","IMPROVING AUDIO CAPTIONING USING SEMANTIC SIMILARITY METRICS","Visually-Aware Audio Captioning With Adaptive Audio-Visual Attention","Leveraging Pre-trained BERT for Audio Captioning","INVESTIGATING LOCAL AND GLOBAL INFORMATION FOR AUTOMATED AUDIO CAPTIONING WITH TRANSFER LEARNING","CONTINUAL LEARNING FOR AUTOMATED AUDIO CAPTIONING USING THE LEARNING WITHOUT FORGETTING APPROACH","WEAKLY-SUPERVISED AUTOMATED AUDIO CAPTIONING VIA TEXT ONLY TRAINING","TRAINING AUDIO CAPTIONING MODELS WITHOUT AUDIO","PREFIX TUNING FOR AUTOMATED AUDIO CAPTIONING"]],["Link",["https://arxiv.org/pdf/2303.17395.pdf","https://arxiv.org/pdf/2303.17395.pdf","https://www.isca-speech.org/archive/pdfs/interspeech_2023/shin23_interspeech.pdf","https://arxiv.org/pdf/2210.17143.pdf","https://inria.hal.science/hal-03522488/document","https://aclanthology.org/N19-1011.pdf","https://dcase.community/documents/workshop2021/proceedings/DCASE2021Workshop_Mei_68.pdf","https://arxiv.org/pdf/2210.16470.pdf","https://arxiv.org/pdf/2210.16428.pdf","https://arxiv.org/pdf/2203.02838.pdf","https://arxiv.org/pdf/2102.11457.pdf","https://arxiv.org/pdf/2107.08028v1.pdf","https://arxiv.org/pdf/2309.12242.pdf","https://arxiv.org/pdf/2309.07372.pdf","https://arxiv.org/pdf/2303.17489.pdf"]],["First Author",["Xinhao Mei","Xinhao Mei","Wooseok Shin","Eungbeom Kim","F\u00e9lix Gontier","Chris Dongjoo Kim","Xinhao Mei","Rehana Mahfuz","Xubo Liu","Xubo Liu","Xuenan Xu","Jan Berg","Theodoros Kouzelis","Soham Deshmukh","Minkyu Kim"]],["Nb params",[171.0,171.0,105.0,108.0,494.0,150.0,117.0,14.0,117.0,25.0,10.0,4.0,380.0,312.0,205.0]],["SPIDEr",[0.444,0.485,0.472,0.475,0.465,0.369,0.42,0.37,0.442,0.419,0.414,0.318,0.403,0.455,0.455]],["Train data",["AC","AC+WC","AC","AC","AC","AC","AC","AC","AC","AC","AC","AC","AC+WC","AC+WC","AC"]],["Test data",["AC","AC","AC","AC","AC","AC","AC","AC","AC","AC","AC","AC","AC","AC","AC"]],["Pretrained",["audio+text","audio+text","audio","audio","audio+text","audio","audio","audio","audio","audio+text","audio","none","audio","audio+text","audio+text"]],["Nb mod\u00e8les",[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]],["RL",[false,false,false,false,false,false,false,false,false,false,false,false,false,false,false]],["Year",["2023","2023","2023","2022","2021","2019","2021","2022","2023","2022","2021","2021","2023","2023","2023"]],["Archi",["trans-trans","trans-trans","trans-trans","trans-trans","cnn-trans","cnn-rnn","trans-trans","cnn-trans","trans-trans","cnn-trans","cnn-rnn","trans-trans","cnn-trans","trans-trans","cnn-trans"]],["Bibtex",["","","","","@inproceedings{Gontier2021, title = {Automated Audio Captioning by Fine-Tuning BART with AudioSet Tags}, author = {Gontier, Felix and Serizel, Romain and Cerisara, Christophe}, year = 2021, month = {November}, booktitle = {Proceedings of the 6th Detection and Classification of Acoustic Scenes and Events 2021 Workshop (DCASE2021)}, address = {Barcelona, Spain}, pages = {170--174}, isbn = {978-84-09-36072-7}, abstract = {Automated audio captioning is the multimodal task of describing environmental audio recordings with fluent natural language. Most current methods utilize pre-trained analysis models to extract relevant semantic content from the audio input. However, prior information on language modeling is rarely introduced, and corresponding architectures are limited in capacity due to data scarcity. In this paper, we present a method leveraging the linguistic information contained in BART, a large-scale conditional language model with general purpose pre-training. The caption generation is conditioned on sequences of textual AudioSet tags. This input is enriched with temporally aligned audio embeddings that allows the model to improve the sound event recognition. The full BART architecture is fine-tuned with few additional parameters. Experimental results demonstrate that, beyond the scaling properties of the architecture, language-only pre-training improves the text quality in the multimodal setting of audio captioning. The best model achieves state-of-the-art performance on AudioCaps with 46.5 SPIDEr.}, doi. = {10.5281/zenodo.5770113} }","","","","","","","","","",""]],["Optim",["Adam","Adam","Adam","AdamW","AdamW","Adam","Adam","?","Adam","Adam","Adam","Adam","Adam","Adam","AdamW"]],["ensemble",["Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single"]],["extra_train",[false,true,false,false,false,false,false,false,false,false,false,false,true,true,false]],["color",["#70b1ff","#ff7270","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#ff7270","#ff7270","#70b1ff"]],["marker",["circle","circle","diamond","diamond","circle","diamond","diamond","diamond","diamond","circle","diamond","triangle","diamond","circle","circle"]],["is_pareto",[false,true,true,true,false,false,false,false,false,true,true,true,false,false,false]],["pareto_idx",["none",1,2,3,"none","none","none","none","none",4,5,6,"none","none","none"]],["size",[13.076696830622021,13.076696830622021,10.246950765959598,10.392304845413264,22.22611077089287,12.24744871391589,10.816653826391969,3.7416573867739413,10.816653826391969,5.0,3.1622776601683795,2.0,19.493588689617926,17.663521732655695,14.317821063276353]],["params_display",["171M","171M","105M","108M","494M","150M","117M","14M","117M","25M","10M","4M","380M","312M","205M"]],["params_xoff",[-28.53834841531101,-28.53834841531101,-27.123475382979798,-27.196152422706632,-33.11305538544644,-28.123724356957943,-27.408326913195985,-23.870828693386972,-27.408326913195985,-24.5,-23.58113883008419,-23.0,-31.74679434480896,-30.831760866327848,-29.158910531638178]],["params_yoff",[-11.53834841531101,-11.53834841531101,-10.123475382979798,-10.196152422706632,-16.113055385446437,-11.123724356957945,-10.408326913195985,-6.87082869338697,-10.408326913195985,-7.5,-6.58113883008419,-6.0,-14.746794344808963,-13.831760866327848,-12.158910531638178]]]}}},"view":{"type":"object","name":"CDSView","id":"p256819","attributes":{"filter":{"type":"object","name":"IndexFilter","id":"p256818","attributes":{"indices":[12,13]}}}},"glyph":{"type":"object","name":"Scatter","id":"p256842","attributes":{"x":{"type":"field","field":"Year"},"y":{"type":"field","field":"SPIDEr"},"size":{"type":"field","field":"size"},"line_alpha":{"type":"value","value":0.15},"fill_color":{"type":"value","value":"#ff7270"},"fill_alpha":{"type":"value","value":0.15},"hatch_color":{"type":"value","value":"#ff7270"},"marker":{"type":"field","field":"marker"}}},"nonselection_glyph":{"type":"object","name":"Scatter","id":"p256843","attributes":{"x":{"type":"field","field":"Year"},"y":{"type":"field","field":"SPIDEr"},"size":{"type":"field","field":"size"},"line_alpha":{"type":"value","value":0.1},"fill_color":{"type":"value","value":"#ff7270"},"fill_alpha":{"type":"value","value":0.1},"hatch_color":{"type":"value","value":"#ff7270"},"hatch_alpha":{"type":"value","value":0.1},"marker":{"type":"field","field":"marker"}}},"muted_glyph":{"type":"object","name":"Scatter","id":"p256844","attributes":{"x":{"type":"field","field":"Year"},"y":{"type":"field","field":"SPIDEr"},"size":{"type":"field","field":"size"},"line_alpha":{"type":"value","value":0.2},"fill_color":{"type":"value","value":"#ff7270"},"fill_alpha":{"type":"value","value":0.2},"hatch_color":{"type":"value","value":"#ff7270"},"hatch_alpha":{"type":"value","value":0.2},"marker":{"type":"field","field":"marker"}}}}},{"type":"object","name":"GlyphRenderer","id":"p256857","attributes":{"data_source":{"type":"object","name":"ColumnDataSource","id":"p256848","attributes":{"selected":{"type":"object","name":"Selection","id":"p256849","attributes":{"indices":[],"line_indices":[]}},"selection_policy":{"type":"object","name":"UnionRenderers","id":"p256850"},"data":{"type":"map","entries":[["#",["2","2","3","6","8","9","11","12","13","15","20","21","31","32","33"]],["Paper name",["WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research","WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research","Rethinking Transfer and Auxiliary Learning for Improving Audio Captioning Transformer","Exploring Train and Test-Time Augmentations for Audio-Language Learning","Automated audio captioning by fine-tuning bart with audioset tags","AudioCaps: Generating Captions for Audios in The Wild","AUDIO CAPTIONING TRANSFORMER","IMPROVING AUDIO CAPTIONING USING SEMANTIC SIMILARITY METRICS","Visually-Aware Audio Captioning With Adaptive Audio-Visual Attention","Leveraging Pre-trained BERT for Audio Captioning","INVESTIGATING LOCAL AND GLOBAL INFORMATION FOR AUTOMATED AUDIO CAPTIONING WITH TRANSFER LEARNING","CONTINUAL LEARNING FOR AUTOMATED AUDIO CAPTIONING USING THE LEARNING WITHOUT FORGETTING APPROACH","WEAKLY-SUPERVISED AUTOMATED AUDIO CAPTIONING VIA TEXT ONLY TRAINING","TRAINING AUDIO CAPTIONING MODELS WITHOUT AUDIO","PREFIX TUNING FOR AUTOMATED AUDIO CAPTIONING"]],["Link",["https://arxiv.org/pdf/2303.17395.pdf","https://arxiv.org/pdf/2303.17395.pdf","https://www.isca-speech.org/archive/pdfs/interspeech_2023/shin23_interspeech.pdf","https://arxiv.org/pdf/2210.17143.pdf","https://inria.hal.science/hal-03522488/document","https://aclanthology.org/N19-1011.pdf","https://dcase.community/documents/workshop2021/proceedings/DCASE2021Workshop_Mei_68.pdf","https://arxiv.org/pdf/2210.16470.pdf","https://arxiv.org/pdf/2210.16428.pdf","https://arxiv.org/pdf/2203.02838.pdf","https://arxiv.org/pdf/2102.11457.pdf","https://arxiv.org/pdf/2107.08028v1.pdf","https://arxiv.org/pdf/2309.12242.pdf","https://arxiv.org/pdf/2309.07372.pdf","https://arxiv.org/pdf/2303.17489.pdf"]],["First Author",["Xinhao Mei","Xinhao Mei","Wooseok Shin","Eungbeom Kim","F\u00e9lix Gontier","Chris Dongjoo Kim","Xinhao Mei","Rehana Mahfuz","Xubo Liu","Xubo Liu","Xuenan Xu","Jan Berg","Theodoros Kouzelis","Soham Deshmukh","Minkyu Kim"]],["Nb params",[171.0,171.0,105.0,108.0,494.0,150.0,117.0,14.0,117.0,25.0,10.0,4.0,380.0,312.0,205.0]],["SPIDEr",[0.444,0.485,0.472,0.475,0.465,0.369,0.42,0.37,0.442,0.419,0.414,0.318,0.403,0.455,0.455]],["Train data",["AC","AC+WC","AC","AC","AC","AC","AC","AC","AC","AC","AC","AC","AC+WC","AC+WC","AC"]],["Test data",["AC","AC","AC","AC","AC","AC","AC","AC","AC","AC","AC","AC","AC","AC","AC"]],["Pretrained",["audio+text","audio+text","audio","audio","audio+text","audio","audio","audio","audio","audio+text","audio","none","audio","audio+text","audio+text"]],["Nb mod\u00e8les",[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]],["RL",[false,false,false,false,false,false,false,false,false,false,false,false,false,false,false]],["Year",["2023","2023","2023","2022","2021","2019","2021","2022","2023","2022","2021","2021","2023","2023","2023"]],["Archi",["trans-trans","trans-trans","trans-trans","trans-trans","cnn-trans","cnn-rnn","trans-trans","cnn-trans","trans-trans","cnn-trans","cnn-rnn","trans-trans","cnn-trans","trans-trans","cnn-trans"]],["Bibtex",["","","","","@inproceedings{Gontier2021, title = {Automated Audio Captioning by Fine-Tuning BART with AudioSet Tags}, author = {Gontier, Felix and Serizel, Romain and Cerisara, Christophe}, year = 2021, month = {November}, booktitle = {Proceedings of the 6th Detection and Classification of Acoustic Scenes and Events 2021 Workshop (DCASE2021)}, address = {Barcelona, Spain}, pages = {170--174}, isbn = {978-84-09-36072-7}, abstract = {Automated audio captioning is the multimodal task of describing environmental audio recordings with fluent natural language. Most current methods utilize pre-trained analysis models to extract relevant semantic content from the audio input. However, prior information on language modeling is rarely introduced, and corresponding architectures are limited in capacity due to data scarcity. In this paper, we present a method leveraging the linguistic information contained in BART, a large-scale conditional language model with general purpose pre-training. The caption generation is conditioned on sequences of textual AudioSet tags. This input is enriched with temporally aligned audio embeddings that allows the model to improve the sound event recognition. The full BART architecture is fine-tuned with few additional parameters. Experimental results demonstrate that, beyond the scaling properties of the architecture, language-only pre-training improves the text quality in the multimodal setting of audio captioning. The best model achieves state-of-the-art performance on AudioCaps with 46.5 SPIDEr.}, doi. = {10.5281/zenodo.5770113} }","","","","","","","","","",""]],["Optim",["Adam","Adam","Adam","AdamW","AdamW","Adam","Adam","?","Adam","Adam","Adam","Adam","Adam","Adam","AdamW"]],["ensemble",["Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single"]],["extra_train",[false,true,false,false,false,false,false,false,false,false,false,false,true,true,false]],["color",["#70b1ff","#ff7270","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#ff7270","#ff7270","#70b1ff"]],["marker",["circle","circle","diamond","diamond","circle","diamond","diamond","diamond","diamond","circle","diamond","triangle","diamond","circle","circle"]],["is_pareto",[false,true,true,true,false,false,false,false,false,true,true,true,false,false,false]],["pareto_idx",["none",1,2,3,"none","none","none","none","none",4,5,6,"none","none","none"]],["size",[13.076696830622021,13.076696830622021,10.246950765959598,10.392304845413264,22.22611077089287,12.24744871391589,10.816653826391969,3.7416573867739413,10.816653826391969,5.0,3.1622776601683795,2.0,19.493588689617926,17.663521732655695,14.317821063276353]],["params_display",["171M","171M","105M","108M","494M","150M","117M","14M","117M","25M","10M","4M","380M","312M","205M"]],["params_xoff",[-28.53834841531101,-28.53834841531101,-27.123475382979798,-27.196152422706632,-33.11305538544644,-28.123724356957943,-27.408326913195985,-23.870828693386972,-27.408326913195985,-24.5,-23.58113883008419,-23.0,-31.74679434480896,-30.831760866327848,-29.158910531638178]],["params_yoff",[-11.53834841531101,-11.53834841531101,-10.123475382979798,-10.196152422706632,-16.113055385446437,-11.123724356957945,-10.408326913195985,-6.87082869338697,-10.408326913195985,-7.5,-6.58113883008419,-6.0,-14.746794344808963,-13.831760866327848,-12.158910531638178]]]}}},"view":{"type":"object","name":"CDSView","id":"p256816","attributes":{"filter":{"type":"object","name":"IndexFilter","id":"p256815","attributes":{"indices":[5,4,3,2]}}}},"glyph":{"type":"object","name":"Scatter","id":"p256854","attributes":{"x":{"type":"field","field":"Year"},"y":{"type":"field","field":"SPIDEr"},"size":{"type":"field","field":"size"},"fill_color":{"type":"value","value":"#70b1ff"},"hatch_color":{"type":"value","value":"#70b1ff"},"marker":{"type":"field","field":"marker"}}},"nonselection_glyph":{"type":"object","name":"Scatter","id":"p256855","attributes":{"x":{"type":"field","field":"Year"},"y":{"type":"field","field":"SPIDEr"},"size":{"type":"field","field":"size"},"line_alpha":{"type":"value","value":0.1},"fill_color":{"type":"value","value":"#70b1ff"},"fill_alpha":{"type":"value","value":0.1},"hatch_color":{"type":"value","value":"#70b1ff"},"hatch_alpha":{"type":"value","value":0.1},"marker":{"type":"field","field":"marker"}}},"muted_glyph":{"type":"object","name":"Scatter","id":"p256856","attributes":{"x":{"type":"field","field":"Year"},"y":{"type":"field","field":"SPIDEr"},"size":{"type":"field","field":"size"},"line_alpha":{"type":"value","value":0.2},"fill_color":{"type":"value","value":"#70b1ff"},"fill_alpha":{"type":"value","value":0.2},"hatch_color":{"type":"value","value":"#70b1ff"},"hatch_alpha":{"type":"value","value":0.2},"marker":{"type":"field","field":"marker"}}}}},{"type":"object","name":"GlyphRenderer","id":"p256869","attributes":{"data_source":{"type":"object","name":"ColumnDataSource","id":"p256860","attributes":{"selected":{"type":"object","name":"Selection","id":"p256861","attributes":{"indices":[],"line_indices":[]}},"selection_policy":{"type":"object","name":"UnionRenderers","id":"p256862"},"data":{"type":"map","entries":[["#",["2","2","3","6","8","9","11","12","13","15","20","21","31","32","33"]],["Paper name",["WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research","WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research","Rethinking Transfer and Auxiliary Learning for Improving Audio Captioning Transformer","Exploring Train and Test-Time Augmentations for Audio-Language Learning","Automated audio captioning by fine-tuning bart with audioset tags","AudioCaps: Generating Captions for Audios in The Wild","AUDIO CAPTIONING TRANSFORMER","IMPROVING AUDIO CAPTIONING USING SEMANTIC SIMILARITY METRICS","Visually-Aware Audio Captioning With Adaptive Audio-Visual Attention","Leveraging Pre-trained BERT for Audio Captioning","INVESTIGATING LOCAL AND GLOBAL INFORMATION FOR AUTOMATED AUDIO CAPTIONING WITH TRANSFER LEARNING","CONTINUAL LEARNING FOR AUTOMATED AUDIO CAPTIONING USING THE LEARNING WITHOUT FORGETTING APPROACH","WEAKLY-SUPERVISED AUTOMATED AUDIO CAPTIONING VIA TEXT ONLY TRAINING","TRAINING AUDIO CAPTIONING MODELS WITHOUT AUDIO","PREFIX TUNING FOR AUTOMATED AUDIO CAPTIONING"]],["Link",["https://arxiv.org/pdf/2303.17395.pdf","https://arxiv.org/pdf/2303.17395.pdf","https://www.isca-speech.org/archive/pdfs/interspeech_2023/shin23_interspeech.pdf","https://arxiv.org/pdf/2210.17143.pdf","https://inria.hal.science/hal-03522488/document","https://aclanthology.org/N19-1011.pdf","https://dcase.community/documents/workshop2021/proceedings/DCASE2021Workshop_Mei_68.pdf","https://arxiv.org/pdf/2210.16470.pdf","https://arxiv.org/pdf/2210.16428.pdf","https://arxiv.org/pdf/2203.02838.pdf","https://arxiv.org/pdf/2102.11457.pdf","https://arxiv.org/pdf/2107.08028v1.pdf","https://arxiv.org/pdf/2309.12242.pdf","https://arxiv.org/pdf/2309.07372.pdf","https://arxiv.org/pdf/2303.17489.pdf"]],["First Author",["Xinhao Mei","Xinhao Mei","Wooseok Shin","Eungbeom Kim","F\u00e9lix Gontier","Chris Dongjoo Kim","Xinhao Mei","Rehana Mahfuz","Xubo Liu","Xubo Liu","Xuenan Xu","Jan Berg","Theodoros Kouzelis","Soham Deshmukh","Minkyu Kim"]],["Nb params",[171.0,171.0,105.0,108.0,494.0,150.0,117.0,14.0,117.0,25.0,10.0,4.0,380.0,312.0,205.0]],["SPIDEr",[0.444,0.485,0.472,0.475,0.465,0.369,0.42,0.37,0.442,0.419,0.414,0.318,0.403,0.455,0.455]],["Train data",["AC","AC+WC","AC","AC","AC","AC","AC","AC","AC","AC","AC","AC","AC+WC","AC+WC","AC"]],["Test data",["AC","AC","AC","AC","AC","AC","AC","AC","AC","AC","AC","AC","AC","AC","AC"]],["Pretrained",["audio+text","audio+text","audio","audio","audio+text","audio","audio","audio","audio","audio+text","audio","none","audio","audio+text","audio+text"]],["Nb mod\u00e8les",[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]],["RL",[false,false,false,false,false,false,false,false,false,false,false,false,false,false,false]],["Year",["2023","2023","2023","2022","2021","2019","2021","2022","2023","2022","2021","2021","2023","2023","2023"]],["Archi",["trans-trans","trans-trans","trans-trans","trans-trans","cnn-trans","cnn-rnn","trans-trans","cnn-trans","trans-trans","cnn-trans","cnn-rnn","trans-trans","cnn-trans","trans-trans","cnn-trans"]],["Bibtex",["","","","","@inproceedings{Gontier2021, title = {Automated Audio Captioning by Fine-Tuning BART with AudioSet Tags}, author = {Gontier, Felix and Serizel, Romain and Cerisara, Christophe}, year = 2021, month = {November}, booktitle = {Proceedings of the 6th Detection and Classification of Acoustic Scenes and Events 2021 Workshop (DCASE2021)}, address = {Barcelona, Spain}, pages = {170--174}, isbn = {978-84-09-36072-7}, abstract = {Automated audio captioning is the multimodal task of describing environmental audio recordings with fluent natural language. Most current methods utilize pre-trained analysis models to extract relevant semantic content from the audio input. However, prior information on language modeling is rarely introduced, and corresponding architectures are limited in capacity due to data scarcity. In this paper, we present a method leveraging the linguistic information contained in BART, a large-scale conditional language model with general purpose pre-training. The caption generation is conditioned on sequences of textual AudioSet tags. This input is enriched with temporally aligned audio embeddings that allows the model to improve the sound event recognition. The full BART architecture is fine-tuned with few additional parameters. Experimental results demonstrate that, beyond the scaling properties of the architecture, language-only pre-training improves the text quality in the multimodal setting of audio captioning. The best model achieves state-of-the-art performance on AudioCaps with 46.5 SPIDEr.}, doi. = {10.5281/zenodo.5770113} }","","","","","","","","","",""]],["Optim",["Adam","Adam","Adam","AdamW","AdamW","Adam","Adam","?","Adam","Adam","Adam","Adam","Adam","Adam","AdamW"]],["ensemble",["Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single"]],["extra_train",[false,true,false,false,false,false,false,false,false,false,false,false,true,true,false]],["color",["#70b1ff","#ff7270","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#ff7270","#ff7270","#70b1ff"]],["marker",["circle","circle","diamond","diamond","circle","diamond","diamond","diamond","diamond","circle","diamond","triangle","diamond","circle","circle"]],["is_pareto",[false,true,true,true,false,false,false,false,false,true,true,true,false,false,false]],["pareto_idx",["none",1,2,3,"none","none","none","none","none",4,5,6,"none","none","none"]],["size",[13.076696830622021,13.076696830622021,10.246950765959598,10.392304845413264,22.22611077089287,12.24744871391589,10.816653826391969,3.7416573867739413,10.816653826391969,5.0,3.1622776601683795,2.0,19.493588689617926,17.663521732655695,14.317821063276353]],["params_display",["171M","171M","105M","108M","494M","150M","117M","14M","117M","25M","10M","4M","380M","312M","205M"]],["params_xoff",[-28.53834841531101,-28.53834841531101,-27.123475382979798,-27.196152422706632,-33.11305538544644,-28.123724356957943,-27.408326913195985,-23.870828693386972,-27.408326913195985,-24.5,-23.58113883008419,-23.0,-31.74679434480896,-30.831760866327848,-29.158910531638178]],["params_yoff",[-11.53834841531101,-11.53834841531101,-10.123475382979798,-10.196152422706632,-16.113055385446437,-11.123724356957945,-10.408326913195985,-6.87082869338697,-10.408326913195985,-7.5,-6.58113883008419,-6.0,-14.746794344808963,-13.831760866327848,-12.158910531638178]]]}}},"view":{"id":"p256816"},"glyph":{"type":"object","name":"Text","id":"p256866","attributes":{"x":{"type":"field","field":"Year"},"y":{"type":"field","field":"SPIDEr"},"text":{"type":"field","field":"params_display"},"x_offset":{"type":"field","field":"params_xoff"},"y_offset":{"type":"field","field":"params_yoff"},"text_color":{"type":"value","value":"#70b1ff"},"text_font_size":{"type":"value","value":"18px"},"text_font_style":{"type":"value","value":"bold"},"text_align":{"type":"value","value":"center"},"text_baseline":{"type":"value","value":"middle"}}},"nonselection_glyph":{"type":"object","name":"Text","id":"p256867","attributes":{"x":{"type":"field","field":"Year"},"y":{"type":"field","field":"SPIDEr"},"text":{"type":"field","field":"params_display"},"x_offset":{"type":"field","field":"params_xoff"},"y_offset":{"type":"field","field":"params_yoff"},"text_color":{"type":"value","value":"#70b1ff"},"text_alpha":{"type":"value","value":0.1},"text_font_size":{"type":"value","value":"18px"},"text_font_style":{"type":"value","value":"bold"},"text_align":{"type":"value","value":"center"},"text_baseline":{"type":"value","value":"middle"}}},"muted_glyph":{"type":"object","name":"Text","id":"p256868","attributes":{"x":{"type":"field","field":"Year"},"y":{"type":"field","field":"SPIDEr"},"text":{"type":"field","field":"params_display"},"x_offset":{"type":"field","field":"params_xoff"},"y_offset":{"type":"field","field":"params_yoff"},"text_color":{"type":"value","value":"#70b1ff"},"text_alpha":{"type":"value","value":0.2},"text_font_size":{"type":"value","value":"18px"},"text_font_style":{"type":"value","value":"bold"},"text_align":{"type":"value","value":"center"},"text_baseline":{"type":"value","value":"middle"}}}}},{"type":"object","name":"GlyphRenderer","id":"p256881","attributes":{"data_source":{"type":"object","name":"ColumnDataSource","id":"p256872","attributes":{"selected":{"type":"object","name":"Selection","id":"p256873","attributes":{"indices":[],"line_indices":[]}},"selection_policy":{"type":"object","name":"UnionRenderers","id":"p256874"},"data":{"type":"map","entries":[["#",["2","2","3","6","8","9","11","12","13","15","20","21","31","32","33"]],["Paper name",["WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research","WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research","Rethinking Transfer and Auxiliary Learning for Improving Audio Captioning Transformer","Exploring Train and Test-Time Augmentations for Audio-Language Learning","Automated audio captioning by fine-tuning bart with audioset tags","AudioCaps: Generating Captions for Audios in The Wild","AUDIO CAPTIONING TRANSFORMER","IMPROVING AUDIO CAPTIONING USING SEMANTIC SIMILARITY METRICS","Visually-Aware Audio Captioning With Adaptive Audio-Visual Attention","Leveraging Pre-trained BERT for Audio Captioning","INVESTIGATING LOCAL AND GLOBAL INFORMATION FOR AUTOMATED AUDIO CAPTIONING WITH TRANSFER LEARNING","CONTINUAL LEARNING FOR AUTOMATED AUDIO CAPTIONING USING THE LEARNING WITHOUT FORGETTING APPROACH","WEAKLY-SUPERVISED AUTOMATED AUDIO CAPTIONING VIA TEXT ONLY TRAINING","TRAINING AUDIO CAPTIONING MODELS WITHOUT AUDIO","PREFIX TUNING FOR AUTOMATED AUDIO CAPTIONING"]],["Link",["https://arxiv.org/pdf/2303.17395.pdf","https://arxiv.org/pdf/2303.17395.pdf","https://www.isca-speech.org/archive/pdfs/interspeech_2023/shin23_interspeech.pdf","https://arxiv.org/pdf/2210.17143.pdf","https://inria.hal.science/hal-03522488/document","https://aclanthology.org/N19-1011.pdf","https://dcase.community/documents/workshop2021/proceedings/DCASE2021Workshop_Mei_68.pdf","https://arxiv.org/pdf/2210.16470.pdf","https://arxiv.org/pdf/2210.16428.pdf","https://arxiv.org/pdf/2203.02838.pdf","https://arxiv.org/pdf/2102.11457.pdf","https://arxiv.org/pdf/2107.08028v1.pdf","https://arxiv.org/pdf/2309.12242.pdf","https://arxiv.org/pdf/2309.07372.pdf","https://arxiv.org/pdf/2303.17489.pdf"]],["First Author",["Xinhao Mei","Xinhao Mei","Wooseok Shin","Eungbeom Kim","F\u00e9lix Gontier","Chris Dongjoo Kim","Xinhao Mei","Rehana Mahfuz","Xubo Liu","Xubo Liu","Xuenan Xu","Jan Berg","Theodoros Kouzelis","Soham Deshmukh","Minkyu Kim"]],["Nb params",[171.0,171.0,105.0,108.0,494.0,150.0,117.0,14.0,117.0,25.0,10.0,4.0,380.0,312.0,205.0]],["SPIDEr",[0.444,0.485,0.472,0.475,0.465,0.369,0.42,0.37,0.442,0.419,0.414,0.318,0.403,0.455,0.455]],["Train data",["AC","AC+WC","AC","AC","AC","AC","AC","AC","AC","AC","AC","AC","AC+WC","AC+WC","AC"]],["Test data",["AC","AC","AC","AC","AC","AC","AC","AC","AC","AC","AC","AC","AC","AC","AC"]],["Pretrained",["audio+text","audio+text","audio","audio","audio+text","audio","audio","audio","audio","audio+text","audio","none","audio","audio+text","audio+text"]],["Nb mod\u00e8les",[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]],["RL",[false,false,false,false,false,false,false,false,false,false,false,false,false,false,false]],["Year",["2023","2023","2023","2022","2021","2019","2021","2022","2023","2022","2021","2021","2023","2023","2023"]],["Archi",["trans-trans","trans-trans","trans-trans","trans-trans","cnn-trans","cnn-rnn","trans-trans","cnn-trans","trans-trans","cnn-trans","cnn-rnn","trans-trans","cnn-trans","trans-trans","cnn-trans"]],["Bibtex",["","","","","@inproceedings{Gontier2021, title = {Automated Audio Captioning by Fine-Tuning BART with AudioSet Tags}, author = {Gontier, Felix and Serizel, Romain and Cerisara, Christophe}, year = 2021, month = {November}, booktitle = {Proceedings of the 6th Detection and Classification of Acoustic Scenes and Events 2021 Workshop (DCASE2021)}, address = {Barcelona, Spain}, pages = {170--174}, isbn = {978-84-09-36072-7}, abstract = {Automated audio captioning is the multimodal task of describing environmental audio recordings with fluent natural language. Most current methods utilize pre-trained analysis models to extract relevant semantic content from the audio input. However, prior information on language modeling is rarely introduced, and corresponding architectures are limited in capacity due to data scarcity. In this paper, we present a method leveraging the linguistic information contained in BART, a large-scale conditional language model with general purpose pre-training. The caption generation is conditioned on sequences of textual AudioSet tags. This input is enriched with temporally aligned audio embeddings that allows the model to improve the sound event recognition. The full BART architecture is fine-tuned with few additional parameters. Experimental results demonstrate that, beyond the scaling properties of the architecture, language-only pre-training improves the text quality in the multimodal setting of audio captioning. The best model achieves state-of-the-art performance on AudioCaps with 46.5 SPIDEr.}, doi. = {10.5281/zenodo.5770113} }","","","","","","","","","",""]],["Optim",["Adam","Adam","Adam","AdamW","AdamW","Adam","Adam","?","Adam","Adam","Adam","Adam","Adam","Adam","AdamW"]],["ensemble",["Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single"]],["extra_train",[false,true,false,false,false,false,false,false,false,false,false,false,true,true,false]],["color",["#70b1ff","#ff7270","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#ff7270","#ff7270","#70b1ff"]],["marker",["circle","circle","diamond","diamond","circle","diamond","diamond","diamond","diamond","circle","diamond","triangle","diamond","circle","circle"]],["is_pareto",[false,true,true,true,false,false,false,false,false,true,true,true,false,false,false]],["pareto_idx",["none",1,2,3,"none","none","none","none","none",4,5,6,"none","none","none"]],["size",[13.076696830622021,13.076696830622021,10.246950765959598,10.392304845413264,22.22611077089287,12.24744871391589,10.816653826391969,3.7416573867739413,10.816653826391969,5.0,3.1622776601683795,2.0,19.493588689617926,17.663521732655695,14.317821063276353]],["params_display",["171M","171M","105M","108M","494M","150M","117M","14M","117M","25M","10M","4M","380M","312M","205M"]],["params_xoff",[-28.53834841531101,-28.53834841531101,-27.123475382979798,-27.196152422706632,-33.11305538544644,-28.123724356957943,-27.408326913195985,-23.870828693386972,-27.408326913195985,-24.5,-23.58113883008419,-23.0,-31.74679434480896,-30.831760866327848,-29.158910531638178]],["params_yoff",[-11.53834841531101,-11.53834841531101,-10.123475382979798,-10.196152422706632,-16.113055385446437,-11.123724356957945,-10.408326913195985,-6.87082869338697,-10.408326913195985,-7.5,-6.58113883008419,-6.0,-14.746794344808963,-13.831760866327848,-12.158910531638178]]]}}},"view":{"type":"object","name":"CDSView","id":"p256822","attributes":{"filter":{"type":"object","name":"IndexFilter","id":"p256821","attributes":{"indices":[1]}}}},"glyph":{"type":"object","name":"Scatter","id":"p256878","attributes":{"x":{"type":"field","field":"Year"},"y":{"type":"field","field":"SPIDEr"},"size":{"type":"field","field":"size"},"fill_color":{"type":"value","value":"#ff7270"},"hatch_color":{"type":"value","value":"#ff7270"},"marker":{"type":"field","field":"marker"}}},"nonselection_glyph":{"type":"object","name":"Scatter","id":"p256879","attributes":{"x":{"type":"field","field":"Year"},"y":{"type":"field","field":"SPIDEr"},"size":{"type":"field","field":"size"},"line_alpha":{"type":"value","value":0.1},"fill_color":{"type":"value","value":"#ff7270"},"fill_alpha":{"type":"value","value":0.1},"hatch_color":{"type":"value","value":"#ff7270"},"hatch_alpha":{"type":"value","value":0.1},"marker":{"type":"field","field":"marker"}}},"muted_glyph":{"type":"object","name":"Scatter","id":"p256880","attributes":{"x":{"type":"field","field":"Year"},"y":{"type":"field","field":"SPIDEr"},"size":{"type":"field","field":"size"},"line_alpha":{"type":"value","value":0.2},"fill_color":{"type":"value","value":"#ff7270"},"fill_alpha":{"type":"value","value":0.2},"hatch_color":{"type":"value","value":"#ff7270"},"hatch_alpha":{"type":"value","value":0.2},"marker":{"type":"field","field":"marker"}}}}},{"type":"object","name":"GlyphRenderer","id":"p256893","attributes":{"data_source":{"type":"object","name":"ColumnDataSource","id":"p256884","attributes":{"selected":{"type":"object","name":"Selection","id":"p256885","attributes":{"indices":[],"line_indices":[]}},"selection_policy":{"type":"object","name":"UnionRenderers","id":"p256886"},"data":{"type":"map","entries":[["#",["2","2","3","6","8","9","11","12","13","15","20","21","31","32","33"]],["Paper name",["WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research","WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research","Rethinking Transfer and Auxiliary Learning for Improving Audio Captioning Transformer","Exploring Train and Test-Time Augmentations for Audio-Language Learning","Automated audio captioning by fine-tuning bart with audioset tags","AudioCaps: Generating Captions for Audios in The Wild","AUDIO CAPTIONING TRANSFORMER","IMPROVING AUDIO CAPTIONING USING SEMANTIC SIMILARITY METRICS","Visually-Aware Audio Captioning With Adaptive Audio-Visual Attention","Leveraging Pre-trained BERT for Audio Captioning","INVESTIGATING LOCAL AND GLOBAL INFORMATION FOR AUTOMATED AUDIO CAPTIONING WITH TRANSFER LEARNING","CONTINUAL LEARNING FOR AUTOMATED AUDIO CAPTIONING USING THE LEARNING WITHOUT FORGETTING APPROACH","WEAKLY-SUPERVISED AUTOMATED AUDIO CAPTIONING VIA TEXT ONLY TRAINING","TRAINING AUDIO CAPTIONING MODELS WITHOUT AUDIO","PREFIX TUNING FOR AUTOMATED AUDIO CAPTIONING"]],["Link",["https://arxiv.org/pdf/2303.17395.pdf","https://arxiv.org/pdf/2303.17395.pdf","https://www.isca-speech.org/archive/pdfs/interspeech_2023/shin23_interspeech.pdf","https://arxiv.org/pdf/2210.17143.pdf","https://inria.hal.science/hal-03522488/document","https://aclanthology.org/N19-1011.pdf","https://dcase.community/documents/workshop2021/proceedings/DCASE2021Workshop_Mei_68.pdf","https://arxiv.org/pdf/2210.16470.pdf","https://arxiv.org/pdf/2210.16428.pdf","https://arxiv.org/pdf/2203.02838.pdf","https://arxiv.org/pdf/2102.11457.pdf","https://arxiv.org/pdf/2107.08028v1.pdf","https://arxiv.org/pdf/2309.12242.pdf","https://arxiv.org/pdf/2309.07372.pdf","https://arxiv.org/pdf/2303.17489.pdf"]],["First Author",["Xinhao Mei","Xinhao Mei","Wooseok Shin","Eungbeom Kim","F\u00e9lix Gontier","Chris Dongjoo Kim","Xinhao Mei","Rehana Mahfuz","Xubo Liu","Xubo Liu","Xuenan Xu","Jan Berg","Theodoros Kouzelis","Soham Deshmukh","Minkyu Kim"]],["Nb params",[171.0,171.0,105.0,108.0,494.0,150.0,117.0,14.0,117.0,25.0,10.0,4.0,380.0,312.0,205.0]],["SPIDEr",[0.444,0.485,0.472,0.475,0.465,0.369,0.42,0.37,0.442,0.419,0.414,0.318,0.403,0.455,0.455]],["Train data",["AC","AC+WC","AC","AC","AC","AC","AC","AC","AC","AC","AC","AC","AC+WC","AC+WC","AC"]],["Test data",["AC","AC","AC","AC","AC","AC","AC","AC","AC","AC","AC","AC","AC","AC","AC"]],["Pretrained",["audio+text","audio+text","audio","audio","audio+text","audio","audio","audio","audio","audio+text","audio","none","audio","audio+text","audio+text"]],["Nb mod\u00e8les",[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]],["RL",[false,false,false,false,false,false,false,false,false,false,false,false,false,false,false]],["Year",["2023","2023","2023","2022","2021","2019","2021","2022","2023","2022","2021","2021","2023","2023","2023"]],["Archi",["trans-trans","trans-trans","trans-trans","trans-trans","cnn-trans","cnn-rnn","trans-trans","cnn-trans","trans-trans","cnn-trans","cnn-rnn","trans-trans","cnn-trans","trans-trans","cnn-trans"]],["Bibtex",["","","","","@inproceedings{Gontier2021, title = {Automated Audio Captioning by Fine-Tuning BART with AudioSet Tags}, author = {Gontier, Felix and Serizel, Romain and Cerisara, Christophe}, year = 2021, month = {November}, booktitle = {Proceedings of the 6th Detection and Classification of Acoustic Scenes and Events 2021 Workshop (DCASE2021)}, address = {Barcelona, Spain}, pages = {170--174}, isbn = {978-84-09-36072-7}, abstract = {Automated audio captioning is the multimodal task of describing environmental audio recordings with fluent natural language. Most current methods utilize pre-trained analysis models to extract relevant semantic content from the audio input. However, prior information on language modeling is rarely introduced, and corresponding architectures are limited in capacity due to data scarcity. In this paper, we present a method leveraging the linguistic information contained in BART, a large-scale conditional language model with general purpose pre-training. The caption generation is conditioned on sequences of textual AudioSet tags. This input is enriched with temporally aligned audio embeddings that allows the model to improve the sound event recognition. The full BART architecture is fine-tuned with few additional parameters. Experimental results demonstrate that, beyond the scaling properties of the architecture, language-only pre-training improves the text quality in the multimodal setting of audio captioning. The best model achieves state-of-the-art performance on AudioCaps with 46.5 SPIDEr.}, doi. = {10.5281/zenodo.5770113} }","","","","","","","","","",""]],["Optim",["Adam","Adam","Adam","AdamW","AdamW","Adam","Adam","?","Adam","Adam","Adam","Adam","Adam","Adam","AdamW"]],["ensemble",["Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single"]],["extra_train",[false,true,false,false,false,false,false,false,false,false,false,false,true,true,false]],["color",["#70b1ff","#ff7270","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#ff7270","#ff7270","#70b1ff"]],["marker",["circle","circle","diamond","diamond","circle","diamond","diamond","diamond","diamond","circle","diamond","triangle","diamond","circle","circle"]],["is_pareto",[false,true,true,true,false,false,false,false,false,true,true,true,false,false,false]],["pareto_idx",["none",1,2,3,"none","none","none","none","none",4,5,6,"none","none","none"]],["size",[13.076696830622021,13.076696830622021,10.246950765959598,10.392304845413264,22.22611077089287,12.24744871391589,10.816653826391969,3.7416573867739413,10.816653826391969,5.0,3.1622776601683795,2.0,19.493588689617926,17.663521732655695,14.317821063276353]],["params_display",["171M","171M","105M","108M","494M","150M","117M","14M","117M","25M","10M","4M","380M","312M","205M"]],["params_xoff",[-28.53834841531101,-28.53834841531101,-27.123475382979798,-27.196152422706632,-33.11305538544644,-28.123724356957943,-27.408326913195985,-23.870828693386972,-27.408326913195985,-24.5,-23.58113883008419,-23.0,-31.74679434480896,-30.831760866327848,-29.158910531638178]],["params_yoff",[-11.53834841531101,-11.53834841531101,-10.123475382979798,-10.196152422706632,-16.113055385446437,-11.123724356957945,-10.408326913195985,-6.87082869338697,-10.408326913195985,-7.5,-6.58113883008419,-6.0,-14.746794344808963,-13.831760866327848,-12.158910531638178]]]}}},"view":{"id":"p256822"},"glyph":{"type":"object","name":"Text","id":"p256890","attributes":{"x":{"type":"field","field":"Year"},"y":{"type":"field","field":"SPIDEr"},"text":{"type":"field","field":"params_display"},"x_offset":{"type":"field","field":"params_xoff"},"y_offset":{"type":"field","field":"params_yoff"},"text_color":{"type":"value","value":"#ff7270"},"text_font_size":{"type":"value","value":"18px"},"text_font_style":{"type":"value","value":"bold"},"text_align":{"type":"value","value":"center"},"text_baseline":{"type":"value","value":"middle"}}},"nonselection_glyph":{"type":"object","name":"Text","id":"p256891","attributes":{"x":{"type":"field","field":"Year"},"y":{"type":"field","field":"SPIDEr"},"text":{"type":"field","field":"params_display"},"x_offset":{"type":"field","field":"params_xoff"},"y_offset":{"type":"field","field":"params_yoff"},"text_color":{"type":"value","value":"#ff7270"},"text_alpha":{"type":"value","value":0.1},"text_font_size":{"type":"value","value":"18px"},"text_font_style":{"type":"value","value":"bold"},"text_align":{"type":"value","value":"center"},"text_baseline":{"type":"value","value":"middle"}}},"muted_glyph":{"type":"object","name":"Text","id":"p256892","attributes":{"x":{"type":"field","field":"Year"},"y":{"type":"field","field":"SPIDEr"},"text":{"type":"field","field":"params_display"},"x_offset":{"type":"field","field":"params_xoff"},"y_offset":{"type":"field","field":"params_yoff"},"text_color":{"type":"value","value":"#ff7270"},"text_alpha":{"type":"value","value":0.2},"text_font_size":{"type":"value","value":"18px"},"text_font_style":{"type":"value","value":"bold"},"text_align":{"type":"value","value":"center"},"text_baseline":{"type":"value","value":"middle"}}}}},{"type":"object","name":"GlyphRenderer","id":"p256906","attributes":{"visible":false,"data_source":{"type":"object","name":"ColumnDataSource","id":"p256900","attributes":{"selected":{"type":"object","name":"Selection","id":"p256901","attributes":{"indices":[],"line_indices":[]}},"selection_policy":{"type":"object","name":"UnionRenderers","id":"p256902"},"data":{"type":"map","entries":[["x",[0]],["y",[0]]]}}},"view":{"type":"object","name":"CDSView","id":"p256907","attributes":{"filter":{"type":"object","name":"AllIndices","id":"p256908"}}},"glyph":{"type":"object","name":"Scatter","id":"p256903","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"fill_color":{"type":"value","value":"#ff7270"},"hatch_color":{"type":"value","value":"#ff7270"}}},"nonselection_glyph":{"type":"object","name":"Scatter","id":"p256904","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"line_alpha":{"type":"value","value":0.1},"fill_color":{"type":"value","value":"#ff7270"},"fill_alpha":{"type":"value","value":0.1},"hatch_color":{"type":"value","value":"#ff7270"},"hatch_alpha":{"type":"value","value":0.1}}},"muted_glyph":{"type":"object","name":"Scatter","id":"p256905","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"line_alpha":{"type":"value","value":0.2},"fill_color":{"type":"value","value":"#ff7270"},"fill_alpha":{"type":"value","value":0.2},"hatch_color":{"type":"value","value":"#ff7270"},"hatch_alpha":{"type":"value","value":0.2}}}}},{"type":"object","name":"GlyphRenderer","id":"p256916","attributes":{"visible":false,"data_source":{"type":"object","name":"ColumnDataSource","id":"p256910","attributes":{"selected":{"type":"object","name":"Selection","id":"p256911","attributes":{"indices":[],"line_indices":[]}},"selection_policy":{"type":"object","name":"UnionRenderers","id":"p256912"},"data":{"type":"map","entries":[["x",[0]],["y",[0]]]}}},"view":{"type":"object","name":"CDSView","id":"p256917","attributes":{"filter":{"type":"object","name":"AllIndices","id":"p256918"}}},"glyph":{"type":"object","name":"Scatter","id":"p256913","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"fill_color":{"type":"value","value":"#70b1ff"},"hatch_color":{"type":"value","value":"#70b1ff"}}},"nonselection_glyph":{"type":"object","name":"Scatter","id":"p256914","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"line_alpha":{"type":"value","value":0.1},"fill_color":{"type":"value","value":"#70b1ff"},"fill_alpha":{"type":"value","value":0.1},"hatch_color":{"type":"value","value":"#70b1ff"},"hatch_alpha":{"type":"value","value":0.1}}},"muted_glyph":{"type":"object","name":"Scatter","id":"p256915","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"line_alpha":{"type":"value","value":0.2},"fill_color":{"type":"value","value":"#70b1ff"},"fill_alpha":{"type":"value","value":0.2},"hatch_color":{"type":"value","value":"#70b1ff"},"hatch_alpha":{"type":"value","value":0.2}}}}},{"type":"object","name":"GlyphRenderer","id":"p256927","attributes":{"visible":false,"data_source":{"type":"object","name":"ColumnDataSource","id":"p256921","attributes":{"selected":{"type":"object","name":"Selection","id":"p256922","attributes":{"indices":[],"line_indices":[]}},"selection_policy":{"type":"object","name":"UnionRenderers","id":"p256923"},"data":{"type":"map","entries":[["x",[0]],["y",[0]]]}}},"view":{"type":"object","name":"CDSView","id":"p256928","attributes":{"filter":{"type":"object","name":"AllIndices","id":"p256929"}}},"glyph":{"type":"object","name":"Scatter","id":"p256924","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"fill_color":{"type":"value","value":"grey"},"hatch_color":{"type":"value","value":"grey"}}},"nonselection_glyph":{"type":"object","name":"Scatter","id":"p256925","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"line_alpha":{"type":"value","value":0.1},"fill_color":{"type":"value","value":"grey"},"fill_alpha":{"type":"value","value":0.1},"hatch_color":{"type":"value","value":"grey"},"hatch_alpha":{"type":"value","value":0.1}}},"muted_glyph":{"type":"object","name":"Scatter","id":"p256926","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"line_alpha":{"type":"value","value":0.2},"fill_color":{"type":"value","value":"grey"},"fill_alpha":{"type":"value","value":0.2},"hatch_color":{"type":"value","value":"grey"},"hatch_alpha":{"type":"value","value":0.2}}}}},{"type":"object","name":"GlyphRenderer","id":"p256937","attributes":{"visible":false,"data_source":{"type":"object","name":"ColumnDataSource","id":"p256931","attributes":{"selected":{"type":"object","name":"Selection","id":"p256932","attributes":{"indices":[],"line_indices":[]}},"selection_policy":{"type":"object","name":"UnionRenderers","id":"p256933"},"data":{"type":"map","entries":[["x",[0]],["y",[0]]]}}},"view":{"type":"object","name":"CDSView","id":"p256938","attributes":{"filter":{"type":"object","name":"AllIndices","id":"p256939"}}},"glyph":{"type":"object","name":"Scatter","id":"p256934","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"fill_color":{"type":"value","value":"grey"},"hatch_color":{"type":"value","value":"grey"},"marker":{"type":"value","value":"diamond"}}},"nonselection_glyph":{"type":"object","name":"Scatter","id":"p256935","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"line_alpha":{"type":"value","value":0.1},"fill_color":{"type":"value","value":"grey"},"fill_alpha":{"type":"value","value":0.1},"hatch_color":{"type":"value","value":"grey"},"hatch_alpha":{"type":"value","value":0.1},"marker":{"type":"value","value":"diamond"}}},"muted_glyph":{"type":"object","name":"Scatter","id":"p256936","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"line_alpha":{"type":"value","value":0.2},"fill_color":{"type":"value","value":"grey"},"fill_alpha":{"type":"value","value":0.2},"hatch_color":{"type":"value","value":"grey"},"hatch_alpha":{"type":"value","value":0.2},"marker":{"type":"value","value":"diamond"}}}}},{"type":"object","name":"GlyphRenderer","id":"p256947","attributes":{"visible":false,"data_source":{"type":"object","name":"ColumnDataSource","id":"p256941","attributes":{"selected":{"type":"object","name":"Selection","id":"p256942","attributes":{"indices":[],"line_indices":[]}},"selection_policy":{"type":"object","name":"UnionRenderers","id":"p256943"},"data":{"type":"map","entries":[["x",[0]],["y",[0]]]}}},"view":{"type":"object","name":"CDSView","id":"p256948","attributes":{"filter":{"type":"object","name":"AllIndices","id":"p256949"}}},"glyph":{"type":"object","name":"Scatter","id":"p256944","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"fill_color":{"type":"value","value":"grey"},"hatch_color":{"type":"value","value":"grey"},"marker":{"type":"value","value":"triangle"}}},"nonselection_glyph":{"type":"object","name":"Scatter","id":"p256945","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"line_alpha":{"type":"value","value":0.1},"fill_color":{"type":"value","value":"grey"},"fill_alpha":{"type":"value","value":0.1},"hatch_color":{"type":"value","value":"grey"},"hatch_alpha":{"type":"value","value":0.1},"marker":{"type":"value","value":"triangle"}}},"muted_glyph":{"type":"object","name":"Scatter","id":"p256946","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"line_alpha":{"type":"value","value":0.2},"fill_color":{"type":"value","value":"grey"},"fill_alpha":{"type":"value","value":0.2},"hatch_color":{"type":"value","value":"grey"},"hatch_alpha":{"type":"value","value":0.2},"marker":{"type":"value","value":"triangle"}}}}}],"toolbar":{"type":"object","name":"Toolbar","id":"p256771","attributes":{"tools":[{"type":"object","name":"PanTool","id":"p256796"},{"type":"object","name":"WheelZoomTool","id":"p256797"},{"type":"object","name":"BoxZoomTool","id":"p256798","attributes":{"overlay":{"type":"object","name":"BoxAnnotation","id":"p256799","attributes":{"syncable":false,"level":"overlay","visible":false,"left_units":"canvas","right_units":"canvas","bottom_units":"canvas","top_units":"canvas","line_color":"black","line_alpha":1.0,"line_width":2,"line_dash":[4,4],"fill_color":"lightgrey","fill_alpha":0.5}}}},{"type":"object","name":"SaveTool","id":"p256800"},{"type":"object","name":"ResetTool","id":"p256801"},{"type":"object","name":"HelpTool","id":"p256802"},{"type":"object","name":"HoverTool","id":"p243513","attributes":{"renderers":"auto","tooltips":[["Title","@{Paper name}"],["Nb params","@{Nb params}"],["SPIDEr","@{SPIDEr}"],["Train data","@{Train data}"],["Pretrained","@{Pretrained}"]]}}]}},"left":[{"type":"object","name":"LinearAxis","id":"p256789","attributes":{"ticker":{"type":"object","name":"BasicTicker","id":"p256792","attributes":{"mantissas":[1,2,5]}},"formatter":{"type":"object","name":"BasicTickFormatter","id":"p256791"},"axis_label":"SPIDEr","axis_label_text_font_size":"20px","major_label_policy":{"type":"object","name":"AllLabels","id":"p256790"},"major_label_text_font_size":"20px"}}],"below":[{"type":"object","name":"LinearAxis","id":"p256782","attributes":{"ticker":{"type":"object","name":"FixedTicker","id":"p256896","attributes":{"ticks":[2019,2021,2022,2023],"minor_ticks":[]}},"formatter":{"type":"object","name":"BasicTickFormatter","id":"p256784"},"axis_label":"Year","axis_label_text_font_size":"20px","major_label_policy":{"type":"object","name":"AllLabels","id":"p256783"},"major_label_text_font_size":"20px"}}],"center":[{"type":"object","name":"Grid","id":"p256788","attributes":{"visible":false,"axis":{"id":"p256782"}}},{"type":"object","name":"Grid","id":"p256795","attributes":{"visible":false,"dimension":1,"axis":{"id":"p256789"}}},{"type":"object","name":"Legend","id":"p256920","attributes":{"location":[285,115],"title":"External data","title_text_font_size":"14px","label_text_font_size":"14px","padding":5,"items":[{"type":"object","name":"LegendItem","id":"p256909","attributes":{"label":{"type":"value","value":"yes"},"renderers":[{"id":"p256906"}],"index":1}},{"type":"object","name":"LegendItem","id":"p256919","attributes":{"label":{"type":"value","value":"no"},"renderers":[{"id":"p256916"}],"index":0}}]}},{"type":"object","name":"Legend","id":"p256951","attributes":{"location":"bottom_right","title":"Pretrained","title_text_font_size":"14px","label_text_font_size":"14px","padding":5,"items":[{"type":"object","name":"LegendItem","id":"p256930","attributes":{"label":{"type":"value","value":"audio+text"},"renderers":[{"id":"p256927"}],"index":0}},{"type":"object","name":"LegendItem","id":"p256940","attributes":{"label":{"type":"value","value":"audio"},"renderers":[{"id":"p256937"}],"index":2}},{"type":"object","name":"LegendItem","id":"p256950","attributes":{"label":{"type":"value","value":"none"},"renderers":[{"id":"p256947"}],"index":11}}]}}],"background_fill_color":null,"border_fill_color":null,"output_backend":"svg"}}],"callbacks":{"type":"map"}}}
    </script>
    <script type="text/javascript">
    (function() {
        const fn = function() {
        Bokeh.safely(function() {
            (function(root) {
            function embed_document(root) {
            const docs_json = document.getElementById('p263267').textContent;
            const render_items = [{"docid":"8d5f837b-1f08-411f-94ab-546c1da821f1","roots":{"p256765":"d1c8c8a7-74fd-4ef4-aee3-4cdb0ca0a7a1"},"root_ids":["p256765"]}];
            root.Bokeh.embed.embed_items(docs_json, render_items);
            }
            if (root.Bokeh !== undefined) {
                embed_document(root);
            } else {
                let attempts = 0;
                const timer = setInterval(function(root) {
                if (root.Bokeh !== undefined) {
                    clearInterval(timer);
                    embed_document(root);
                } else {
                    attempts++;
                    if (attempts > 100) {
                    clearInterval(timer);
                    console.log("Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing");
                    }
                }
                }, 10, root)
            }
            })(window);
        });
        };
        if (document.readyState != "loading") fn();
        else document.addEventListener("DOMContentLoaded", fn);
    })();
    </script>

    <h1>SPIDEr as the function of the years for the Clotho (CL) dataset.</h1>
    <div id="b430d07d-b027-46a9-8da8-b7db3b71398f" data-root-id="p252238" style="display: contents;"></div>

    <script type="application/json" id="p262737">
    {"7206771e-8b4a-412f-b975-efe307b53dbf":{"version":"3.1.1","title":"Bokeh Application","defs":[],"roots":[{"type":"object","name":"Figure","id":"p252238","attributes":{"width":500,"height":500,"x_range":{"type":"object","name":"Range1d","id":"p252371","attributes":{"start":2019.192,"end":2023.8092}},"y_range":{"type":"object","name":"Range1d","id":"p252372","attributes":{"start":0.15604,"end":0.35616000000000003}},"x_scale":{"type":"object","name":"LinearScale","id":"p252251"},"y_scale":{"type":"object","name":"LinearScale","id":"p252253"},"title":null,"outline_line_color":null,"renderers":[{"type":"object","name":"GlyphRenderer","id":"p252306","attributes":{"data_source":{"type":"object","name":"ColumnDataSource","id":"p252297","attributes":{"selected":{"type":"object","name":"Selection","id":"p252298","attributes":{"indices":[],"line_indices":[]}},"selection_policy":{"type":"object","name":"UnionRenderers","id":"p252299"},"data":{"type":"map","entries":[["#",["1","1","2","2","5","10","10","12","14","18","20","22","23","28","29","31","32","33","35","36","36","39","40","41","43","44","44","44","46","46","46","46"]],["Paper name",["BEATS-BASED AUDIO CAPTIONING MODEL WITH INSTRUCTOR EMBEDDING SUPERVISION AND CHATGPT MIX-UP","BEATS-BASED AUDIO CAPTIONING MODEL WITH INSTRUCTOR EMBEDDING SUPERVISION AND CHATGPT MIX-UP","WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research","WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research","CAU SUBMISSION TO DCASE 2021 TASK6: TRANSFORMER FOLLOWED BY TRANSFER LEARNING FOR AUDIO CAPTIONING","THE NTT DCASE2020 CHALLENGE TASK 6 SYSTEM: AUTOMATED AUDIO CAPTIONING WITH KEYWORDS AND SENTENCE LENGTH ESTIMATION","THE NTT DCASE2020 CHALLENGE TASK 6 SYSTEM: AUTOMATED AUDIO CAPTIONING WITH KEYWORDS AND SENTENCE LENGTH ESTIMATION","IMPROVING AUDIO CAPTIONING USING SEMANTIC SIMILARITY METRICS","AUTOMATED AUDIO CAPTIONING WITH KEYWORDS GUIDANCE","EFFICIENT AUDIO CAPTIONING TRANSFORMER WITH PATCHOUT AND TEXT GUIDANCE","INVESTIGATING LOCAL AND GLOBAL INFORMATION FOR AUTOMATED AUDIO CAPTIONING WITH TRANSFER LEARNING","WaveTransformer: A Novel Architecture for Audio Captioning Based on Learning Temporal and Time-Frequency Information","Automated Audio Captioning With Topic Modeling","AUTOMATED AUDIO CAPTIONING USING TRANSFER LEARNING AND RECONSTRUCTION LATENT SPACE SIMILARITY REGULARIZATION","IMPROVING THE PERFORMANCE OF AUTOMATED AUDIO CAPTIONING VIA INTEGRATING THE ACOUSTIC AND SEMANTIC INFORMATION","WEAKLY-SUPERVISED AUTOMATED AUDIO CAPTIONING VIA TEXT ONLY TRAINING","TRAINING AUDIO CAPTIONING MODELS WITHOUT AUDIO","PREFIX TUNING FOR AUTOMATED AUDIO CAPTIONING","AUTOMATED AUDIO CAPTIONING WITH MULTI-TASK LEARNING","CP-JKU\u2019S SUBMISSION TO TASK 6A OF THE DCASE2022 CHALLENGE: A BART ENCODER-DECODER FOR AUTOMATIC AUDIO CAPTIONING TRAINED VIA THE REINFORCE ALGORITHM AND TRANSFER LEARNING","CP-JKU\u2019S SUBMISSION TO TASK 6A OF THE DCASE2022 CHALLENGE: A BART ENCODER-DECODER FOR AUTOMATIC AUDIO CAPTIONING TRAINED VIA THE REINFORCE ALGORITHM AND TRANSFER LEARNING","PEACS: PREFIX ENCODING FOR AUDITORY CAPTION SYNTHESIS","AUDIO CAPTIONING BASED ON TRANSFORMER AND PRE-TRAINING FOR 2020 DCASE AUDIO CAPTIONING CHALLENGE","AUTOMATED AUDIO CAPTIONING WITH TEMPORAL ATTENTION","THE DCASE2021 CHALLENGE TASK 6 SYSTEM : AUTOMATED AUDIO CAPTION","A WHISPER TRANSFORMER FOR AUDIO CAPTIONING TRAINED WITH SYNTHETIC CAPTIONS AND TRANSFER LEARNING","A WHISPER TRANSFORMER FOR AUDIO CAPTIONING TRAINED WITH SYNTHETIC CAPTIONS AND TRANSFER LEARNING","A WHISPER TRANSFORMER FOR AUDIO CAPTIONING TRAINED WITH SYNTHETIC CAPTIONS AND TRANSFER LEARNING","EXPLORING AUDIO CAPTIONING WITH KEYWORD-GUIDED TEXT GENERATION","EXPLORING AUDIO CAPTIONING WITH KEYWORD-GUIDED TEXT GENERATION","EXPLORING AUDIO CAPTIONING WITH KEYWORD-GUIDED TEXT GENERATION","EXPLORING AUDIO CAPTIONING WITH KEYWORD-GUIDED TEXT GENERATION"]],["Link",["https://dcase.community/documents/challenge2023/technical_reports/DCASE2023_Wu_31_t6a.pdf","https://dcase.community/documents/challenge2023/technical_reports/DCASE2023_Wu_31_t6a.pdf","https://arxiv.org/pdf/2303.17395.pdf","https://arxiv.org/pdf/2303.17395.pdf","https://dcase.community/documents/challenge2021/technical_reports/DCASE2021_Won_103_t6.pdf","https://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Koizumi_63_t6.pdf","https://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Koizumi_63_t6.pdf","https://arxiv.org/pdf/2210.16470.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Mei_117_t6a.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Kouzelis_60_t6a.pdf","https://arxiv.org/pdf/2102.11457.pdf","https://arxiv.org/pdf/2010.11098.pdf","https://www.researchgate.net/publication/367063417_Automated_Audio_Captioning_with_Topic_Modeling/fulltext/63c003f33fcb6855ce7dc192/Automated-Audio-Captioning-With-Topic-Modeling.pdf?origin=publicationDetail&amp;_sg%5B0%5D=dkU_nXJBP2xbhG7xq1Kpptu2LS2Uiq-Fo5RM6TaN7kL5JHW8WO8EUF0hcE4lf-4L7ZQKXRdEVd61EQPaMhGQtw.S1AXevlPqorLIQ_q4bs-6w49K76AG2QOJO7tgbPDXZqZzD4m4H_wzSHQ8AcgNnJngfn4funEYEqMGo4AnRtz2w&amp;_sg%5B1%5D=reAe0vy9g4yvBWpyUKSyvVlseW88gNcN8Dx8LZgBRXPgb8dervQNWpA1cZAZNCUzRlKWai2NkbcrTfj76stgeMdXhgB76EPmstMSZGyRzigi.S1AXevlPqorLIQ_q4bs-6w49K76AG2QOJO7tgbPDXZqZzD4m4H_wzSHQ8AcgNnJngfn4funEYEqMGo4AnRtz2w&amp;_iepl=&amp;_rtd=eyJjb250ZW50SW50ZW50IjoibWFpbkl0ZW0ifQ%3D%3D&amp;_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6Il9kaXJlY3QiLCJwYWdlIjoiX2RpcmVjdCIsInBvc2l0aW9uIjoicGFnZUhlYWRlciJ9fQ","https://arxiv.org/pdf/2108.04692.pdf","https://arxiv.org/pdf/2110.06100.pdf","https://arxiv.org/pdf/2309.12242.pdf","https://arxiv.org/pdf/2309.07372.pdf","https://arxiv.org/pdf/2303.17489.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Zou_37_t6a.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Primus_97_t6a.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Primus_97_t6a.pdf","https://dcase.community/documents/challenge2023/technical_reports/DCASE2023_Schaumloeffel_107_t6a.pdf","https://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Wu_136_t6.pdf","https://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Wang_5_t6.pdf","https://dcase.community/documents/challenge2021/technical_reports/DCASE2021_Yang_76_t6.pdf","https://dcase.community/documents/challenge2023/technical_reports/DCASE2023_Kadlcik_68_t6a.pdf","https://dcase.community/documents/challenge2023/technical_reports/DCASE2023_Kadlcik_68_t6a.pdf","https://dcase.community/documents/challenge2023/technical_reports/DCASE2023_Kadlcik_68_t6a.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Kicinski_115_t6a.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Kicinski_115_t6a.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Kicinski_115_t6a.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Kicinski_115_t6a.pdf"]],["First Author",["Shih-Lun Wu","Shih-Lun Wu","Xinhao Mei","Xinhao Mei","Hyejin Won","Yuma Koizumi","Yuma Koizumi","Rehana Mahfuz","Xinhao Mei","Thodoris Kouzelis","Xuenan Xu","An Tran","Xinhao Mei","Andrew Koh","Zhongjie ye","Theodoros Kouzelis","Soham Deshmukh","Minkyu Kim","Zhongjie Ye","Paul Primus","Paul Primus","Timothy Schauml\u00a8offel","Yusong Wu","Helin Wang","Liu Yang","Marek Kadlc\u0131k","Marek Kadlc\u0131k","Marek Kadlc\u0131k","Dawid Kici\u00b4nski","Dawid Kici\u00b4nski","Dawid Kici\u00b4nski","Dawid Kici\u00b4nski"]],["Nb params",[1600.0,3500.0,220.0,220.0,88.0,82.5,1.0,14.0,8.0,560.0,10.0,4.0,222.0,10.0,80.0,380.0,312.0,205.0,86.0,130.0,130.0,248.0,8.0,12.0,3.0,39.0,244.0,1000.0,104.0,207.0,104.0,207.0]],["SPIDEr",[0.326,0.336,0.255,0.31,0.285,0.207,0.196,0.208,0.266,0.296,0.246,0.182,0.264,0.246,0.269,0.247,0.261,0.255,0.286,0.264,0.284,0.292,0.227,0.172,0.166,0.224,0.269,0.279,0.279,0.255,0.26,0.249]],["Train data",["AC+CL","AC+CL","CL","CL+WC","CL","CL","CL","CL","CL","AC+CL+MA","CL","CL","CL","CL","CL","CL+WC","CL+WC","CL","CL","CL","AC+CL","AC+CL+MA+SD+WT","CL","CL","CL","AC+AS+CL","AC+AS+CL","AC+AS+CL","AC+CL+FSD","AC+CL+FSD","AC+CL+FSD","AC+CL+FSD"]],["Test data",["CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL"]],["Pretrained",["audio+text","audio+text","audio+text","audio+text","audio","none","none","audio","audio","audio","audio","none","audio+text","audio","audio","audio","audio+text","audio+text","audio","audio+text","audio+text","audio+text","none","none","none","audio+text","audio+text","audio+text","audio","audio+text","audio","audio+text"]],["Nb mod\u00e8les",[1,20,1,1,1,50,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]],["RL",[false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false]],["Year",["2023","2023","2023","2023","2021","2020","2020","2022","2022","2022","2021","2020","2023","2021","2021","2023","2023","2023","2022","2022","2022","2023","2020","2020","2021","2023","2023","2023","2022","2022","2022","2022"]],["Archi",["trans-trans","trans-trans","cnn-trans","cnn-trans","cnn-trans","cnn-rnn","cnn-rnn","cnn-trans","cnn-trans","trans-trans","cnn-rnn","trans-trans","cnn-trans","cnn-trans","cnn-rnn","cnn-trans","trans-trans","cnn-trans","cnn-rnn","cnn-trans","cnn-trans","cnn-trans","cnn-trans","cnn-rnn","cnn-trans","trans-trans","trans-trans","trans-trans","cnn-trans","cnn-trans","cnn-trans","cnn-trans"]],["Bibtex",["","","","","","@techreport{koizumi2020_t1, title = {The {NTT} {DCASE2020} Challenge Task 6 System: Automated Audio Captioning With Keywords and Sentence Length Estimation}, author = {Koizumi, Yuma and Takeuchi, Daiki and Ohishi, Yasunori and Harada, Noboru and Kashino, Kunio}, year = 2020, month = {June}, institution = {DCASE2020 Challenge}, abstract = {This technical report describes the system participating to the Detection and Classification of Acoustic Scenes and Events (DCASE) 2020 Challenge, Task 6: automated audio captioning. Our submission focuses on solving two indeterminacy problems in automated audio captioning: word selection indeterminacy and sentence length indeterminacy. We simultaneously solve the main caption generation and sub indeterminacy problems by estimating keywords and sentence length through multi-task learning. We tested a simplified model of our submission using the development-testing dataset. Our model achieved 20.7 SPIDEr score where that of the baseline system was 5.4.} }","@techreport{koizumi2020_t1, title = {The {NTT} {DCASE2020} Challenge Task 6 System: Automated Audio Captioning With Keywords and Sentence Length Estimation}, author = {Koizumi, Yuma and Takeuchi, Daiki and Ohishi, Yasunori and Harada, Noboru and Kashino, Kunio}, year = 2020, month = {June}, institution = {DCASE2020 Challenge}, abstract = {This technical report describes the system participating to the Detection and Classification of Acoustic Scenes and Events (DCASE) 2020 Challenge, Task 6: automated audio captioning. Our submission focuses on solving two indeterminacy problems in automated audio captioning: word selection indeterminacy and sentence length indeterminacy. We simultaneously solve the main caption generation and sub indeterminacy problems by estimating keywords and sentence length through multi-task learning. We tested a simplified model of our submission using the development-testing dataset. Our model achieved 20.7 SPIDEr score where that of the baseline system was 5.4.} }","","","","","","","","","","","","","","","@techreport{schaumloeffel2023_t6a, title = {PEACS: Prefix encoding for auditory caption synthesis}, author = {Schauml\u00f6ffel, Timothy and Vilas, Martina G. and Roig, Gemma}, year = 2023, month = {May}, institution = {DCASE2023 Challenge}, abstract = {This technical report describes an Automated Audio Captioning system for the Detection and Classification of Acoustic Scenes and Events (DCASE) 2023 Challenge, Task 6a (automated audio captioning). Our approach employs an encoder-decoder architecture, with the encoder utilizing a large contrastive pre-trained HTS-AT capable of handling variable-length audio segments. The decoder is based on the GPT2 model. To incorporate audio into the decoding process, we employ a light mapping network that translates audio representations into a prefix, effectively guiding the decoder\u2019s generation process. Given the limited data availability, we pre-train our model on various audio captioning datasets and fine-tune it on Clotho. We reach a SPIDERr-FL score of 29.3 on the evaluation split of the Clotho-v2 dataset.} }","","","","","","","@techreport{kicinski2022_t6a, title = {Exploring audio captioning with keyword-guided text generation}, author = {Kicinski, Dawid and de Gail, Teodor Lamort and Bujnowski, Pawel}, year = 2022, month = {July}, institution = {DCASE2022 Challenge}, abstract = {This technical report describes our submission to the DCASE 2022 challenge, Task 6 A: automated audio captioning. In our system, we explore the use of pre-trained language models for the audio captioning task. The proposed system is an encoder-decoder architecture consisting of a pre-trained PANN encoder and a GPT2 decoder. Audio embeddings are encoded to language model prompts using a simple mapping network. We further develop our system by employing strategies of guiding the decoder with textual information. We prompt the decoder with keywords extracted from semantically similar audios and use them to choose the best matching caption by their occurrence.} }","@techreport{kicinski2022_t6a, title = {Exploring audio captioning with keyword-guided text generation}, author = {Kicinski, Dawid and de Gail, Teodor Lamort and Bujnowski, Pawel}, year = 2022, month = {July}, institution = {DCASE2022 Challenge}, abstract = {This technical report describes our submission to the DCASE 2022 challenge, Task 6 A: automated audio captioning. In our system, we explore the use of pre-trained language models for the audio captioning task. The proposed system is an encoder-decoder architecture consisting of a pre-trained PANN encoder and a GPT2 decoder. Audio embeddings are encoded to language model prompts using a simple mapping network. We further develop our system by employing strategies of guiding the decoder with textual information. We prompt the decoder with keywords extracted from semantically similar audios and use them to choose the best matching caption by their occurrence.} }","@techreport{kicinski2022_t6a, title = {Exploring audio captioning with keyword-guided text generation}, author = {Kicinski, Dawid and de Gail, Teodor Lamort and Bujnowski, Pawel}, year = 2022, month = {July}, institution = {DCASE2022 Challenge}, abstract = {This technical report describes our submission to the DCASE 2022 challenge, Task 6 A: automated audio captioning. In our system, we explore the use of pre-trained language models for the audio captioning task. The proposed system is an encoder-decoder architecture consisting of a pre-trained PANN encoder and a GPT2 decoder. Audio embeddings are encoded to language model prompts using a simple mapping network. We further develop our system by employing strategies of guiding the decoder with textual information. We prompt the decoder with keywords extracted from semantically similar audios and use them to choose the best matching caption by their occurrence.} }","@techreport{kicinski2022_t6a, title = {Exploring audio captioning with keyword-guided text generation}, author = {Kicinski, Dawid and de Gail, Teodor Lamort and Bujnowski, Pawel}, year = 2022, month = {July}, institution = {DCASE2022 Challenge}, abstract = {This technical report describes our submission to the DCASE 2022 challenge, Task 6 A: automated audio captioning. In our system, we explore the use of pre-trained language models for the audio captioning task. The proposed system is an encoder-decoder architecture consisting of a pre-trained PANN encoder and a GPT2 decoder. Audio embeddings are encoded to language model prompts using a simple mapping network. We further develop our system by employing strategies of guiding the decoder with textual information. We prompt the decoder with keywords extracted from semantically similar audios and use them to choose the best matching caption by their occurrence.} }"]],["Optim",["AdamW","AdamW","Adam","Adam","Adam","AdamW","AdamW","?","?","?","Adam","Adam","AdamW","?","Adam","Adam","Adam","AdamW","Adam","Adam","Adam","AdamW","?","Adam","Adam","?","?","?","AdamW","AdamW","AdamW","AdamW"]],["ensemble",["Single","Ensemble","Single","Single","Single","Ensemble","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single"]],["extra_train",[true,true,false,true,false,false,false,false,false,true,false,false,false,false,false,true,true,false,false,false,true,true,false,false,false,true,true,true,true,true,true,true]],["color",["#ff7270","#ff7270","#70b1ff","#ff7270","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#ff7270","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#ff7270","#ff7270","#70b1ff","#70b1ff","#70b1ff","#ff7270","#ff7270","#70b1ff","#70b1ff","#70b1ff","#ff7270","#ff7270","#ff7270","#ff7270","#ff7270","#ff7270","#ff7270"]],["marker",["circle","circle","circle","circle","diamond","triangle","triangle","diamond","diamond","diamond","diamond","triangle","circle","diamond","diamond","diamond","circle","circle","diamond","circle","circle","circle","triangle","triangle","triangle","circle","circle","circle","diamond","circle","diamond","circle"]],["is_pareto",[true,true,false,true,false,false,true,false,true,false,false,false,false,false,true,false,false,false,true,false,false,false,false,false,false,false,false,false,false,false,false,false]],["pareto_idx",[1,1,"none",2,"none","none",3,"none",4,"none","none","none","none","none",5,"none","none","none",6,"none","none","none","none","none","none","none","none","none","none","none","none","none"]],["size",[40.0,59.16079783099616,14.832396974191326,14.832396974191326,9.38083151964686,9.082951062292475,1.0,3.7416573867739413,2.8284271247461903,23.664319132398465,3.1622776601683795,2.0,14.89966442575134,3.1622776601683795,8.94427190999916,19.493588689617926,17.663521732655695,14.317821063276353,9.273618495495704,11.40175425099138,11.40175425099138,15.748015748023622,2.8284271247461903,3.4641016151377544,1.7320508075688772,6.244997998398398,15.620499351813308,31.622776601683793,10.198039027185569,14.38749456993816,10.198039027185569,14.38749456993816]],["params_display",["1600M","3500M","220M","220M","88M","82M","1M","14M","8M","560M","10M","4M","222M","10M","80M","380M","312M","205M","86M","130M","130M","248M","8M","12M","3M","39M","244M","1000M","104M","207M","104M","207M"]],["params_xoff",[-42.0,-51.58039891549808,-29.416198487095663,-29.416198487095663,-26.69041575982343,-26.541475531146236,-22.5,-23.870828693386972,-23.414213562373096,-33.83215956619923,-23.58113883008419,-23.0,-29.44983221287567,-23.58113883008419,-26.47213595499958,-31.74679434480896,-30.831760866327848,-29.158910531638178,-26.636809247747852,-27.70087712549569,-27.70087712549569,-29.87400787401181,-23.414213562373096,-23.73205080756888,-22.866025403784437,-25.1224989991992,-29.810249675906654,-37.8113883008419,-27.099019513592786,-29.193747284969078,-27.099019513592786,-29.193747284969078]],["params_yoff",[-25.0,-34.58039891549808,-12.416198487095663,-12.416198487095663,-9.69041575982343,-9.541475531146236,-5.5,-6.87082869338697,-6.414213562373095,-16.83215956619923,-6.58113883008419,-6.0,-12.44983221287567,-6.58113883008419,-9.47213595499958,-14.746794344808963,-13.831760866327848,-12.158910531638178,-9.636809247747852,-10.70087712549569,-10.70087712549569,-12.874007874011811,-6.414213562373095,-6.732050807568877,-5.866025403784438,-8.122498999199198,-12.810249675906654,-20.811388300841898,-10.099019513592784,-12.19374728496908,-10.099019513592784,-12.19374728496908]]]}}},"view":{"type":"object","name":"CDSView","id":"p252286","attributes":{"filter":{"type":"object","name":"IndexFilter","id":"p252285","attributes":{"indices":[5,6,11,23,10,13,14,24,7,8,19,2,17]}}}},"glyph":{"type":"object","name":"Scatter","id":"p252303","attributes":{"x":{"type":"field","field":"Year"},"y":{"type":"field","field":"SPIDEr"},"size":{"type":"field","field":"size"},"line_alpha":{"type":"value","value":0.15},"fill_color":{"type":"value","value":"#70b1ff"},"fill_alpha":{"type":"value","value":0.15},"hatch_color":{"type":"value","value":"#70b1ff"},"marker":{"type":"field","field":"marker"}}},"nonselection_glyph":{"type":"object","name":"Scatter","id":"p252304","attributes":{"x":{"type":"field","field":"Year"},"y":{"type":"field","field":"SPIDEr"},"size":{"type":"field","field":"size"},"line_alpha":{"type":"value","value":0.1},"fill_color":{"type":"value","value":"#70b1ff"},"fill_alpha":{"type":"value","value":0.1},"hatch_color":{"type":"value","value":"#70b1ff"},"hatch_alpha":{"type":"value","value":0.1},"marker":{"type":"field","field":"marker"}}},"muted_glyph":{"type":"object","name":"Scatter","id":"p252305","attributes":{"x":{"type":"field","field":"Year"},"y":{"type":"field","field":"SPIDEr"},"size":{"type":"field","field":"size"},"line_alpha":{"type":"value","value":0.2},"fill_color":{"type":"value","value":"#70b1ff"},"fill_alpha":{"type":"value","value":0.2},"hatch_color":{"type":"value","value":"#70b1ff"},"hatch_alpha":{"type":"value","value":0.2},"marker":{"type":"field","field":"marker"}}}}},{"type":"object","name":"GlyphRenderer","id":"p252318","attributes":{"data_source":{"type":"object","name":"ColumnDataSource","id":"p252309","attributes":{"selected":{"type":"object","name":"Selection","id":"p252310","attributes":{"indices":[],"line_indices":[]}},"selection_policy":{"type":"object","name":"UnionRenderers","id":"p252311"},"data":{"type":"map","entries":[["#",["1","1","2","2","5","10","10","12","14","18","20","22","23","28","29","31","32","33","35","36","36","39","40","41","43","44","44","44","46","46","46","46"]],["Paper name",["BEATS-BASED AUDIO CAPTIONING MODEL WITH INSTRUCTOR EMBEDDING SUPERVISION AND CHATGPT MIX-UP","BEATS-BASED AUDIO CAPTIONING MODEL WITH INSTRUCTOR EMBEDDING SUPERVISION AND CHATGPT MIX-UP","WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research","WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research","CAU SUBMISSION TO DCASE 2021 TASK6: TRANSFORMER FOLLOWED BY TRANSFER LEARNING FOR AUDIO CAPTIONING","THE NTT DCASE2020 CHALLENGE TASK 6 SYSTEM: AUTOMATED AUDIO CAPTIONING WITH KEYWORDS AND SENTENCE LENGTH ESTIMATION","THE NTT DCASE2020 CHALLENGE TASK 6 SYSTEM: AUTOMATED AUDIO CAPTIONING WITH KEYWORDS AND SENTENCE LENGTH ESTIMATION","IMPROVING AUDIO CAPTIONING USING SEMANTIC SIMILARITY METRICS","AUTOMATED AUDIO CAPTIONING WITH KEYWORDS GUIDANCE","EFFICIENT AUDIO CAPTIONING TRANSFORMER WITH PATCHOUT AND TEXT GUIDANCE","INVESTIGATING LOCAL AND GLOBAL INFORMATION FOR AUTOMATED AUDIO CAPTIONING WITH TRANSFER LEARNING","WaveTransformer: A Novel Architecture for Audio Captioning Based on Learning Temporal and Time-Frequency Information","Automated Audio Captioning With Topic Modeling","AUTOMATED AUDIO CAPTIONING USING TRANSFER LEARNING AND RECONSTRUCTION LATENT SPACE SIMILARITY REGULARIZATION","IMPROVING THE PERFORMANCE OF AUTOMATED AUDIO CAPTIONING VIA INTEGRATING THE ACOUSTIC AND SEMANTIC INFORMATION","WEAKLY-SUPERVISED AUTOMATED AUDIO CAPTIONING VIA TEXT ONLY TRAINING","TRAINING AUDIO CAPTIONING MODELS WITHOUT AUDIO","PREFIX TUNING FOR AUTOMATED AUDIO CAPTIONING","AUTOMATED AUDIO CAPTIONING WITH MULTI-TASK LEARNING","CP-JKU\u2019S SUBMISSION TO TASK 6A OF THE DCASE2022 CHALLENGE: A BART ENCODER-DECODER FOR AUTOMATIC AUDIO CAPTIONING TRAINED VIA THE REINFORCE ALGORITHM AND TRANSFER LEARNING","CP-JKU\u2019S SUBMISSION TO TASK 6A OF THE DCASE2022 CHALLENGE: A BART ENCODER-DECODER FOR AUTOMATIC AUDIO CAPTIONING TRAINED VIA THE REINFORCE ALGORITHM AND TRANSFER LEARNING","PEACS: PREFIX ENCODING FOR AUDITORY CAPTION SYNTHESIS","AUDIO CAPTIONING BASED ON TRANSFORMER AND PRE-TRAINING FOR 2020 DCASE AUDIO CAPTIONING CHALLENGE","AUTOMATED AUDIO CAPTIONING WITH TEMPORAL ATTENTION","THE DCASE2021 CHALLENGE TASK 6 SYSTEM : AUTOMATED AUDIO CAPTION","A WHISPER TRANSFORMER FOR AUDIO CAPTIONING TRAINED WITH SYNTHETIC CAPTIONS AND TRANSFER LEARNING","A WHISPER TRANSFORMER FOR AUDIO CAPTIONING TRAINED WITH SYNTHETIC CAPTIONS AND TRANSFER LEARNING","A WHISPER TRANSFORMER FOR AUDIO CAPTIONING TRAINED WITH SYNTHETIC CAPTIONS AND TRANSFER LEARNING","EXPLORING AUDIO CAPTIONING WITH KEYWORD-GUIDED TEXT GENERATION","EXPLORING AUDIO CAPTIONING WITH KEYWORD-GUIDED TEXT GENERATION","EXPLORING AUDIO CAPTIONING WITH KEYWORD-GUIDED TEXT GENERATION","EXPLORING AUDIO CAPTIONING WITH KEYWORD-GUIDED TEXT GENERATION"]],["Link",["https://dcase.community/documents/challenge2023/technical_reports/DCASE2023_Wu_31_t6a.pdf","https://dcase.community/documents/challenge2023/technical_reports/DCASE2023_Wu_31_t6a.pdf","https://arxiv.org/pdf/2303.17395.pdf","https://arxiv.org/pdf/2303.17395.pdf","https://dcase.community/documents/challenge2021/technical_reports/DCASE2021_Won_103_t6.pdf","https://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Koizumi_63_t6.pdf","https://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Koizumi_63_t6.pdf","https://arxiv.org/pdf/2210.16470.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Mei_117_t6a.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Kouzelis_60_t6a.pdf","https://arxiv.org/pdf/2102.11457.pdf","https://arxiv.org/pdf/2010.11098.pdf","https://www.researchgate.net/publication/367063417_Automated_Audio_Captioning_with_Topic_Modeling/fulltext/63c003f33fcb6855ce7dc192/Automated-Audio-Captioning-With-Topic-Modeling.pdf?origin=publicationDetail&amp;_sg%5B0%5D=dkU_nXJBP2xbhG7xq1Kpptu2LS2Uiq-Fo5RM6TaN7kL5JHW8WO8EUF0hcE4lf-4L7ZQKXRdEVd61EQPaMhGQtw.S1AXevlPqorLIQ_q4bs-6w49K76AG2QOJO7tgbPDXZqZzD4m4H_wzSHQ8AcgNnJngfn4funEYEqMGo4AnRtz2w&amp;_sg%5B1%5D=reAe0vy9g4yvBWpyUKSyvVlseW88gNcN8Dx8LZgBRXPgb8dervQNWpA1cZAZNCUzRlKWai2NkbcrTfj76stgeMdXhgB76EPmstMSZGyRzigi.S1AXevlPqorLIQ_q4bs-6w49K76AG2QOJO7tgbPDXZqZzD4m4H_wzSHQ8AcgNnJngfn4funEYEqMGo4AnRtz2w&amp;_iepl=&amp;_rtd=eyJjb250ZW50SW50ZW50IjoibWFpbkl0ZW0ifQ%3D%3D&amp;_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6Il9kaXJlY3QiLCJwYWdlIjoiX2RpcmVjdCIsInBvc2l0aW9uIjoicGFnZUhlYWRlciJ9fQ","https://arxiv.org/pdf/2108.04692.pdf","https://arxiv.org/pdf/2110.06100.pdf","https://arxiv.org/pdf/2309.12242.pdf","https://arxiv.org/pdf/2309.07372.pdf","https://arxiv.org/pdf/2303.17489.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Zou_37_t6a.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Primus_97_t6a.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Primus_97_t6a.pdf","https://dcase.community/documents/challenge2023/technical_reports/DCASE2023_Schaumloeffel_107_t6a.pdf","https://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Wu_136_t6.pdf","https://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Wang_5_t6.pdf","https://dcase.community/documents/challenge2021/technical_reports/DCASE2021_Yang_76_t6.pdf","https://dcase.community/documents/challenge2023/technical_reports/DCASE2023_Kadlcik_68_t6a.pdf","https://dcase.community/documents/challenge2023/technical_reports/DCASE2023_Kadlcik_68_t6a.pdf","https://dcase.community/documents/challenge2023/technical_reports/DCASE2023_Kadlcik_68_t6a.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Kicinski_115_t6a.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Kicinski_115_t6a.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Kicinski_115_t6a.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Kicinski_115_t6a.pdf"]],["First Author",["Shih-Lun Wu","Shih-Lun Wu","Xinhao Mei","Xinhao Mei","Hyejin Won","Yuma Koizumi","Yuma Koizumi","Rehana Mahfuz","Xinhao Mei","Thodoris Kouzelis","Xuenan Xu","An Tran","Xinhao Mei","Andrew Koh","Zhongjie ye","Theodoros Kouzelis","Soham Deshmukh","Minkyu Kim","Zhongjie Ye","Paul Primus","Paul Primus","Timothy Schauml\u00a8offel","Yusong Wu","Helin Wang","Liu Yang","Marek Kadlc\u0131k","Marek Kadlc\u0131k","Marek Kadlc\u0131k","Dawid Kici\u00b4nski","Dawid Kici\u00b4nski","Dawid Kici\u00b4nski","Dawid Kici\u00b4nski"]],["Nb params",[1600.0,3500.0,220.0,220.0,88.0,82.5,1.0,14.0,8.0,560.0,10.0,4.0,222.0,10.0,80.0,380.0,312.0,205.0,86.0,130.0,130.0,248.0,8.0,12.0,3.0,39.0,244.0,1000.0,104.0,207.0,104.0,207.0]],["SPIDEr",[0.326,0.336,0.255,0.31,0.285,0.207,0.196,0.208,0.266,0.296,0.246,0.182,0.264,0.246,0.269,0.247,0.261,0.255,0.286,0.264,0.284,0.292,0.227,0.172,0.166,0.224,0.269,0.279,0.279,0.255,0.26,0.249]],["Train data",["AC+CL","AC+CL","CL","CL+WC","CL","CL","CL","CL","CL","AC+CL+MA","CL","CL","CL","CL","CL","CL+WC","CL+WC","CL","CL","CL","AC+CL","AC+CL+MA+SD+WT","CL","CL","CL","AC+AS+CL","AC+AS+CL","AC+AS+CL","AC+CL+FSD","AC+CL+FSD","AC+CL+FSD","AC+CL+FSD"]],["Test data",["CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL"]],["Pretrained",["audio+text","audio+text","audio+text","audio+text","audio","none","none","audio","audio","audio","audio","none","audio+text","audio","audio","audio","audio+text","audio+text","audio","audio+text","audio+text","audio+text","none","none","none","audio+text","audio+text","audio+text","audio","audio+text","audio","audio+text"]],["Nb mod\u00e8les",[1,20,1,1,1,50,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]],["RL",[false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false]],["Year",["2023","2023","2023","2023","2021","2020","2020","2022","2022","2022","2021","2020","2023","2021","2021","2023","2023","2023","2022","2022","2022","2023","2020","2020","2021","2023","2023","2023","2022","2022","2022","2022"]],["Archi",["trans-trans","trans-trans","cnn-trans","cnn-trans","cnn-trans","cnn-rnn","cnn-rnn","cnn-trans","cnn-trans","trans-trans","cnn-rnn","trans-trans","cnn-trans","cnn-trans","cnn-rnn","cnn-trans","trans-trans","cnn-trans","cnn-rnn","cnn-trans","cnn-trans","cnn-trans","cnn-trans","cnn-rnn","cnn-trans","trans-trans","trans-trans","trans-trans","cnn-trans","cnn-trans","cnn-trans","cnn-trans"]],["Bibtex",["","","","","","@techreport{koizumi2020_t1, title = {The {NTT} {DCASE2020} Challenge Task 6 System: Automated Audio Captioning With Keywords and Sentence Length Estimation}, author = {Koizumi, Yuma and Takeuchi, Daiki and Ohishi, Yasunori and Harada, Noboru and Kashino, Kunio}, year = 2020, month = {June}, institution = {DCASE2020 Challenge}, abstract = {This technical report describes the system participating to the Detection and Classification of Acoustic Scenes and Events (DCASE) 2020 Challenge, Task 6: automated audio captioning. Our submission focuses on solving two indeterminacy problems in automated audio captioning: word selection indeterminacy and sentence length indeterminacy. We simultaneously solve the main caption generation and sub indeterminacy problems by estimating keywords and sentence length through multi-task learning. We tested a simplified model of our submission using the development-testing dataset. Our model achieved 20.7 SPIDEr score where that of the baseline system was 5.4.} }","@techreport{koizumi2020_t1, title = {The {NTT} {DCASE2020} Challenge Task 6 System: Automated Audio Captioning With Keywords and Sentence Length Estimation}, author = {Koizumi, Yuma and Takeuchi, Daiki and Ohishi, Yasunori and Harada, Noboru and Kashino, Kunio}, year = 2020, month = {June}, institution = {DCASE2020 Challenge}, abstract = {This technical report describes the system participating to the Detection and Classification of Acoustic Scenes and Events (DCASE) 2020 Challenge, Task 6: automated audio captioning. Our submission focuses on solving two indeterminacy problems in automated audio captioning: word selection indeterminacy and sentence length indeterminacy. We simultaneously solve the main caption generation and sub indeterminacy problems by estimating keywords and sentence length through multi-task learning. We tested a simplified model of our submission using the development-testing dataset. Our model achieved 20.7 SPIDEr score where that of the baseline system was 5.4.} }","","","","","","","","","","","","","","","@techreport{schaumloeffel2023_t6a, title = {PEACS: Prefix encoding for auditory caption synthesis}, author = {Schauml\u00f6ffel, Timothy and Vilas, Martina G. and Roig, Gemma}, year = 2023, month = {May}, institution = {DCASE2023 Challenge}, abstract = {This technical report describes an Automated Audio Captioning system for the Detection and Classification of Acoustic Scenes and Events (DCASE) 2023 Challenge, Task 6a (automated audio captioning). Our approach employs an encoder-decoder architecture, with the encoder utilizing a large contrastive pre-trained HTS-AT capable of handling variable-length audio segments. The decoder is based on the GPT2 model. To incorporate audio into the decoding process, we employ a light mapping network that translates audio representations into a prefix, effectively guiding the decoder\u2019s generation process. Given the limited data availability, we pre-train our model on various audio captioning datasets and fine-tune it on Clotho. We reach a SPIDERr-FL score of 29.3 on the evaluation split of the Clotho-v2 dataset.} }","","","","","","","@techreport{kicinski2022_t6a, title = {Exploring audio captioning with keyword-guided text generation}, author = {Kicinski, Dawid and de Gail, Teodor Lamort and Bujnowski, Pawel}, year = 2022, month = {July}, institution = {DCASE2022 Challenge}, abstract = {This technical report describes our submission to the DCASE 2022 challenge, Task 6 A: automated audio captioning. In our system, we explore the use of pre-trained language models for the audio captioning task. The proposed system is an encoder-decoder architecture consisting of a pre-trained PANN encoder and a GPT2 decoder. Audio embeddings are encoded to language model prompts using a simple mapping network. We further develop our system by employing strategies of guiding the decoder with textual information. We prompt the decoder with keywords extracted from semantically similar audios and use them to choose the best matching caption by their occurrence.} }","@techreport{kicinski2022_t6a, title = {Exploring audio captioning with keyword-guided text generation}, author = {Kicinski, Dawid and de Gail, Teodor Lamort and Bujnowski, Pawel}, year = 2022, month = {July}, institution = {DCASE2022 Challenge}, abstract = {This technical report describes our submission to the DCASE 2022 challenge, Task 6 A: automated audio captioning. In our system, we explore the use of pre-trained language models for the audio captioning task. The proposed system is an encoder-decoder architecture consisting of a pre-trained PANN encoder and a GPT2 decoder. Audio embeddings are encoded to language model prompts using a simple mapping network. We further develop our system by employing strategies of guiding the decoder with textual information. We prompt the decoder with keywords extracted from semantically similar audios and use them to choose the best matching caption by their occurrence.} }","@techreport{kicinski2022_t6a, title = {Exploring audio captioning with keyword-guided text generation}, author = {Kicinski, Dawid and de Gail, Teodor Lamort and Bujnowski, Pawel}, year = 2022, month = {July}, institution = {DCASE2022 Challenge}, abstract = {This technical report describes our submission to the DCASE 2022 challenge, Task 6 A: automated audio captioning. In our system, we explore the use of pre-trained language models for the audio captioning task. The proposed system is an encoder-decoder architecture consisting of a pre-trained PANN encoder and a GPT2 decoder. Audio embeddings are encoded to language model prompts using a simple mapping network. We further develop our system by employing strategies of guiding the decoder with textual information. We prompt the decoder with keywords extracted from semantically similar audios and use them to choose the best matching caption by their occurrence.} }","@techreport{kicinski2022_t6a, title = {Exploring audio captioning with keyword-guided text generation}, author = {Kicinski, Dawid and de Gail, Teodor Lamort and Bujnowski, Pawel}, year = 2022, month = {July}, institution = {DCASE2022 Challenge}, abstract = {This technical report describes our submission to the DCASE 2022 challenge, Task 6 A: automated audio captioning. In our system, we explore the use of pre-trained language models for the audio captioning task. The proposed system is an encoder-decoder architecture consisting of a pre-trained PANN encoder and a GPT2 decoder. Audio embeddings are encoded to language model prompts using a simple mapping network. We further develop our system by employing strategies of guiding the decoder with textual information. We prompt the decoder with keywords extracted from semantically similar audios and use them to choose the best matching caption by their occurrence.} }"]],["Optim",["AdamW","AdamW","Adam","Adam","Adam","AdamW","AdamW","?","?","?","Adam","Adam","AdamW","?","Adam","Adam","Adam","AdamW","Adam","Adam","Adam","AdamW","?","Adam","Adam","?","?","?","AdamW","AdamW","AdamW","AdamW"]],["ensemble",["Single","Ensemble","Single","Single","Single","Ensemble","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single"]],["extra_train",[true,true,false,true,false,false,false,false,false,true,false,false,false,false,false,true,true,false,false,false,true,true,false,false,false,true,true,true,true,true,true,true]],["color",["#ff7270","#ff7270","#70b1ff","#ff7270","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#ff7270","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#ff7270","#ff7270","#70b1ff","#70b1ff","#70b1ff","#ff7270","#ff7270","#70b1ff","#70b1ff","#70b1ff","#ff7270","#ff7270","#ff7270","#ff7270","#ff7270","#ff7270","#ff7270"]],["marker",["circle","circle","circle","circle","diamond","triangle","triangle","diamond","diamond","diamond","diamond","triangle","circle","diamond","diamond","diamond","circle","circle","diamond","circle","circle","circle","triangle","triangle","triangle","circle","circle","circle","diamond","circle","diamond","circle"]],["is_pareto",[true,true,false,true,false,false,true,false,true,false,false,false,false,false,true,false,false,false,true,false,false,false,false,false,false,false,false,false,false,false,false,false]],["pareto_idx",[1,1,"none",2,"none","none",3,"none",4,"none","none","none","none","none",5,"none","none","none",6,"none","none","none","none","none","none","none","none","none","none","none","none","none"]],["size",[40.0,59.16079783099616,14.832396974191326,14.832396974191326,9.38083151964686,9.082951062292475,1.0,3.7416573867739413,2.8284271247461903,23.664319132398465,3.1622776601683795,2.0,14.89966442575134,3.1622776601683795,8.94427190999916,19.493588689617926,17.663521732655695,14.317821063276353,9.273618495495704,11.40175425099138,11.40175425099138,15.748015748023622,2.8284271247461903,3.4641016151377544,1.7320508075688772,6.244997998398398,15.620499351813308,31.622776601683793,10.198039027185569,14.38749456993816,10.198039027185569,14.38749456993816]],["params_display",["1600M","3500M","220M","220M","88M","82M","1M","14M","8M","560M","10M","4M","222M","10M","80M","380M","312M","205M","86M","130M","130M","248M","8M","12M","3M","39M","244M","1000M","104M","207M","104M","207M"]],["params_xoff",[-42.0,-51.58039891549808,-29.416198487095663,-29.416198487095663,-26.69041575982343,-26.541475531146236,-22.5,-23.870828693386972,-23.414213562373096,-33.83215956619923,-23.58113883008419,-23.0,-29.44983221287567,-23.58113883008419,-26.47213595499958,-31.74679434480896,-30.831760866327848,-29.158910531638178,-26.636809247747852,-27.70087712549569,-27.70087712549569,-29.87400787401181,-23.414213562373096,-23.73205080756888,-22.866025403784437,-25.1224989991992,-29.810249675906654,-37.8113883008419,-27.099019513592786,-29.193747284969078,-27.099019513592786,-29.193747284969078]],["params_yoff",[-25.0,-34.58039891549808,-12.416198487095663,-12.416198487095663,-9.69041575982343,-9.541475531146236,-5.5,-6.87082869338697,-6.414213562373095,-16.83215956619923,-6.58113883008419,-6.0,-12.44983221287567,-6.58113883008419,-9.47213595499958,-14.746794344808963,-13.831760866327848,-12.158910531638178,-9.636809247747852,-10.70087712549569,-10.70087712549569,-12.874007874011811,-6.414213562373095,-6.732050807568877,-5.866025403784438,-8.122498999199198,-12.810249675906654,-20.811388300841898,-10.099019513592784,-12.19374728496908,-10.099019513592784,-12.19374728496908]]]}}},"view":{"type":"object","name":"CDSView","id":"p252292","attributes":{"filter":{"type":"object","name":"IndexFilter","id":"p252291","attributes":{"indices":[20,28,29,30,31,0,3,15,16,21,25,26,27]}}}},"glyph":{"type":"object","name":"Scatter","id":"p252315","attributes":{"x":{"type":"field","field":"Year"},"y":{"type":"field","field":"SPIDEr"},"size":{"type":"field","field":"size"},"line_alpha":{"type":"value","value":0.15},"fill_color":{"type":"value","value":"#ff7270"},"fill_alpha":{"type":"value","value":0.15},"hatch_color":{"type":"value","value":"#ff7270"},"marker":{"type":"field","field":"marker"}}},"nonselection_glyph":{"type":"object","name":"Scatter","id":"p252316","attributes":{"x":{"type":"field","field":"Year"},"y":{"type":"field","field":"SPIDEr"},"size":{"type":"field","field":"size"},"line_alpha":{"type":"value","value":0.1},"fill_color":{"type":"value","value":"#ff7270"},"fill_alpha":{"type":"value","value":0.1},"hatch_color":{"type":"value","value":"#ff7270"},"hatch_alpha":{"type":"value","value":0.1},"marker":{"type":"field","field":"marker"}}},"muted_glyph":{"type":"object","name":"Scatter","id":"p252317","attributes":{"x":{"type":"field","field":"Year"},"y":{"type":"field","field":"SPIDEr"},"size":{"type":"field","field":"size"},"line_alpha":{"type":"value","value":0.2},"fill_color":{"type":"value","value":"#ff7270"},"fill_alpha":{"type":"value","value":0.2},"hatch_color":{"type":"value","value":"#ff7270"},"hatch_alpha":{"type":"value","value":0.2},"marker":{"type":"field","field":"marker"}}}}},{"type":"object","name":"GlyphRenderer","id":"p252330","attributes":{"data_source":{"type":"object","name":"ColumnDataSource","id":"p252321","attributes":{"selected":{"type":"object","name":"Selection","id":"p252322","attributes":{"indices":[],"line_indices":[]}},"selection_policy":{"type":"object","name":"UnionRenderers","id":"p252323"},"data":{"type":"map","entries":[["#",["1","1","2","2","5","10","10","12","14","18","20","22","23","28","29","31","32","33","35","36","36","39","40","41","43","44","44","44","46","46","46","46"]],["Paper name",["BEATS-BASED AUDIO CAPTIONING MODEL WITH INSTRUCTOR EMBEDDING SUPERVISION AND CHATGPT MIX-UP","BEATS-BASED AUDIO CAPTIONING MODEL WITH INSTRUCTOR EMBEDDING SUPERVISION AND CHATGPT MIX-UP","WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research","WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research","CAU SUBMISSION TO DCASE 2021 TASK6: TRANSFORMER FOLLOWED BY TRANSFER LEARNING FOR AUDIO CAPTIONING","THE NTT DCASE2020 CHALLENGE TASK 6 SYSTEM: AUTOMATED AUDIO CAPTIONING WITH KEYWORDS AND SENTENCE LENGTH ESTIMATION","THE NTT DCASE2020 CHALLENGE TASK 6 SYSTEM: AUTOMATED AUDIO CAPTIONING WITH KEYWORDS AND SENTENCE LENGTH ESTIMATION","IMPROVING AUDIO CAPTIONING USING SEMANTIC SIMILARITY METRICS","AUTOMATED AUDIO CAPTIONING WITH KEYWORDS GUIDANCE","EFFICIENT AUDIO CAPTIONING TRANSFORMER WITH PATCHOUT AND TEXT GUIDANCE","INVESTIGATING LOCAL AND GLOBAL INFORMATION FOR AUTOMATED AUDIO CAPTIONING WITH TRANSFER LEARNING","WaveTransformer: A Novel Architecture for Audio Captioning Based on Learning Temporal and Time-Frequency Information","Automated Audio Captioning With Topic Modeling","AUTOMATED AUDIO CAPTIONING USING TRANSFER LEARNING AND RECONSTRUCTION LATENT SPACE SIMILARITY REGULARIZATION","IMPROVING THE PERFORMANCE OF AUTOMATED AUDIO CAPTIONING VIA INTEGRATING THE ACOUSTIC AND SEMANTIC INFORMATION","WEAKLY-SUPERVISED AUTOMATED AUDIO CAPTIONING VIA TEXT ONLY TRAINING","TRAINING AUDIO CAPTIONING MODELS WITHOUT AUDIO","PREFIX TUNING FOR AUTOMATED AUDIO CAPTIONING","AUTOMATED AUDIO CAPTIONING WITH MULTI-TASK LEARNING","CP-JKU\u2019S SUBMISSION TO TASK 6A OF THE DCASE2022 CHALLENGE: A BART ENCODER-DECODER FOR AUTOMATIC AUDIO CAPTIONING TRAINED VIA THE REINFORCE ALGORITHM AND TRANSFER LEARNING","CP-JKU\u2019S SUBMISSION TO TASK 6A OF THE DCASE2022 CHALLENGE: A BART ENCODER-DECODER FOR AUTOMATIC AUDIO CAPTIONING TRAINED VIA THE REINFORCE ALGORITHM AND TRANSFER LEARNING","PEACS: PREFIX ENCODING FOR AUDITORY CAPTION SYNTHESIS","AUDIO CAPTIONING BASED ON TRANSFORMER AND PRE-TRAINING FOR 2020 DCASE AUDIO CAPTIONING CHALLENGE","AUTOMATED AUDIO CAPTIONING WITH TEMPORAL ATTENTION","THE DCASE2021 CHALLENGE TASK 6 SYSTEM : AUTOMATED AUDIO CAPTION","A WHISPER TRANSFORMER FOR AUDIO CAPTIONING TRAINED WITH SYNTHETIC CAPTIONS AND TRANSFER LEARNING","A WHISPER TRANSFORMER FOR AUDIO CAPTIONING TRAINED WITH SYNTHETIC CAPTIONS AND TRANSFER LEARNING","A WHISPER TRANSFORMER FOR AUDIO CAPTIONING TRAINED WITH SYNTHETIC CAPTIONS AND TRANSFER LEARNING","EXPLORING AUDIO CAPTIONING WITH KEYWORD-GUIDED TEXT GENERATION","EXPLORING AUDIO CAPTIONING WITH KEYWORD-GUIDED TEXT GENERATION","EXPLORING AUDIO CAPTIONING WITH KEYWORD-GUIDED TEXT GENERATION","EXPLORING AUDIO CAPTIONING WITH KEYWORD-GUIDED TEXT GENERATION"]],["Link",["https://dcase.community/documents/challenge2023/technical_reports/DCASE2023_Wu_31_t6a.pdf","https://dcase.community/documents/challenge2023/technical_reports/DCASE2023_Wu_31_t6a.pdf","https://arxiv.org/pdf/2303.17395.pdf","https://arxiv.org/pdf/2303.17395.pdf","https://dcase.community/documents/challenge2021/technical_reports/DCASE2021_Won_103_t6.pdf","https://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Koizumi_63_t6.pdf","https://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Koizumi_63_t6.pdf","https://arxiv.org/pdf/2210.16470.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Mei_117_t6a.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Kouzelis_60_t6a.pdf","https://arxiv.org/pdf/2102.11457.pdf","https://arxiv.org/pdf/2010.11098.pdf","https://www.researchgate.net/publication/367063417_Automated_Audio_Captioning_with_Topic_Modeling/fulltext/63c003f33fcb6855ce7dc192/Automated-Audio-Captioning-With-Topic-Modeling.pdf?origin=publicationDetail&amp;_sg%5B0%5D=dkU_nXJBP2xbhG7xq1Kpptu2LS2Uiq-Fo5RM6TaN7kL5JHW8WO8EUF0hcE4lf-4L7ZQKXRdEVd61EQPaMhGQtw.S1AXevlPqorLIQ_q4bs-6w49K76AG2QOJO7tgbPDXZqZzD4m4H_wzSHQ8AcgNnJngfn4funEYEqMGo4AnRtz2w&amp;_sg%5B1%5D=reAe0vy9g4yvBWpyUKSyvVlseW88gNcN8Dx8LZgBRXPgb8dervQNWpA1cZAZNCUzRlKWai2NkbcrTfj76stgeMdXhgB76EPmstMSZGyRzigi.S1AXevlPqorLIQ_q4bs-6w49K76AG2QOJO7tgbPDXZqZzD4m4H_wzSHQ8AcgNnJngfn4funEYEqMGo4AnRtz2w&amp;_iepl=&amp;_rtd=eyJjb250ZW50SW50ZW50IjoibWFpbkl0ZW0ifQ%3D%3D&amp;_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6Il9kaXJlY3QiLCJwYWdlIjoiX2RpcmVjdCIsInBvc2l0aW9uIjoicGFnZUhlYWRlciJ9fQ","https://arxiv.org/pdf/2108.04692.pdf","https://arxiv.org/pdf/2110.06100.pdf","https://arxiv.org/pdf/2309.12242.pdf","https://arxiv.org/pdf/2309.07372.pdf","https://arxiv.org/pdf/2303.17489.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Zou_37_t6a.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Primus_97_t6a.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Primus_97_t6a.pdf","https://dcase.community/documents/challenge2023/technical_reports/DCASE2023_Schaumloeffel_107_t6a.pdf","https://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Wu_136_t6.pdf","https://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Wang_5_t6.pdf","https://dcase.community/documents/challenge2021/technical_reports/DCASE2021_Yang_76_t6.pdf","https://dcase.community/documents/challenge2023/technical_reports/DCASE2023_Kadlcik_68_t6a.pdf","https://dcase.community/documents/challenge2023/technical_reports/DCASE2023_Kadlcik_68_t6a.pdf","https://dcase.community/documents/challenge2023/technical_reports/DCASE2023_Kadlcik_68_t6a.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Kicinski_115_t6a.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Kicinski_115_t6a.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Kicinski_115_t6a.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Kicinski_115_t6a.pdf"]],["First Author",["Shih-Lun Wu","Shih-Lun Wu","Xinhao Mei","Xinhao Mei","Hyejin Won","Yuma Koizumi","Yuma Koizumi","Rehana Mahfuz","Xinhao Mei","Thodoris Kouzelis","Xuenan Xu","An Tran","Xinhao Mei","Andrew Koh","Zhongjie ye","Theodoros Kouzelis","Soham Deshmukh","Minkyu Kim","Zhongjie Ye","Paul Primus","Paul Primus","Timothy Schauml\u00a8offel","Yusong Wu","Helin Wang","Liu Yang","Marek Kadlc\u0131k","Marek Kadlc\u0131k","Marek Kadlc\u0131k","Dawid Kici\u00b4nski","Dawid Kici\u00b4nski","Dawid Kici\u00b4nski","Dawid Kici\u00b4nski"]],["Nb params",[1600.0,3500.0,220.0,220.0,88.0,82.5,1.0,14.0,8.0,560.0,10.0,4.0,222.0,10.0,80.0,380.0,312.0,205.0,86.0,130.0,130.0,248.0,8.0,12.0,3.0,39.0,244.0,1000.0,104.0,207.0,104.0,207.0]],["SPIDEr",[0.326,0.336,0.255,0.31,0.285,0.207,0.196,0.208,0.266,0.296,0.246,0.182,0.264,0.246,0.269,0.247,0.261,0.255,0.286,0.264,0.284,0.292,0.227,0.172,0.166,0.224,0.269,0.279,0.279,0.255,0.26,0.249]],["Train data",["AC+CL","AC+CL","CL","CL+WC","CL","CL","CL","CL","CL","AC+CL+MA","CL","CL","CL","CL","CL","CL+WC","CL+WC","CL","CL","CL","AC+CL","AC+CL+MA+SD+WT","CL","CL","CL","AC+AS+CL","AC+AS+CL","AC+AS+CL","AC+CL+FSD","AC+CL+FSD","AC+CL+FSD","AC+CL+FSD"]],["Test data",["CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL"]],["Pretrained",["audio+text","audio+text","audio+text","audio+text","audio","none","none","audio","audio","audio","audio","none","audio+text","audio","audio","audio","audio+text","audio+text","audio","audio+text","audio+text","audio+text","none","none","none","audio+text","audio+text","audio+text","audio","audio+text","audio","audio+text"]],["Nb mod\u00e8les",[1,20,1,1,1,50,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]],["RL",[false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false]],["Year",["2023","2023","2023","2023","2021","2020","2020","2022","2022","2022","2021","2020","2023","2021","2021","2023","2023","2023","2022","2022","2022","2023","2020","2020","2021","2023","2023","2023","2022","2022","2022","2022"]],["Archi",["trans-trans","trans-trans","cnn-trans","cnn-trans","cnn-trans","cnn-rnn","cnn-rnn","cnn-trans","cnn-trans","trans-trans","cnn-rnn","trans-trans","cnn-trans","cnn-trans","cnn-rnn","cnn-trans","trans-trans","cnn-trans","cnn-rnn","cnn-trans","cnn-trans","cnn-trans","cnn-trans","cnn-rnn","cnn-trans","trans-trans","trans-trans","trans-trans","cnn-trans","cnn-trans","cnn-trans","cnn-trans"]],["Bibtex",["","","","","","@techreport{koizumi2020_t1, title = {The {NTT} {DCASE2020} Challenge Task 6 System: Automated Audio Captioning With Keywords and Sentence Length Estimation}, author = {Koizumi, Yuma and Takeuchi, Daiki and Ohishi, Yasunori and Harada, Noboru and Kashino, Kunio}, year = 2020, month = {June}, institution = {DCASE2020 Challenge}, abstract = {This technical report describes the system participating to the Detection and Classification of Acoustic Scenes and Events (DCASE) 2020 Challenge, Task 6: automated audio captioning. Our submission focuses on solving two indeterminacy problems in automated audio captioning: word selection indeterminacy and sentence length indeterminacy. We simultaneously solve the main caption generation and sub indeterminacy problems by estimating keywords and sentence length through multi-task learning. We tested a simplified model of our submission using the development-testing dataset. Our model achieved 20.7 SPIDEr score where that of the baseline system was 5.4.} }","@techreport{koizumi2020_t1, title = {The {NTT} {DCASE2020} Challenge Task 6 System: Automated Audio Captioning With Keywords and Sentence Length Estimation}, author = {Koizumi, Yuma and Takeuchi, Daiki and Ohishi, Yasunori and Harada, Noboru and Kashino, Kunio}, year = 2020, month = {June}, institution = {DCASE2020 Challenge}, abstract = {This technical report describes the system participating to the Detection and Classification of Acoustic Scenes and Events (DCASE) 2020 Challenge, Task 6: automated audio captioning. Our submission focuses on solving two indeterminacy problems in automated audio captioning: word selection indeterminacy and sentence length indeterminacy. We simultaneously solve the main caption generation and sub indeterminacy problems by estimating keywords and sentence length through multi-task learning. We tested a simplified model of our submission using the development-testing dataset. Our model achieved 20.7 SPIDEr score where that of the baseline system was 5.4.} }","","","","","","","","","","","","","","","@techreport{schaumloeffel2023_t6a, title = {PEACS: Prefix encoding for auditory caption synthesis}, author = {Schauml\u00f6ffel, Timothy and Vilas, Martina G. and Roig, Gemma}, year = 2023, month = {May}, institution = {DCASE2023 Challenge}, abstract = {This technical report describes an Automated Audio Captioning system for the Detection and Classification of Acoustic Scenes and Events (DCASE) 2023 Challenge, Task 6a (automated audio captioning). Our approach employs an encoder-decoder architecture, with the encoder utilizing a large contrastive pre-trained HTS-AT capable of handling variable-length audio segments. The decoder is based on the GPT2 model. To incorporate audio into the decoding process, we employ a light mapping network that translates audio representations into a prefix, effectively guiding the decoder\u2019s generation process. Given the limited data availability, we pre-train our model on various audio captioning datasets and fine-tune it on Clotho. We reach a SPIDERr-FL score of 29.3 on the evaluation split of the Clotho-v2 dataset.} }","","","","","","","@techreport{kicinski2022_t6a, title = {Exploring audio captioning with keyword-guided text generation}, author = {Kicinski, Dawid and de Gail, Teodor Lamort and Bujnowski, Pawel}, year = 2022, month = {July}, institution = {DCASE2022 Challenge}, abstract = {This technical report describes our submission to the DCASE 2022 challenge, Task 6 A: automated audio captioning. In our system, we explore the use of pre-trained language models for the audio captioning task. The proposed system is an encoder-decoder architecture consisting of a pre-trained PANN encoder and a GPT2 decoder. Audio embeddings are encoded to language model prompts using a simple mapping network. We further develop our system by employing strategies of guiding the decoder with textual information. We prompt the decoder with keywords extracted from semantically similar audios and use them to choose the best matching caption by their occurrence.} }","@techreport{kicinski2022_t6a, title = {Exploring audio captioning with keyword-guided text generation}, author = {Kicinski, Dawid and de Gail, Teodor Lamort and Bujnowski, Pawel}, year = 2022, month = {July}, institution = {DCASE2022 Challenge}, abstract = {This technical report describes our submission to the DCASE 2022 challenge, Task 6 A: automated audio captioning. In our system, we explore the use of pre-trained language models for the audio captioning task. The proposed system is an encoder-decoder architecture consisting of a pre-trained PANN encoder and a GPT2 decoder. Audio embeddings are encoded to language model prompts using a simple mapping network. We further develop our system by employing strategies of guiding the decoder with textual information. We prompt the decoder with keywords extracted from semantically similar audios and use them to choose the best matching caption by their occurrence.} }","@techreport{kicinski2022_t6a, title = {Exploring audio captioning with keyword-guided text generation}, author = {Kicinski, Dawid and de Gail, Teodor Lamort and Bujnowski, Pawel}, year = 2022, month = {July}, institution = {DCASE2022 Challenge}, abstract = {This technical report describes our submission to the DCASE 2022 challenge, Task 6 A: automated audio captioning. In our system, we explore the use of pre-trained language models for the audio captioning task. The proposed system is an encoder-decoder architecture consisting of a pre-trained PANN encoder and a GPT2 decoder. Audio embeddings are encoded to language model prompts using a simple mapping network. We further develop our system by employing strategies of guiding the decoder with textual information. We prompt the decoder with keywords extracted from semantically similar audios and use them to choose the best matching caption by their occurrence.} }","@techreport{kicinski2022_t6a, title = {Exploring audio captioning with keyword-guided text generation}, author = {Kicinski, Dawid and de Gail, Teodor Lamort and Bujnowski, Pawel}, year = 2022, month = {July}, institution = {DCASE2022 Challenge}, abstract = {This technical report describes our submission to the DCASE 2022 challenge, Task 6 A: automated audio captioning. In our system, we explore the use of pre-trained language models for the audio captioning task. The proposed system is an encoder-decoder architecture consisting of a pre-trained PANN encoder and a GPT2 decoder. Audio embeddings are encoded to language model prompts using a simple mapping network. We further develop our system by employing strategies of guiding the decoder with textual information. We prompt the decoder with keywords extracted from semantically similar audios and use them to choose the best matching caption by their occurrence.} }"]],["Optim",["AdamW","AdamW","Adam","Adam","Adam","AdamW","AdamW","?","?","?","Adam","Adam","AdamW","?","Adam","Adam","Adam","AdamW","Adam","Adam","Adam","AdamW","?","Adam","Adam","?","?","?","AdamW","AdamW","AdamW","AdamW"]],["ensemble",["Single","Ensemble","Single","Single","Single","Ensemble","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single"]],["extra_train",[true,true,false,true,false,false,false,false,false,true,false,false,false,false,false,true,true,false,false,false,true,true,false,false,false,true,true,true,true,true,true,true]],["color",["#ff7270","#ff7270","#70b1ff","#ff7270","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#ff7270","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#ff7270","#ff7270","#70b1ff","#70b1ff","#70b1ff","#ff7270","#ff7270","#70b1ff","#70b1ff","#70b1ff","#ff7270","#ff7270","#ff7270","#ff7270","#ff7270","#ff7270","#ff7270"]],["marker",["circle","circle","circle","circle","diamond","triangle","triangle","diamond","diamond","diamond","diamond","triangle","circle","diamond","diamond","diamond","circle","circle","diamond","circle","circle","circle","triangle","triangle","triangle","circle","circle","circle","diamond","circle","diamond","circle"]],["is_pareto",[true,true,false,true,false,false,true,false,true,false,false,false,false,false,true,false,false,false,true,false,false,false,false,false,false,false,false,false,false,false,false,false]],["pareto_idx",[1,1,"none",2,"none","none",3,"none",4,"none","none","none","none","none",5,"none","none","none",6,"none","none","none","none","none","none","none","none","none","none","none","none","none"]],["size",[40.0,59.16079783099616,14.832396974191326,14.832396974191326,9.38083151964686,9.082951062292475,1.0,3.7416573867739413,2.8284271247461903,23.664319132398465,3.1622776601683795,2.0,14.89966442575134,3.1622776601683795,8.94427190999916,19.493588689617926,17.663521732655695,14.317821063276353,9.273618495495704,11.40175425099138,11.40175425099138,15.748015748023622,2.8284271247461903,3.4641016151377544,1.7320508075688772,6.244997998398398,15.620499351813308,31.622776601683793,10.198039027185569,14.38749456993816,10.198039027185569,14.38749456993816]],["params_display",["1600M","3500M","220M","220M","88M","82M","1M","14M","8M","560M","10M","4M","222M","10M","80M","380M","312M","205M","86M","130M","130M","248M","8M","12M","3M","39M","244M","1000M","104M","207M","104M","207M"]],["params_xoff",[-42.0,-51.58039891549808,-29.416198487095663,-29.416198487095663,-26.69041575982343,-26.541475531146236,-22.5,-23.870828693386972,-23.414213562373096,-33.83215956619923,-23.58113883008419,-23.0,-29.44983221287567,-23.58113883008419,-26.47213595499958,-31.74679434480896,-30.831760866327848,-29.158910531638178,-26.636809247747852,-27.70087712549569,-27.70087712549569,-29.87400787401181,-23.414213562373096,-23.73205080756888,-22.866025403784437,-25.1224989991992,-29.810249675906654,-37.8113883008419,-27.099019513592786,-29.193747284969078,-27.099019513592786,-29.193747284969078]],["params_yoff",[-25.0,-34.58039891549808,-12.416198487095663,-12.416198487095663,-9.69041575982343,-9.541475531146236,-5.5,-6.87082869338697,-6.414213562373095,-16.83215956619923,-6.58113883008419,-6.0,-12.44983221287567,-6.58113883008419,-9.47213595499958,-14.746794344808963,-13.831760866327848,-12.158910531638178,-9.636809247747852,-10.70087712549569,-10.70087712549569,-12.874007874011811,-6.414213562373095,-6.732050807568877,-5.866025403784438,-8.122498999199198,-12.810249675906654,-20.811388300841898,-10.099019513592784,-12.19374728496908,-10.099019513592784,-12.19374728496908]]]}}},"view":{"type":"object","name":"CDSView","id":"p252289","attributes":{"filter":{"type":"object","name":"IndexFilter","id":"p252288","attributes":{"indices":[22,4,18,12]}}}},"glyph":{"type":"object","name":"Scatter","id":"p252327","attributes":{"x":{"type":"field","field":"Year"},"y":{"type":"field","field":"SPIDEr"},"size":{"type":"field","field":"size"},"fill_color":{"type":"value","value":"#70b1ff"},"hatch_color":{"type":"value","value":"#70b1ff"},"marker":{"type":"field","field":"marker"}}},"nonselection_glyph":{"type":"object","name":"Scatter","id":"p252328","attributes":{"x":{"type":"field","field":"Year"},"y":{"type":"field","field":"SPIDEr"},"size":{"type":"field","field":"size"},"line_alpha":{"type":"value","value":0.1},"fill_color":{"type":"value","value":"#70b1ff"},"fill_alpha":{"type":"value","value":0.1},"hatch_color":{"type":"value","value":"#70b1ff"},"hatch_alpha":{"type":"value","value":0.1},"marker":{"type":"field","field":"marker"}}},"muted_glyph":{"type":"object","name":"Scatter","id":"p252329","attributes":{"x":{"type":"field","field":"Year"},"y":{"type":"field","field":"SPIDEr"},"size":{"type":"field","field":"size"},"line_alpha":{"type":"value","value":0.2},"fill_color":{"type":"value","value":"#70b1ff"},"fill_alpha":{"type":"value","value":0.2},"hatch_color":{"type":"value","value":"#70b1ff"},"hatch_alpha":{"type":"value","value":0.2},"marker":{"type":"field","field":"marker"}}}}},{"type":"object","name":"GlyphRenderer","id":"p252342","attributes":{"data_source":{"type":"object","name":"ColumnDataSource","id":"p252333","attributes":{"selected":{"type":"object","name":"Selection","id":"p252334","attributes":{"indices":[],"line_indices":[]}},"selection_policy":{"type":"object","name":"UnionRenderers","id":"p252335"},"data":{"type":"map","entries":[["#",["1","1","2","2","5","10","10","12","14","18","20","22","23","28","29","31","32","33","35","36","36","39","40","41","43","44","44","44","46","46","46","46"]],["Paper name",["BEATS-BASED AUDIO CAPTIONING MODEL WITH INSTRUCTOR EMBEDDING SUPERVISION AND CHATGPT MIX-UP","BEATS-BASED AUDIO CAPTIONING MODEL WITH INSTRUCTOR EMBEDDING SUPERVISION AND CHATGPT MIX-UP","WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research","WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research","CAU SUBMISSION TO DCASE 2021 TASK6: TRANSFORMER FOLLOWED BY TRANSFER LEARNING FOR AUDIO CAPTIONING","THE NTT DCASE2020 CHALLENGE TASK 6 SYSTEM: AUTOMATED AUDIO CAPTIONING WITH KEYWORDS AND SENTENCE LENGTH ESTIMATION","THE NTT DCASE2020 CHALLENGE TASK 6 SYSTEM: AUTOMATED AUDIO CAPTIONING WITH KEYWORDS AND SENTENCE LENGTH ESTIMATION","IMPROVING AUDIO CAPTIONING USING SEMANTIC SIMILARITY METRICS","AUTOMATED AUDIO CAPTIONING WITH KEYWORDS GUIDANCE","EFFICIENT AUDIO CAPTIONING TRANSFORMER WITH PATCHOUT AND TEXT GUIDANCE","INVESTIGATING LOCAL AND GLOBAL INFORMATION FOR AUTOMATED AUDIO CAPTIONING WITH TRANSFER LEARNING","WaveTransformer: A Novel Architecture for Audio Captioning Based on Learning Temporal and Time-Frequency Information","Automated Audio Captioning With Topic Modeling","AUTOMATED AUDIO CAPTIONING USING TRANSFER LEARNING AND RECONSTRUCTION LATENT SPACE SIMILARITY REGULARIZATION","IMPROVING THE PERFORMANCE OF AUTOMATED AUDIO CAPTIONING VIA INTEGRATING THE ACOUSTIC AND SEMANTIC INFORMATION","WEAKLY-SUPERVISED AUTOMATED AUDIO CAPTIONING VIA TEXT ONLY TRAINING","TRAINING AUDIO CAPTIONING MODELS WITHOUT AUDIO","PREFIX TUNING FOR AUTOMATED AUDIO CAPTIONING","AUTOMATED AUDIO CAPTIONING WITH MULTI-TASK LEARNING","CP-JKU\u2019S SUBMISSION TO TASK 6A OF THE DCASE2022 CHALLENGE: A BART ENCODER-DECODER FOR AUTOMATIC AUDIO CAPTIONING TRAINED VIA THE REINFORCE ALGORITHM AND TRANSFER LEARNING","CP-JKU\u2019S SUBMISSION TO TASK 6A OF THE DCASE2022 CHALLENGE: A BART ENCODER-DECODER FOR AUTOMATIC AUDIO CAPTIONING TRAINED VIA THE REINFORCE ALGORITHM AND TRANSFER LEARNING","PEACS: PREFIX ENCODING FOR AUDITORY CAPTION SYNTHESIS","AUDIO CAPTIONING BASED ON TRANSFORMER AND PRE-TRAINING FOR 2020 DCASE AUDIO CAPTIONING CHALLENGE","AUTOMATED AUDIO CAPTIONING WITH TEMPORAL ATTENTION","THE DCASE2021 CHALLENGE TASK 6 SYSTEM : AUTOMATED AUDIO CAPTION","A WHISPER TRANSFORMER FOR AUDIO CAPTIONING TRAINED WITH SYNTHETIC CAPTIONS AND TRANSFER LEARNING","A WHISPER TRANSFORMER FOR AUDIO CAPTIONING TRAINED WITH SYNTHETIC CAPTIONS AND TRANSFER LEARNING","A WHISPER TRANSFORMER FOR AUDIO CAPTIONING TRAINED WITH SYNTHETIC CAPTIONS AND TRANSFER LEARNING","EXPLORING AUDIO CAPTIONING WITH KEYWORD-GUIDED TEXT GENERATION","EXPLORING AUDIO CAPTIONING WITH KEYWORD-GUIDED TEXT GENERATION","EXPLORING AUDIO CAPTIONING WITH KEYWORD-GUIDED TEXT GENERATION","EXPLORING AUDIO CAPTIONING WITH KEYWORD-GUIDED TEXT GENERATION"]],["Link",["https://dcase.community/documents/challenge2023/technical_reports/DCASE2023_Wu_31_t6a.pdf","https://dcase.community/documents/challenge2023/technical_reports/DCASE2023_Wu_31_t6a.pdf","https://arxiv.org/pdf/2303.17395.pdf","https://arxiv.org/pdf/2303.17395.pdf","https://dcase.community/documents/challenge2021/technical_reports/DCASE2021_Won_103_t6.pdf","https://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Koizumi_63_t6.pdf","https://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Koizumi_63_t6.pdf","https://arxiv.org/pdf/2210.16470.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Mei_117_t6a.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Kouzelis_60_t6a.pdf","https://arxiv.org/pdf/2102.11457.pdf","https://arxiv.org/pdf/2010.11098.pdf","https://www.researchgate.net/publication/367063417_Automated_Audio_Captioning_with_Topic_Modeling/fulltext/63c003f33fcb6855ce7dc192/Automated-Audio-Captioning-With-Topic-Modeling.pdf?origin=publicationDetail&amp;_sg%5B0%5D=dkU_nXJBP2xbhG7xq1Kpptu2LS2Uiq-Fo5RM6TaN7kL5JHW8WO8EUF0hcE4lf-4L7ZQKXRdEVd61EQPaMhGQtw.S1AXevlPqorLIQ_q4bs-6w49K76AG2QOJO7tgbPDXZqZzD4m4H_wzSHQ8AcgNnJngfn4funEYEqMGo4AnRtz2w&amp;_sg%5B1%5D=reAe0vy9g4yvBWpyUKSyvVlseW88gNcN8Dx8LZgBRXPgb8dervQNWpA1cZAZNCUzRlKWai2NkbcrTfj76stgeMdXhgB76EPmstMSZGyRzigi.S1AXevlPqorLIQ_q4bs-6w49K76AG2QOJO7tgbPDXZqZzD4m4H_wzSHQ8AcgNnJngfn4funEYEqMGo4AnRtz2w&amp;_iepl=&amp;_rtd=eyJjb250ZW50SW50ZW50IjoibWFpbkl0ZW0ifQ%3D%3D&amp;_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6Il9kaXJlY3QiLCJwYWdlIjoiX2RpcmVjdCIsInBvc2l0aW9uIjoicGFnZUhlYWRlciJ9fQ","https://arxiv.org/pdf/2108.04692.pdf","https://arxiv.org/pdf/2110.06100.pdf","https://arxiv.org/pdf/2309.12242.pdf","https://arxiv.org/pdf/2309.07372.pdf","https://arxiv.org/pdf/2303.17489.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Zou_37_t6a.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Primus_97_t6a.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Primus_97_t6a.pdf","https://dcase.community/documents/challenge2023/technical_reports/DCASE2023_Schaumloeffel_107_t6a.pdf","https://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Wu_136_t6.pdf","https://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Wang_5_t6.pdf","https://dcase.community/documents/challenge2021/technical_reports/DCASE2021_Yang_76_t6.pdf","https://dcase.community/documents/challenge2023/technical_reports/DCASE2023_Kadlcik_68_t6a.pdf","https://dcase.community/documents/challenge2023/technical_reports/DCASE2023_Kadlcik_68_t6a.pdf","https://dcase.community/documents/challenge2023/technical_reports/DCASE2023_Kadlcik_68_t6a.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Kicinski_115_t6a.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Kicinski_115_t6a.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Kicinski_115_t6a.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Kicinski_115_t6a.pdf"]],["First Author",["Shih-Lun Wu","Shih-Lun Wu","Xinhao Mei","Xinhao Mei","Hyejin Won","Yuma Koizumi","Yuma Koizumi","Rehana Mahfuz","Xinhao Mei","Thodoris Kouzelis","Xuenan Xu","An Tran","Xinhao Mei","Andrew Koh","Zhongjie ye","Theodoros Kouzelis","Soham Deshmukh","Minkyu Kim","Zhongjie Ye","Paul Primus","Paul Primus","Timothy Schauml\u00a8offel","Yusong Wu","Helin Wang","Liu Yang","Marek Kadlc\u0131k","Marek Kadlc\u0131k","Marek Kadlc\u0131k","Dawid Kici\u00b4nski","Dawid Kici\u00b4nski","Dawid Kici\u00b4nski","Dawid Kici\u00b4nski"]],["Nb params",[1600.0,3500.0,220.0,220.0,88.0,82.5,1.0,14.0,8.0,560.0,10.0,4.0,222.0,10.0,80.0,380.0,312.0,205.0,86.0,130.0,130.0,248.0,8.0,12.0,3.0,39.0,244.0,1000.0,104.0,207.0,104.0,207.0]],["SPIDEr",[0.326,0.336,0.255,0.31,0.285,0.207,0.196,0.208,0.266,0.296,0.246,0.182,0.264,0.246,0.269,0.247,0.261,0.255,0.286,0.264,0.284,0.292,0.227,0.172,0.166,0.224,0.269,0.279,0.279,0.255,0.26,0.249]],["Train data",["AC+CL","AC+CL","CL","CL+WC","CL","CL","CL","CL","CL","AC+CL+MA","CL","CL","CL","CL","CL","CL+WC","CL+WC","CL","CL","CL","AC+CL","AC+CL+MA+SD+WT","CL","CL","CL","AC+AS+CL","AC+AS+CL","AC+AS+CL","AC+CL+FSD","AC+CL+FSD","AC+CL+FSD","AC+CL+FSD"]],["Test data",["CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL"]],["Pretrained",["audio+text","audio+text","audio+text","audio+text","audio","none","none","audio","audio","audio","audio","none","audio+text","audio","audio","audio","audio+text","audio+text","audio","audio+text","audio+text","audio+text","none","none","none","audio+text","audio+text","audio+text","audio","audio+text","audio","audio+text"]],["Nb mod\u00e8les",[1,20,1,1,1,50,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]],["RL",[false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false]],["Year",["2023","2023","2023","2023","2021","2020","2020","2022","2022","2022","2021","2020","2023","2021","2021","2023","2023","2023","2022","2022","2022","2023","2020","2020","2021","2023","2023","2023","2022","2022","2022","2022"]],["Archi",["trans-trans","trans-trans","cnn-trans","cnn-trans","cnn-trans","cnn-rnn","cnn-rnn","cnn-trans","cnn-trans","trans-trans","cnn-rnn","trans-trans","cnn-trans","cnn-trans","cnn-rnn","cnn-trans","trans-trans","cnn-trans","cnn-rnn","cnn-trans","cnn-trans","cnn-trans","cnn-trans","cnn-rnn","cnn-trans","trans-trans","trans-trans","trans-trans","cnn-trans","cnn-trans","cnn-trans","cnn-trans"]],["Bibtex",["","","","","","@techreport{koizumi2020_t1, title = {The {NTT} {DCASE2020} Challenge Task 6 System: Automated Audio Captioning With Keywords and Sentence Length Estimation}, author = {Koizumi, Yuma and Takeuchi, Daiki and Ohishi, Yasunori and Harada, Noboru and Kashino, Kunio}, year = 2020, month = {June}, institution = {DCASE2020 Challenge}, abstract = {This technical report describes the system participating to the Detection and Classification of Acoustic Scenes and Events (DCASE) 2020 Challenge, Task 6: automated audio captioning. Our submission focuses on solving two indeterminacy problems in automated audio captioning: word selection indeterminacy and sentence length indeterminacy. We simultaneously solve the main caption generation and sub indeterminacy problems by estimating keywords and sentence length through multi-task learning. We tested a simplified model of our submission using the development-testing dataset. Our model achieved 20.7 SPIDEr score where that of the baseline system was 5.4.} }","@techreport{koizumi2020_t1, title = {The {NTT} {DCASE2020} Challenge Task 6 System: Automated Audio Captioning With Keywords and Sentence Length Estimation}, author = {Koizumi, Yuma and Takeuchi, Daiki and Ohishi, Yasunori and Harada, Noboru and Kashino, Kunio}, year = 2020, month = {June}, institution = {DCASE2020 Challenge}, abstract = {This technical report describes the system participating to the Detection and Classification of Acoustic Scenes and Events (DCASE) 2020 Challenge, Task 6: automated audio captioning. Our submission focuses on solving two indeterminacy problems in automated audio captioning: word selection indeterminacy and sentence length indeterminacy. We simultaneously solve the main caption generation and sub indeterminacy problems by estimating keywords and sentence length through multi-task learning. We tested a simplified model of our submission using the development-testing dataset. Our model achieved 20.7 SPIDEr score where that of the baseline system was 5.4.} }","","","","","","","","","","","","","","","@techreport{schaumloeffel2023_t6a, title = {PEACS: Prefix encoding for auditory caption synthesis}, author = {Schauml\u00f6ffel, Timothy and Vilas, Martina G. and Roig, Gemma}, year = 2023, month = {May}, institution = {DCASE2023 Challenge}, abstract = {This technical report describes an Automated Audio Captioning system for the Detection and Classification of Acoustic Scenes and Events (DCASE) 2023 Challenge, Task 6a (automated audio captioning). Our approach employs an encoder-decoder architecture, with the encoder utilizing a large contrastive pre-trained HTS-AT capable of handling variable-length audio segments. The decoder is based on the GPT2 model. To incorporate audio into the decoding process, we employ a light mapping network that translates audio representations into a prefix, effectively guiding the decoder\u2019s generation process. Given the limited data availability, we pre-train our model on various audio captioning datasets and fine-tune it on Clotho. We reach a SPIDERr-FL score of 29.3 on the evaluation split of the Clotho-v2 dataset.} }","","","","","","","@techreport{kicinski2022_t6a, title = {Exploring audio captioning with keyword-guided text generation}, author = {Kicinski, Dawid and de Gail, Teodor Lamort and Bujnowski, Pawel}, year = 2022, month = {July}, institution = {DCASE2022 Challenge}, abstract = {This technical report describes our submission to the DCASE 2022 challenge, Task 6 A: automated audio captioning. In our system, we explore the use of pre-trained language models for the audio captioning task. The proposed system is an encoder-decoder architecture consisting of a pre-trained PANN encoder and a GPT2 decoder. Audio embeddings are encoded to language model prompts using a simple mapping network. We further develop our system by employing strategies of guiding the decoder with textual information. We prompt the decoder with keywords extracted from semantically similar audios and use them to choose the best matching caption by their occurrence.} }","@techreport{kicinski2022_t6a, title = {Exploring audio captioning with keyword-guided text generation}, author = {Kicinski, Dawid and de Gail, Teodor Lamort and Bujnowski, Pawel}, year = 2022, month = {July}, institution = {DCASE2022 Challenge}, abstract = {This technical report describes our submission to the DCASE 2022 challenge, Task 6 A: automated audio captioning. In our system, we explore the use of pre-trained language models for the audio captioning task. The proposed system is an encoder-decoder architecture consisting of a pre-trained PANN encoder and a GPT2 decoder. Audio embeddings are encoded to language model prompts using a simple mapping network. We further develop our system by employing strategies of guiding the decoder with textual information. We prompt the decoder with keywords extracted from semantically similar audios and use them to choose the best matching caption by their occurrence.} }","@techreport{kicinski2022_t6a, title = {Exploring audio captioning with keyword-guided text generation}, author = {Kicinski, Dawid and de Gail, Teodor Lamort and Bujnowski, Pawel}, year = 2022, month = {July}, institution = {DCASE2022 Challenge}, abstract = {This technical report describes our submission to the DCASE 2022 challenge, Task 6 A: automated audio captioning. In our system, we explore the use of pre-trained language models for the audio captioning task. The proposed system is an encoder-decoder architecture consisting of a pre-trained PANN encoder and a GPT2 decoder. Audio embeddings are encoded to language model prompts using a simple mapping network. We further develop our system by employing strategies of guiding the decoder with textual information. We prompt the decoder with keywords extracted from semantically similar audios and use them to choose the best matching caption by their occurrence.} }","@techreport{kicinski2022_t6a, title = {Exploring audio captioning with keyword-guided text generation}, author = {Kicinski, Dawid and de Gail, Teodor Lamort and Bujnowski, Pawel}, year = 2022, month = {July}, institution = {DCASE2022 Challenge}, abstract = {This technical report describes our submission to the DCASE 2022 challenge, Task 6 A: automated audio captioning. In our system, we explore the use of pre-trained language models for the audio captioning task. The proposed system is an encoder-decoder architecture consisting of a pre-trained PANN encoder and a GPT2 decoder. Audio embeddings are encoded to language model prompts using a simple mapping network. We further develop our system by employing strategies of guiding the decoder with textual information. We prompt the decoder with keywords extracted from semantically similar audios and use them to choose the best matching caption by their occurrence.} }"]],["Optim",["AdamW","AdamW","Adam","Adam","Adam","AdamW","AdamW","?","?","?","Adam","Adam","AdamW","?","Adam","Adam","Adam","AdamW","Adam","Adam","Adam","AdamW","?","Adam","Adam","?","?","?","AdamW","AdamW","AdamW","AdamW"]],["ensemble",["Single","Ensemble","Single","Single","Single","Ensemble","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single"]],["extra_train",[true,true,false,true,false,false,false,false,false,true,false,false,false,false,false,true,true,false,false,false,true,true,false,false,false,true,true,true,true,true,true,true]],["color",["#ff7270","#ff7270","#70b1ff","#ff7270","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#ff7270","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#ff7270","#ff7270","#70b1ff","#70b1ff","#70b1ff","#ff7270","#ff7270","#70b1ff","#70b1ff","#70b1ff","#ff7270","#ff7270","#ff7270","#ff7270","#ff7270","#ff7270","#ff7270"]],["marker",["circle","circle","circle","circle","diamond","triangle","triangle","diamond","diamond","diamond","diamond","triangle","circle","diamond","diamond","diamond","circle","circle","diamond","circle","circle","circle","triangle","triangle","triangle","circle","circle","circle","diamond","circle","diamond","circle"]],["is_pareto",[true,true,false,true,false,false,true,false,true,false,false,false,false,false,true,false,false,false,true,false,false,false,false,false,false,false,false,false,false,false,false,false]],["pareto_idx",[1,1,"none",2,"none","none",3,"none",4,"none","none","none","none","none",5,"none","none","none",6,"none","none","none","none","none","none","none","none","none","none","none","none","none"]],["size",[40.0,59.16079783099616,14.832396974191326,14.832396974191326,9.38083151964686,9.082951062292475,1.0,3.7416573867739413,2.8284271247461903,23.664319132398465,3.1622776601683795,2.0,14.89966442575134,3.1622776601683795,8.94427190999916,19.493588689617926,17.663521732655695,14.317821063276353,9.273618495495704,11.40175425099138,11.40175425099138,15.748015748023622,2.8284271247461903,3.4641016151377544,1.7320508075688772,6.244997998398398,15.620499351813308,31.622776601683793,10.198039027185569,14.38749456993816,10.198039027185569,14.38749456993816]],["params_display",["1600M","3500M","220M","220M","88M","82M","1M","14M","8M","560M","10M","4M","222M","10M","80M","380M","312M","205M","86M","130M","130M","248M","8M","12M","3M","39M","244M","1000M","104M","207M","104M","207M"]],["params_xoff",[-42.0,-51.58039891549808,-29.416198487095663,-29.416198487095663,-26.69041575982343,-26.541475531146236,-22.5,-23.870828693386972,-23.414213562373096,-33.83215956619923,-23.58113883008419,-23.0,-29.44983221287567,-23.58113883008419,-26.47213595499958,-31.74679434480896,-30.831760866327848,-29.158910531638178,-26.636809247747852,-27.70087712549569,-27.70087712549569,-29.87400787401181,-23.414213562373096,-23.73205080756888,-22.866025403784437,-25.1224989991992,-29.810249675906654,-37.8113883008419,-27.099019513592786,-29.193747284969078,-27.099019513592786,-29.193747284969078]],["params_yoff",[-25.0,-34.58039891549808,-12.416198487095663,-12.416198487095663,-9.69041575982343,-9.541475531146236,-5.5,-6.87082869338697,-6.414213562373095,-16.83215956619923,-6.58113883008419,-6.0,-12.44983221287567,-6.58113883008419,-9.47213595499958,-14.746794344808963,-13.831760866327848,-12.158910531638178,-9.636809247747852,-10.70087712549569,-10.70087712549569,-12.874007874011811,-6.414213562373095,-6.732050807568877,-5.866025403784438,-8.122498999199198,-12.810249675906654,-20.811388300841898,-10.099019513592784,-12.19374728496908,-10.099019513592784,-12.19374728496908]]]}}},"view":{"id":"p252289"},"glyph":{"type":"object","name":"Text","id":"p252339","attributes":{"x":{"type":"field","field":"Year"},"y":{"type":"field","field":"SPIDEr"},"text":{"type":"field","field":"params_display"},"x_offset":{"type":"field","field":"params_xoff"},"y_offset":{"type":"field","field":"params_yoff"},"text_color":{"type":"value","value":"#70b1ff"},"text_font_size":{"type":"value","value":"18px"},"text_font_style":{"type":"value","value":"bold"},"text_align":{"type":"value","value":"center"},"text_baseline":{"type":"value","value":"middle"}}},"nonselection_glyph":{"type":"object","name":"Text","id":"p252340","attributes":{"x":{"type":"field","field":"Year"},"y":{"type":"field","field":"SPIDEr"},"text":{"type":"field","field":"params_display"},"x_offset":{"type":"field","field":"params_xoff"},"y_offset":{"type":"field","field":"params_yoff"},"text_color":{"type":"value","value":"#70b1ff"},"text_alpha":{"type":"value","value":0.1},"text_font_size":{"type":"value","value":"18px"},"text_font_style":{"type":"value","value":"bold"},"text_align":{"type":"value","value":"center"},"text_baseline":{"type":"value","value":"middle"}}},"muted_glyph":{"type":"object","name":"Text","id":"p252341","attributes":{"x":{"type":"field","field":"Year"},"y":{"type":"field","field":"SPIDEr"},"text":{"type":"field","field":"params_display"},"x_offset":{"type":"field","field":"params_xoff"},"y_offset":{"type":"field","field":"params_yoff"},"text_color":{"type":"value","value":"#70b1ff"},"text_alpha":{"type":"value","value":0.2},"text_font_size":{"type":"value","value":"18px"},"text_font_style":{"type":"value","value":"bold"},"text_align":{"type":"value","value":"center"},"text_baseline":{"type":"value","value":"middle"}}}}},{"type":"object","name":"GlyphRenderer","id":"p252354","attributes":{"data_source":{"type":"object","name":"ColumnDataSource","id":"p252345","attributes":{"selected":{"type":"object","name":"Selection","id":"p252346","attributes":{"indices":[],"line_indices":[]}},"selection_policy":{"type":"object","name":"UnionRenderers","id":"p252347"},"data":{"type":"map","entries":[["#",["1","1","2","2","5","10","10","12","14","18","20","22","23","28","29","31","32","33","35","36","36","39","40","41","43","44","44","44","46","46","46","46"]],["Paper name",["BEATS-BASED AUDIO CAPTIONING MODEL WITH INSTRUCTOR EMBEDDING SUPERVISION AND CHATGPT MIX-UP","BEATS-BASED AUDIO CAPTIONING MODEL WITH INSTRUCTOR EMBEDDING SUPERVISION AND CHATGPT MIX-UP","WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research","WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research","CAU SUBMISSION TO DCASE 2021 TASK6: TRANSFORMER FOLLOWED BY TRANSFER LEARNING FOR AUDIO CAPTIONING","THE NTT DCASE2020 CHALLENGE TASK 6 SYSTEM: AUTOMATED AUDIO CAPTIONING WITH KEYWORDS AND SENTENCE LENGTH ESTIMATION","THE NTT DCASE2020 CHALLENGE TASK 6 SYSTEM: AUTOMATED AUDIO CAPTIONING WITH KEYWORDS AND SENTENCE LENGTH ESTIMATION","IMPROVING AUDIO CAPTIONING USING SEMANTIC SIMILARITY METRICS","AUTOMATED AUDIO CAPTIONING WITH KEYWORDS GUIDANCE","EFFICIENT AUDIO CAPTIONING TRANSFORMER WITH PATCHOUT AND TEXT GUIDANCE","INVESTIGATING LOCAL AND GLOBAL INFORMATION FOR AUTOMATED AUDIO CAPTIONING WITH TRANSFER LEARNING","WaveTransformer: A Novel Architecture for Audio Captioning Based on Learning Temporal and Time-Frequency Information","Automated Audio Captioning With Topic Modeling","AUTOMATED AUDIO CAPTIONING USING TRANSFER LEARNING AND RECONSTRUCTION LATENT SPACE SIMILARITY REGULARIZATION","IMPROVING THE PERFORMANCE OF AUTOMATED AUDIO CAPTIONING VIA INTEGRATING THE ACOUSTIC AND SEMANTIC INFORMATION","WEAKLY-SUPERVISED AUTOMATED AUDIO CAPTIONING VIA TEXT ONLY TRAINING","TRAINING AUDIO CAPTIONING MODELS WITHOUT AUDIO","PREFIX TUNING FOR AUTOMATED AUDIO CAPTIONING","AUTOMATED AUDIO CAPTIONING WITH MULTI-TASK LEARNING","CP-JKU\u2019S SUBMISSION TO TASK 6A OF THE DCASE2022 CHALLENGE: A BART ENCODER-DECODER FOR AUTOMATIC AUDIO CAPTIONING TRAINED VIA THE REINFORCE ALGORITHM AND TRANSFER LEARNING","CP-JKU\u2019S SUBMISSION TO TASK 6A OF THE DCASE2022 CHALLENGE: A BART ENCODER-DECODER FOR AUTOMATIC AUDIO CAPTIONING TRAINED VIA THE REINFORCE ALGORITHM AND TRANSFER LEARNING","PEACS: PREFIX ENCODING FOR AUDITORY CAPTION SYNTHESIS","AUDIO CAPTIONING BASED ON TRANSFORMER AND PRE-TRAINING FOR 2020 DCASE AUDIO CAPTIONING CHALLENGE","AUTOMATED AUDIO CAPTIONING WITH TEMPORAL ATTENTION","THE DCASE2021 CHALLENGE TASK 6 SYSTEM : AUTOMATED AUDIO CAPTION","A WHISPER TRANSFORMER FOR AUDIO CAPTIONING TRAINED WITH SYNTHETIC CAPTIONS AND TRANSFER LEARNING","A WHISPER TRANSFORMER FOR AUDIO CAPTIONING TRAINED WITH SYNTHETIC CAPTIONS AND TRANSFER LEARNING","A WHISPER TRANSFORMER FOR AUDIO CAPTIONING TRAINED WITH SYNTHETIC CAPTIONS AND TRANSFER LEARNING","EXPLORING AUDIO CAPTIONING WITH KEYWORD-GUIDED TEXT GENERATION","EXPLORING AUDIO CAPTIONING WITH KEYWORD-GUIDED TEXT GENERATION","EXPLORING AUDIO CAPTIONING WITH KEYWORD-GUIDED TEXT GENERATION","EXPLORING AUDIO CAPTIONING WITH KEYWORD-GUIDED TEXT GENERATION"]],["Link",["https://dcase.community/documents/challenge2023/technical_reports/DCASE2023_Wu_31_t6a.pdf","https://dcase.community/documents/challenge2023/technical_reports/DCASE2023_Wu_31_t6a.pdf","https://arxiv.org/pdf/2303.17395.pdf","https://arxiv.org/pdf/2303.17395.pdf","https://dcase.community/documents/challenge2021/technical_reports/DCASE2021_Won_103_t6.pdf","https://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Koizumi_63_t6.pdf","https://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Koizumi_63_t6.pdf","https://arxiv.org/pdf/2210.16470.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Mei_117_t6a.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Kouzelis_60_t6a.pdf","https://arxiv.org/pdf/2102.11457.pdf","https://arxiv.org/pdf/2010.11098.pdf","https://www.researchgate.net/publication/367063417_Automated_Audio_Captioning_with_Topic_Modeling/fulltext/63c003f33fcb6855ce7dc192/Automated-Audio-Captioning-With-Topic-Modeling.pdf?origin=publicationDetail&amp;_sg%5B0%5D=dkU_nXJBP2xbhG7xq1Kpptu2LS2Uiq-Fo5RM6TaN7kL5JHW8WO8EUF0hcE4lf-4L7ZQKXRdEVd61EQPaMhGQtw.S1AXevlPqorLIQ_q4bs-6w49K76AG2QOJO7tgbPDXZqZzD4m4H_wzSHQ8AcgNnJngfn4funEYEqMGo4AnRtz2w&amp;_sg%5B1%5D=reAe0vy9g4yvBWpyUKSyvVlseW88gNcN8Dx8LZgBRXPgb8dervQNWpA1cZAZNCUzRlKWai2NkbcrTfj76stgeMdXhgB76EPmstMSZGyRzigi.S1AXevlPqorLIQ_q4bs-6w49K76AG2QOJO7tgbPDXZqZzD4m4H_wzSHQ8AcgNnJngfn4funEYEqMGo4AnRtz2w&amp;_iepl=&amp;_rtd=eyJjb250ZW50SW50ZW50IjoibWFpbkl0ZW0ifQ%3D%3D&amp;_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6Il9kaXJlY3QiLCJwYWdlIjoiX2RpcmVjdCIsInBvc2l0aW9uIjoicGFnZUhlYWRlciJ9fQ","https://arxiv.org/pdf/2108.04692.pdf","https://arxiv.org/pdf/2110.06100.pdf","https://arxiv.org/pdf/2309.12242.pdf","https://arxiv.org/pdf/2309.07372.pdf","https://arxiv.org/pdf/2303.17489.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Zou_37_t6a.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Primus_97_t6a.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Primus_97_t6a.pdf","https://dcase.community/documents/challenge2023/technical_reports/DCASE2023_Schaumloeffel_107_t6a.pdf","https://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Wu_136_t6.pdf","https://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Wang_5_t6.pdf","https://dcase.community/documents/challenge2021/technical_reports/DCASE2021_Yang_76_t6.pdf","https://dcase.community/documents/challenge2023/technical_reports/DCASE2023_Kadlcik_68_t6a.pdf","https://dcase.community/documents/challenge2023/technical_reports/DCASE2023_Kadlcik_68_t6a.pdf","https://dcase.community/documents/challenge2023/technical_reports/DCASE2023_Kadlcik_68_t6a.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Kicinski_115_t6a.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Kicinski_115_t6a.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Kicinski_115_t6a.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Kicinski_115_t6a.pdf"]],["First Author",["Shih-Lun Wu","Shih-Lun Wu","Xinhao Mei","Xinhao Mei","Hyejin Won","Yuma Koizumi","Yuma Koizumi","Rehana Mahfuz","Xinhao Mei","Thodoris Kouzelis","Xuenan Xu","An Tran","Xinhao Mei","Andrew Koh","Zhongjie ye","Theodoros Kouzelis","Soham Deshmukh","Minkyu Kim","Zhongjie Ye","Paul Primus","Paul Primus","Timothy Schauml\u00a8offel","Yusong Wu","Helin Wang","Liu Yang","Marek Kadlc\u0131k","Marek Kadlc\u0131k","Marek Kadlc\u0131k","Dawid Kici\u00b4nski","Dawid Kici\u00b4nski","Dawid Kici\u00b4nski","Dawid Kici\u00b4nski"]],["Nb params",[1600.0,3500.0,220.0,220.0,88.0,82.5,1.0,14.0,8.0,560.0,10.0,4.0,222.0,10.0,80.0,380.0,312.0,205.0,86.0,130.0,130.0,248.0,8.0,12.0,3.0,39.0,244.0,1000.0,104.0,207.0,104.0,207.0]],["SPIDEr",[0.326,0.336,0.255,0.31,0.285,0.207,0.196,0.208,0.266,0.296,0.246,0.182,0.264,0.246,0.269,0.247,0.261,0.255,0.286,0.264,0.284,0.292,0.227,0.172,0.166,0.224,0.269,0.279,0.279,0.255,0.26,0.249]],["Train data",["AC+CL","AC+CL","CL","CL+WC","CL","CL","CL","CL","CL","AC+CL+MA","CL","CL","CL","CL","CL","CL+WC","CL+WC","CL","CL","CL","AC+CL","AC+CL+MA+SD+WT","CL","CL","CL","AC+AS+CL","AC+AS+CL","AC+AS+CL","AC+CL+FSD","AC+CL+FSD","AC+CL+FSD","AC+CL+FSD"]],["Test data",["CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL"]],["Pretrained",["audio+text","audio+text","audio+text","audio+text","audio","none","none","audio","audio","audio","audio","none","audio+text","audio","audio","audio","audio+text","audio+text","audio","audio+text","audio+text","audio+text","none","none","none","audio+text","audio+text","audio+text","audio","audio+text","audio","audio+text"]],["Nb mod\u00e8les",[1,20,1,1,1,50,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]],["RL",[false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false]],["Year",["2023","2023","2023","2023","2021","2020","2020","2022","2022","2022","2021","2020","2023","2021","2021","2023","2023","2023","2022","2022","2022","2023","2020","2020","2021","2023","2023","2023","2022","2022","2022","2022"]],["Archi",["trans-trans","trans-trans","cnn-trans","cnn-trans","cnn-trans","cnn-rnn","cnn-rnn","cnn-trans","cnn-trans","trans-trans","cnn-rnn","trans-trans","cnn-trans","cnn-trans","cnn-rnn","cnn-trans","trans-trans","cnn-trans","cnn-rnn","cnn-trans","cnn-trans","cnn-trans","cnn-trans","cnn-rnn","cnn-trans","trans-trans","trans-trans","trans-trans","cnn-trans","cnn-trans","cnn-trans","cnn-trans"]],["Bibtex",["","","","","","@techreport{koizumi2020_t1, title = {The {NTT} {DCASE2020} Challenge Task 6 System: Automated Audio Captioning With Keywords and Sentence Length Estimation}, author = {Koizumi, Yuma and Takeuchi, Daiki and Ohishi, Yasunori and Harada, Noboru and Kashino, Kunio}, year = 2020, month = {June}, institution = {DCASE2020 Challenge}, abstract = {This technical report describes the system participating to the Detection and Classification of Acoustic Scenes and Events (DCASE) 2020 Challenge, Task 6: automated audio captioning. Our submission focuses on solving two indeterminacy problems in automated audio captioning: word selection indeterminacy and sentence length indeterminacy. We simultaneously solve the main caption generation and sub indeterminacy problems by estimating keywords and sentence length through multi-task learning. We tested a simplified model of our submission using the development-testing dataset. Our model achieved 20.7 SPIDEr score where that of the baseline system was 5.4.} }","@techreport{koizumi2020_t1, title = {The {NTT} {DCASE2020} Challenge Task 6 System: Automated Audio Captioning With Keywords and Sentence Length Estimation}, author = {Koizumi, Yuma and Takeuchi, Daiki and Ohishi, Yasunori and Harada, Noboru and Kashino, Kunio}, year = 2020, month = {June}, institution = {DCASE2020 Challenge}, abstract = {This technical report describes the system participating to the Detection and Classification of Acoustic Scenes and Events (DCASE) 2020 Challenge, Task 6: automated audio captioning. Our submission focuses on solving two indeterminacy problems in automated audio captioning: word selection indeterminacy and sentence length indeterminacy. We simultaneously solve the main caption generation and sub indeterminacy problems by estimating keywords and sentence length through multi-task learning. We tested a simplified model of our submission using the development-testing dataset. Our model achieved 20.7 SPIDEr score where that of the baseline system was 5.4.} }","","","","","","","","","","","","","","","@techreport{schaumloeffel2023_t6a, title = {PEACS: Prefix encoding for auditory caption synthesis}, author = {Schauml\u00f6ffel, Timothy and Vilas, Martina G. and Roig, Gemma}, year = 2023, month = {May}, institution = {DCASE2023 Challenge}, abstract = {This technical report describes an Automated Audio Captioning system for the Detection and Classification of Acoustic Scenes and Events (DCASE) 2023 Challenge, Task 6a (automated audio captioning). Our approach employs an encoder-decoder architecture, with the encoder utilizing a large contrastive pre-trained HTS-AT capable of handling variable-length audio segments. The decoder is based on the GPT2 model. To incorporate audio into the decoding process, we employ a light mapping network that translates audio representations into a prefix, effectively guiding the decoder\u2019s generation process. Given the limited data availability, we pre-train our model on various audio captioning datasets and fine-tune it on Clotho. We reach a SPIDERr-FL score of 29.3 on the evaluation split of the Clotho-v2 dataset.} }","","","","","","","@techreport{kicinski2022_t6a, title = {Exploring audio captioning with keyword-guided text generation}, author = {Kicinski, Dawid and de Gail, Teodor Lamort and Bujnowski, Pawel}, year = 2022, month = {July}, institution = {DCASE2022 Challenge}, abstract = {This technical report describes our submission to the DCASE 2022 challenge, Task 6 A: automated audio captioning. In our system, we explore the use of pre-trained language models for the audio captioning task. The proposed system is an encoder-decoder architecture consisting of a pre-trained PANN encoder and a GPT2 decoder. Audio embeddings are encoded to language model prompts using a simple mapping network. We further develop our system by employing strategies of guiding the decoder with textual information. We prompt the decoder with keywords extracted from semantically similar audios and use them to choose the best matching caption by their occurrence.} }","@techreport{kicinski2022_t6a, title = {Exploring audio captioning with keyword-guided text generation}, author = {Kicinski, Dawid and de Gail, Teodor Lamort and Bujnowski, Pawel}, year = 2022, month = {July}, institution = {DCASE2022 Challenge}, abstract = {This technical report describes our submission to the DCASE 2022 challenge, Task 6 A: automated audio captioning. In our system, we explore the use of pre-trained language models for the audio captioning task. The proposed system is an encoder-decoder architecture consisting of a pre-trained PANN encoder and a GPT2 decoder. Audio embeddings are encoded to language model prompts using a simple mapping network. We further develop our system by employing strategies of guiding the decoder with textual information. We prompt the decoder with keywords extracted from semantically similar audios and use them to choose the best matching caption by their occurrence.} }","@techreport{kicinski2022_t6a, title = {Exploring audio captioning with keyword-guided text generation}, author = {Kicinski, Dawid and de Gail, Teodor Lamort and Bujnowski, Pawel}, year = 2022, month = {July}, institution = {DCASE2022 Challenge}, abstract = {This technical report describes our submission to the DCASE 2022 challenge, Task 6 A: automated audio captioning. In our system, we explore the use of pre-trained language models for the audio captioning task. The proposed system is an encoder-decoder architecture consisting of a pre-trained PANN encoder and a GPT2 decoder. Audio embeddings are encoded to language model prompts using a simple mapping network. We further develop our system by employing strategies of guiding the decoder with textual information. We prompt the decoder with keywords extracted from semantically similar audios and use them to choose the best matching caption by their occurrence.} }","@techreport{kicinski2022_t6a, title = {Exploring audio captioning with keyword-guided text generation}, author = {Kicinski, Dawid and de Gail, Teodor Lamort and Bujnowski, Pawel}, year = 2022, month = {July}, institution = {DCASE2022 Challenge}, abstract = {This technical report describes our submission to the DCASE 2022 challenge, Task 6 A: automated audio captioning. In our system, we explore the use of pre-trained language models for the audio captioning task. The proposed system is an encoder-decoder architecture consisting of a pre-trained PANN encoder and a GPT2 decoder. Audio embeddings are encoded to language model prompts using a simple mapping network. We further develop our system by employing strategies of guiding the decoder with textual information. We prompt the decoder with keywords extracted from semantically similar audios and use them to choose the best matching caption by their occurrence.} }"]],["Optim",["AdamW","AdamW","Adam","Adam","Adam","AdamW","AdamW","?","?","?","Adam","Adam","AdamW","?","Adam","Adam","Adam","AdamW","Adam","Adam","Adam","AdamW","?","Adam","Adam","?","?","?","AdamW","AdamW","AdamW","AdamW"]],["ensemble",["Single","Ensemble","Single","Single","Single","Ensemble","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single"]],["extra_train",[true,true,false,true,false,false,false,false,false,true,false,false,false,false,false,true,true,false,false,false,true,true,false,false,false,true,true,true,true,true,true,true]],["color",["#ff7270","#ff7270","#70b1ff","#ff7270","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#ff7270","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#ff7270","#ff7270","#70b1ff","#70b1ff","#70b1ff","#ff7270","#ff7270","#70b1ff","#70b1ff","#70b1ff","#ff7270","#ff7270","#ff7270","#ff7270","#ff7270","#ff7270","#ff7270"]],["marker",["circle","circle","circle","circle","diamond","triangle","triangle","diamond","diamond","diamond","diamond","triangle","circle","diamond","diamond","diamond","circle","circle","diamond","circle","circle","circle","triangle","triangle","triangle","circle","circle","circle","diamond","circle","diamond","circle"]],["is_pareto",[true,true,false,true,false,false,true,false,true,false,false,false,false,false,true,false,false,false,true,false,false,false,false,false,false,false,false,false,false,false,false,false]],["pareto_idx",[1,1,"none",2,"none","none",3,"none",4,"none","none","none","none","none",5,"none","none","none",6,"none","none","none","none","none","none","none","none","none","none","none","none","none"]],["size",[40.0,59.16079783099616,14.832396974191326,14.832396974191326,9.38083151964686,9.082951062292475,1.0,3.7416573867739413,2.8284271247461903,23.664319132398465,3.1622776601683795,2.0,14.89966442575134,3.1622776601683795,8.94427190999916,19.493588689617926,17.663521732655695,14.317821063276353,9.273618495495704,11.40175425099138,11.40175425099138,15.748015748023622,2.8284271247461903,3.4641016151377544,1.7320508075688772,6.244997998398398,15.620499351813308,31.622776601683793,10.198039027185569,14.38749456993816,10.198039027185569,14.38749456993816]],["params_display",["1600M","3500M","220M","220M","88M","82M","1M","14M","8M","560M","10M","4M","222M","10M","80M","380M","312M","205M","86M","130M","130M","248M","8M","12M","3M","39M","244M","1000M","104M","207M","104M","207M"]],["params_xoff",[-42.0,-51.58039891549808,-29.416198487095663,-29.416198487095663,-26.69041575982343,-26.541475531146236,-22.5,-23.870828693386972,-23.414213562373096,-33.83215956619923,-23.58113883008419,-23.0,-29.44983221287567,-23.58113883008419,-26.47213595499958,-31.74679434480896,-30.831760866327848,-29.158910531638178,-26.636809247747852,-27.70087712549569,-27.70087712549569,-29.87400787401181,-23.414213562373096,-23.73205080756888,-22.866025403784437,-25.1224989991992,-29.810249675906654,-37.8113883008419,-27.099019513592786,-29.193747284969078,-27.099019513592786,-29.193747284969078]],["params_yoff",[-25.0,-34.58039891549808,-12.416198487095663,-12.416198487095663,-9.69041575982343,-9.541475531146236,-5.5,-6.87082869338697,-6.414213562373095,-16.83215956619923,-6.58113883008419,-6.0,-12.44983221287567,-6.58113883008419,-9.47213595499958,-14.746794344808963,-13.831760866327848,-12.158910531638178,-9.636809247747852,-10.70087712549569,-10.70087712549569,-12.874007874011811,-6.414213562373095,-6.732050807568877,-5.866025403784438,-8.122498999199198,-12.810249675906654,-20.811388300841898,-10.099019513592784,-12.19374728496908,-10.099019513592784,-12.19374728496908]]]}}},"view":{"type":"object","name":"CDSView","id":"p252295","attributes":{"filter":{"type":"object","name":"IndexFilter","id":"p252294","attributes":{"indices":[9,1]}}}},"glyph":{"type":"object","name":"Scatter","id":"p252351","attributes":{"x":{"type":"field","field":"Year"},"y":{"type":"field","field":"SPIDEr"},"size":{"type":"field","field":"size"},"fill_color":{"type":"value","value":"#ff7270"},"hatch_color":{"type":"value","value":"#ff7270"},"marker":{"type":"field","field":"marker"}}},"nonselection_glyph":{"type":"object","name":"Scatter","id":"p252352","attributes":{"x":{"type":"field","field":"Year"},"y":{"type":"field","field":"SPIDEr"},"size":{"type":"field","field":"size"},"line_alpha":{"type":"value","value":0.1},"fill_color":{"type":"value","value":"#ff7270"},"fill_alpha":{"type":"value","value":0.1},"hatch_color":{"type":"value","value":"#ff7270"},"hatch_alpha":{"type":"value","value":0.1},"marker":{"type":"field","field":"marker"}}},"muted_glyph":{"type":"object","name":"Scatter","id":"p252353","attributes":{"x":{"type":"field","field":"Year"},"y":{"type":"field","field":"SPIDEr"},"size":{"type":"field","field":"size"},"line_alpha":{"type":"value","value":0.2},"fill_color":{"type":"value","value":"#ff7270"},"fill_alpha":{"type":"value","value":0.2},"hatch_color":{"type":"value","value":"#ff7270"},"hatch_alpha":{"type":"value","value":0.2},"marker":{"type":"field","field":"marker"}}}}},{"type":"object","name":"GlyphRenderer","id":"p252366","attributes":{"data_source":{"type":"object","name":"ColumnDataSource","id":"p252357","attributes":{"selected":{"type":"object","name":"Selection","id":"p252358","attributes":{"indices":[],"line_indices":[]}},"selection_policy":{"type":"object","name":"UnionRenderers","id":"p252359"},"data":{"type":"map","entries":[["#",["1","1","2","2","5","10","10","12","14","18","20","22","23","28","29","31","32","33","35","36","36","39","40","41","43","44","44","44","46","46","46","46"]],["Paper name",["BEATS-BASED AUDIO CAPTIONING MODEL WITH INSTRUCTOR EMBEDDING SUPERVISION AND CHATGPT MIX-UP","BEATS-BASED AUDIO CAPTIONING MODEL WITH INSTRUCTOR EMBEDDING SUPERVISION AND CHATGPT MIX-UP","WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research","WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research","CAU SUBMISSION TO DCASE 2021 TASK6: TRANSFORMER FOLLOWED BY TRANSFER LEARNING FOR AUDIO CAPTIONING","THE NTT DCASE2020 CHALLENGE TASK 6 SYSTEM: AUTOMATED AUDIO CAPTIONING WITH KEYWORDS AND SENTENCE LENGTH ESTIMATION","THE NTT DCASE2020 CHALLENGE TASK 6 SYSTEM: AUTOMATED AUDIO CAPTIONING WITH KEYWORDS AND SENTENCE LENGTH ESTIMATION","IMPROVING AUDIO CAPTIONING USING SEMANTIC SIMILARITY METRICS","AUTOMATED AUDIO CAPTIONING WITH KEYWORDS GUIDANCE","EFFICIENT AUDIO CAPTIONING TRANSFORMER WITH PATCHOUT AND TEXT GUIDANCE","INVESTIGATING LOCAL AND GLOBAL INFORMATION FOR AUTOMATED AUDIO CAPTIONING WITH TRANSFER LEARNING","WaveTransformer: A Novel Architecture for Audio Captioning Based on Learning Temporal and Time-Frequency Information","Automated Audio Captioning With Topic Modeling","AUTOMATED AUDIO CAPTIONING USING TRANSFER LEARNING AND RECONSTRUCTION LATENT SPACE SIMILARITY REGULARIZATION","IMPROVING THE PERFORMANCE OF AUTOMATED AUDIO CAPTIONING VIA INTEGRATING THE ACOUSTIC AND SEMANTIC INFORMATION","WEAKLY-SUPERVISED AUTOMATED AUDIO CAPTIONING VIA TEXT ONLY TRAINING","TRAINING AUDIO CAPTIONING MODELS WITHOUT AUDIO","PREFIX TUNING FOR AUTOMATED AUDIO CAPTIONING","AUTOMATED AUDIO CAPTIONING WITH MULTI-TASK LEARNING","CP-JKU\u2019S SUBMISSION TO TASK 6A OF THE DCASE2022 CHALLENGE: A BART ENCODER-DECODER FOR AUTOMATIC AUDIO CAPTIONING TRAINED VIA THE REINFORCE ALGORITHM AND TRANSFER LEARNING","CP-JKU\u2019S SUBMISSION TO TASK 6A OF THE DCASE2022 CHALLENGE: A BART ENCODER-DECODER FOR AUTOMATIC AUDIO CAPTIONING TRAINED VIA THE REINFORCE ALGORITHM AND TRANSFER LEARNING","PEACS: PREFIX ENCODING FOR AUDITORY CAPTION SYNTHESIS","AUDIO CAPTIONING BASED ON TRANSFORMER AND PRE-TRAINING FOR 2020 DCASE AUDIO CAPTIONING CHALLENGE","AUTOMATED AUDIO CAPTIONING WITH TEMPORAL ATTENTION","THE DCASE2021 CHALLENGE TASK 6 SYSTEM : AUTOMATED AUDIO CAPTION","A WHISPER TRANSFORMER FOR AUDIO CAPTIONING TRAINED WITH SYNTHETIC CAPTIONS AND TRANSFER LEARNING","A WHISPER TRANSFORMER FOR AUDIO CAPTIONING TRAINED WITH SYNTHETIC CAPTIONS AND TRANSFER LEARNING","A WHISPER TRANSFORMER FOR AUDIO CAPTIONING TRAINED WITH SYNTHETIC CAPTIONS AND TRANSFER LEARNING","EXPLORING AUDIO CAPTIONING WITH KEYWORD-GUIDED TEXT GENERATION","EXPLORING AUDIO CAPTIONING WITH KEYWORD-GUIDED TEXT GENERATION","EXPLORING AUDIO CAPTIONING WITH KEYWORD-GUIDED TEXT GENERATION","EXPLORING AUDIO CAPTIONING WITH KEYWORD-GUIDED TEXT GENERATION"]],["Link",["https://dcase.community/documents/challenge2023/technical_reports/DCASE2023_Wu_31_t6a.pdf","https://dcase.community/documents/challenge2023/technical_reports/DCASE2023_Wu_31_t6a.pdf","https://arxiv.org/pdf/2303.17395.pdf","https://arxiv.org/pdf/2303.17395.pdf","https://dcase.community/documents/challenge2021/technical_reports/DCASE2021_Won_103_t6.pdf","https://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Koizumi_63_t6.pdf","https://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Koizumi_63_t6.pdf","https://arxiv.org/pdf/2210.16470.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Mei_117_t6a.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Kouzelis_60_t6a.pdf","https://arxiv.org/pdf/2102.11457.pdf","https://arxiv.org/pdf/2010.11098.pdf","https://www.researchgate.net/publication/367063417_Automated_Audio_Captioning_with_Topic_Modeling/fulltext/63c003f33fcb6855ce7dc192/Automated-Audio-Captioning-With-Topic-Modeling.pdf?origin=publicationDetail&amp;_sg%5B0%5D=dkU_nXJBP2xbhG7xq1Kpptu2LS2Uiq-Fo5RM6TaN7kL5JHW8WO8EUF0hcE4lf-4L7ZQKXRdEVd61EQPaMhGQtw.S1AXevlPqorLIQ_q4bs-6w49K76AG2QOJO7tgbPDXZqZzD4m4H_wzSHQ8AcgNnJngfn4funEYEqMGo4AnRtz2w&amp;_sg%5B1%5D=reAe0vy9g4yvBWpyUKSyvVlseW88gNcN8Dx8LZgBRXPgb8dervQNWpA1cZAZNCUzRlKWai2NkbcrTfj76stgeMdXhgB76EPmstMSZGyRzigi.S1AXevlPqorLIQ_q4bs-6w49K76AG2QOJO7tgbPDXZqZzD4m4H_wzSHQ8AcgNnJngfn4funEYEqMGo4AnRtz2w&amp;_iepl=&amp;_rtd=eyJjb250ZW50SW50ZW50IjoibWFpbkl0ZW0ifQ%3D%3D&amp;_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6Il9kaXJlY3QiLCJwYWdlIjoiX2RpcmVjdCIsInBvc2l0aW9uIjoicGFnZUhlYWRlciJ9fQ","https://arxiv.org/pdf/2108.04692.pdf","https://arxiv.org/pdf/2110.06100.pdf","https://arxiv.org/pdf/2309.12242.pdf","https://arxiv.org/pdf/2309.07372.pdf","https://arxiv.org/pdf/2303.17489.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Zou_37_t6a.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Primus_97_t6a.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Primus_97_t6a.pdf","https://dcase.community/documents/challenge2023/technical_reports/DCASE2023_Schaumloeffel_107_t6a.pdf","https://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Wu_136_t6.pdf","https://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Wang_5_t6.pdf","https://dcase.community/documents/challenge2021/technical_reports/DCASE2021_Yang_76_t6.pdf","https://dcase.community/documents/challenge2023/technical_reports/DCASE2023_Kadlcik_68_t6a.pdf","https://dcase.community/documents/challenge2023/technical_reports/DCASE2023_Kadlcik_68_t6a.pdf","https://dcase.community/documents/challenge2023/technical_reports/DCASE2023_Kadlcik_68_t6a.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Kicinski_115_t6a.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Kicinski_115_t6a.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Kicinski_115_t6a.pdf","https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Kicinski_115_t6a.pdf"]],["First Author",["Shih-Lun Wu","Shih-Lun Wu","Xinhao Mei","Xinhao Mei","Hyejin Won","Yuma Koizumi","Yuma Koizumi","Rehana Mahfuz","Xinhao Mei","Thodoris Kouzelis","Xuenan Xu","An Tran","Xinhao Mei","Andrew Koh","Zhongjie ye","Theodoros Kouzelis","Soham Deshmukh","Minkyu Kim","Zhongjie Ye","Paul Primus","Paul Primus","Timothy Schauml\u00a8offel","Yusong Wu","Helin Wang","Liu Yang","Marek Kadlc\u0131k","Marek Kadlc\u0131k","Marek Kadlc\u0131k","Dawid Kici\u00b4nski","Dawid Kici\u00b4nski","Dawid Kici\u00b4nski","Dawid Kici\u00b4nski"]],["Nb params",[1600.0,3500.0,220.0,220.0,88.0,82.5,1.0,14.0,8.0,560.0,10.0,4.0,222.0,10.0,80.0,380.0,312.0,205.0,86.0,130.0,130.0,248.0,8.0,12.0,3.0,39.0,244.0,1000.0,104.0,207.0,104.0,207.0]],["SPIDEr",[0.326,0.336,0.255,0.31,0.285,0.207,0.196,0.208,0.266,0.296,0.246,0.182,0.264,0.246,0.269,0.247,0.261,0.255,0.286,0.264,0.284,0.292,0.227,0.172,0.166,0.224,0.269,0.279,0.279,0.255,0.26,0.249]],["Train data",["AC+CL","AC+CL","CL","CL+WC","CL","CL","CL","CL","CL","AC+CL+MA","CL","CL","CL","CL","CL","CL+WC","CL+WC","CL","CL","CL","AC+CL","AC+CL+MA+SD+WT","CL","CL","CL","AC+AS+CL","AC+AS+CL","AC+AS+CL","AC+CL+FSD","AC+CL+FSD","AC+CL+FSD","AC+CL+FSD"]],["Test data",["CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL","CL"]],["Pretrained",["audio+text","audio+text","audio+text","audio+text","audio","none","none","audio","audio","audio","audio","none","audio+text","audio","audio","audio","audio+text","audio+text","audio","audio+text","audio+text","audio+text","none","none","none","audio+text","audio+text","audio+text","audio","audio+text","audio","audio+text"]],["Nb mod\u00e8les",[1,20,1,1,1,50,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]],["RL",[false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false]],["Year",["2023","2023","2023","2023","2021","2020","2020","2022","2022","2022","2021","2020","2023","2021","2021","2023","2023","2023","2022","2022","2022","2023","2020","2020","2021","2023","2023","2023","2022","2022","2022","2022"]],["Archi",["trans-trans","trans-trans","cnn-trans","cnn-trans","cnn-trans","cnn-rnn","cnn-rnn","cnn-trans","cnn-trans","trans-trans","cnn-rnn","trans-trans","cnn-trans","cnn-trans","cnn-rnn","cnn-trans","trans-trans","cnn-trans","cnn-rnn","cnn-trans","cnn-trans","cnn-trans","cnn-trans","cnn-rnn","cnn-trans","trans-trans","trans-trans","trans-trans","cnn-trans","cnn-trans","cnn-trans","cnn-trans"]],["Bibtex",["","","","","","@techreport{koizumi2020_t1, title = {The {NTT} {DCASE2020} Challenge Task 6 System: Automated Audio Captioning With Keywords and Sentence Length Estimation}, author = {Koizumi, Yuma and Takeuchi, Daiki and Ohishi, Yasunori and Harada, Noboru and Kashino, Kunio}, year = 2020, month = {June}, institution = {DCASE2020 Challenge}, abstract = {This technical report describes the system participating to the Detection and Classification of Acoustic Scenes and Events (DCASE) 2020 Challenge, Task 6: automated audio captioning. Our submission focuses on solving two indeterminacy problems in automated audio captioning: word selection indeterminacy and sentence length indeterminacy. We simultaneously solve the main caption generation and sub indeterminacy problems by estimating keywords and sentence length through multi-task learning. We tested a simplified model of our submission using the development-testing dataset. Our model achieved 20.7 SPIDEr score where that of the baseline system was 5.4.} }","@techreport{koizumi2020_t1, title = {The {NTT} {DCASE2020} Challenge Task 6 System: Automated Audio Captioning With Keywords and Sentence Length Estimation}, author = {Koizumi, Yuma and Takeuchi, Daiki and Ohishi, Yasunori and Harada, Noboru and Kashino, Kunio}, year = 2020, month = {June}, institution = {DCASE2020 Challenge}, abstract = {This technical report describes the system participating to the Detection and Classification of Acoustic Scenes and Events (DCASE) 2020 Challenge, Task 6: automated audio captioning. Our submission focuses on solving two indeterminacy problems in automated audio captioning: word selection indeterminacy and sentence length indeterminacy. We simultaneously solve the main caption generation and sub indeterminacy problems by estimating keywords and sentence length through multi-task learning. We tested a simplified model of our submission using the development-testing dataset. Our model achieved 20.7 SPIDEr score where that of the baseline system was 5.4.} }","","","","","","","","","","","","","","","@techreport{schaumloeffel2023_t6a, title = {PEACS: Prefix encoding for auditory caption synthesis}, author = {Schauml\u00f6ffel, Timothy and Vilas, Martina G. and Roig, Gemma}, year = 2023, month = {May}, institution = {DCASE2023 Challenge}, abstract = {This technical report describes an Automated Audio Captioning system for the Detection and Classification of Acoustic Scenes and Events (DCASE) 2023 Challenge, Task 6a (automated audio captioning). Our approach employs an encoder-decoder architecture, with the encoder utilizing a large contrastive pre-trained HTS-AT capable of handling variable-length audio segments. The decoder is based on the GPT2 model. To incorporate audio into the decoding process, we employ a light mapping network that translates audio representations into a prefix, effectively guiding the decoder\u2019s generation process. Given the limited data availability, we pre-train our model on various audio captioning datasets and fine-tune it on Clotho. We reach a SPIDERr-FL score of 29.3 on the evaluation split of the Clotho-v2 dataset.} }","","","","","","","@techreport{kicinski2022_t6a, title = {Exploring audio captioning with keyword-guided text generation}, author = {Kicinski, Dawid and de Gail, Teodor Lamort and Bujnowski, Pawel}, year = 2022, month = {July}, institution = {DCASE2022 Challenge}, abstract = {This technical report describes our submission to the DCASE 2022 challenge, Task 6 A: automated audio captioning. In our system, we explore the use of pre-trained language models for the audio captioning task. The proposed system is an encoder-decoder architecture consisting of a pre-trained PANN encoder and a GPT2 decoder. Audio embeddings are encoded to language model prompts using a simple mapping network. We further develop our system by employing strategies of guiding the decoder with textual information. We prompt the decoder with keywords extracted from semantically similar audios and use them to choose the best matching caption by their occurrence.} }","@techreport{kicinski2022_t6a, title = {Exploring audio captioning with keyword-guided text generation}, author = {Kicinski, Dawid and de Gail, Teodor Lamort and Bujnowski, Pawel}, year = 2022, month = {July}, institution = {DCASE2022 Challenge}, abstract = {This technical report describes our submission to the DCASE 2022 challenge, Task 6 A: automated audio captioning. In our system, we explore the use of pre-trained language models for the audio captioning task. The proposed system is an encoder-decoder architecture consisting of a pre-trained PANN encoder and a GPT2 decoder. Audio embeddings are encoded to language model prompts using a simple mapping network. We further develop our system by employing strategies of guiding the decoder with textual information. We prompt the decoder with keywords extracted from semantically similar audios and use them to choose the best matching caption by their occurrence.} }","@techreport{kicinski2022_t6a, title = {Exploring audio captioning with keyword-guided text generation}, author = {Kicinski, Dawid and de Gail, Teodor Lamort and Bujnowski, Pawel}, year = 2022, month = {July}, institution = {DCASE2022 Challenge}, abstract = {This technical report describes our submission to the DCASE 2022 challenge, Task 6 A: automated audio captioning. In our system, we explore the use of pre-trained language models for the audio captioning task. The proposed system is an encoder-decoder architecture consisting of a pre-trained PANN encoder and a GPT2 decoder. Audio embeddings are encoded to language model prompts using a simple mapping network. We further develop our system by employing strategies of guiding the decoder with textual information. We prompt the decoder with keywords extracted from semantically similar audios and use them to choose the best matching caption by their occurrence.} }","@techreport{kicinski2022_t6a, title = {Exploring audio captioning with keyword-guided text generation}, author = {Kicinski, Dawid and de Gail, Teodor Lamort and Bujnowski, Pawel}, year = 2022, month = {July}, institution = {DCASE2022 Challenge}, abstract = {This technical report describes our submission to the DCASE 2022 challenge, Task 6 A: automated audio captioning. In our system, we explore the use of pre-trained language models for the audio captioning task. The proposed system is an encoder-decoder architecture consisting of a pre-trained PANN encoder and a GPT2 decoder. Audio embeddings are encoded to language model prompts using a simple mapping network. We further develop our system by employing strategies of guiding the decoder with textual information. We prompt the decoder with keywords extracted from semantically similar audios and use them to choose the best matching caption by their occurrence.} }"]],["Optim",["AdamW","AdamW","Adam","Adam","Adam","AdamW","AdamW","?","?","?","Adam","Adam","AdamW","?","Adam","Adam","Adam","AdamW","Adam","Adam","Adam","AdamW","?","Adam","Adam","?","?","?","AdamW","AdamW","AdamW","AdamW"]],["ensemble",["Single","Ensemble","Single","Single","Single","Ensemble","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single","Single"]],["extra_train",[true,true,false,true,false,false,false,false,false,true,false,false,false,false,false,true,true,false,false,false,true,true,false,false,false,true,true,true,true,true,true,true]],["color",["#ff7270","#ff7270","#70b1ff","#ff7270","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#ff7270","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#70b1ff","#ff7270","#ff7270","#70b1ff","#70b1ff","#70b1ff","#ff7270","#ff7270","#70b1ff","#70b1ff","#70b1ff","#ff7270","#ff7270","#ff7270","#ff7270","#ff7270","#ff7270","#ff7270"]],["marker",["circle","circle","circle","circle","diamond","triangle","triangle","diamond","diamond","diamond","diamond","triangle","circle","diamond","diamond","diamond","circle","circle","diamond","circle","circle","circle","triangle","triangle","triangle","circle","circle","circle","diamond","circle","diamond","circle"]],["is_pareto",[true,true,false,true,false,false,true,false,true,false,false,false,false,false,true,false,false,false,true,false,false,false,false,false,false,false,false,false,false,false,false,false]],["pareto_idx",[1,1,"none",2,"none","none",3,"none",4,"none","none","none","none","none",5,"none","none","none",6,"none","none","none","none","none","none","none","none","none","none","none","none","none"]],["size",[40.0,59.16079783099616,14.832396974191326,14.832396974191326,9.38083151964686,9.082951062292475,1.0,3.7416573867739413,2.8284271247461903,23.664319132398465,3.1622776601683795,2.0,14.89966442575134,3.1622776601683795,8.94427190999916,19.493588689617926,17.663521732655695,14.317821063276353,9.273618495495704,11.40175425099138,11.40175425099138,15.748015748023622,2.8284271247461903,3.4641016151377544,1.7320508075688772,6.244997998398398,15.620499351813308,31.622776601683793,10.198039027185569,14.38749456993816,10.198039027185569,14.38749456993816]],["params_display",["1600M","3500M","220M","220M","88M","82M","1M","14M","8M","560M","10M","4M","222M","10M","80M","380M","312M","205M","86M","130M","130M","248M","8M","12M","3M","39M","244M","1000M","104M","207M","104M","207M"]],["params_xoff",[-42.0,-51.58039891549808,-29.416198487095663,-29.416198487095663,-26.69041575982343,-26.541475531146236,-22.5,-23.870828693386972,-23.414213562373096,-33.83215956619923,-23.58113883008419,-23.0,-29.44983221287567,-23.58113883008419,-26.47213595499958,-31.74679434480896,-30.831760866327848,-29.158910531638178,-26.636809247747852,-27.70087712549569,-27.70087712549569,-29.87400787401181,-23.414213562373096,-23.73205080756888,-22.866025403784437,-25.1224989991992,-29.810249675906654,-37.8113883008419,-27.099019513592786,-29.193747284969078,-27.099019513592786,-29.193747284969078]],["params_yoff",[-25.0,-34.58039891549808,-12.416198487095663,-12.416198487095663,-9.69041575982343,-9.541475531146236,-5.5,-6.87082869338697,-6.414213562373095,-16.83215956619923,-6.58113883008419,-6.0,-12.44983221287567,-6.58113883008419,-9.47213595499958,-14.746794344808963,-13.831760866327848,-12.158910531638178,-9.636809247747852,-10.70087712549569,-10.70087712549569,-12.874007874011811,-6.414213562373095,-6.732050807568877,-5.866025403784438,-8.122498999199198,-12.810249675906654,-20.811388300841898,-10.099019513592784,-12.19374728496908,-10.099019513592784,-12.19374728496908]]]}}},"view":{"id":"p252295"},"glyph":{"type":"object","name":"Text","id":"p252363","attributes":{"x":{"type":"field","field":"Year"},"y":{"type":"field","field":"SPIDEr"},"text":{"type":"field","field":"params_display"},"x_offset":{"type":"field","field":"params_xoff"},"y_offset":{"type":"field","field":"params_yoff"},"text_color":{"type":"value","value":"#ff7270"},"text_font_size":{"type":"value","value":"18px"},"text_font_style":{"type":"value","value":"bold"},"text_align":{"type":"value","value":"center"},"text_baseline":{"type":"value","value":"middle"}}},"nonselection_glyph":{"type":"object","name":"Text","id":"p252364","attributes":{"x":{"type":"field","field":"Year"},"y":{"type":"field","field":"SPIDEr"},"text":{"type":"field","field":"params_display"},"x_offset":{"type":"field","field":"params_xoff"},"y_offset":{"type":"field","field":"params_yoff"},"text_color":{"type":"value","value":"#ff7270"},"text_alpha":{"type":"value","value":0.1},"text_font_size":{"type":"value","value":"18px"},"text_font_style":{"type":"value","value":"bold"},"text_align":{"type":"value","value":"center"},"text_baseline":{"type":"value","value":"middle"}}},"muted_glyph":{"type":"object","name":"Text","id":"p252365","attributes":{"x":{"type":"field","field":"Year"},"y":{"type":"field","field":"SPIDEr"},"text":{"type":"field","field":"params_display"},"x_offset":{"type":"field","field":"params_xoff"},"y_offset":{"type":"field","field":"params_yoff"},"text_color":{"type":"value","value":"#ff7270"},"text_alpha":{"type":"value","value":0.2},"text_font_size":{"type":"value","value":"18px"},"text_font_style":{"type":"value","value":"bold"},"text_align":{"type":"value","value":"center"},"text_baseline":{"type":"value","value":"middle"}}}}},{"type":"object","name":"GlyphRenderer","id":"p252379","attributes":{"visible":false,"data_source":{"type":"object","name":"ColumnDataSource","id":"p252373","attributes":{"selected":{"type":"object","name":"Selection","id":"p252374","attributes":{"indices":[],"line_indices":[]}},"selection_policy":{"type":"object","name":"UnionRenderers","id":"p252375"},"data":{"type":"map","entries":[["x",[0]],["y",[0]]]}}},"view":{"type":"object","name":"CDSView","id":"p252380","attributes":{"filter":{"type":"object","name":"AllIndices","id":"p252381"}}},"glyph":{"type":"object","name":"Scatter","id":"p252376","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"fill_color":{"type":"value","value":"#ff7270"},"hatch_color":{"type":"value","value":"#ff7270"}}},"nonselection_glyph":{"type":"object","name":"Scatter","id":"p252377","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"line_alpha":{"type":"value","value":0.1},"fill_color":{"type":"value","value":"#ff7270"},"fill_alpha":{"type":"value","value":0.1},"hatch_color":{"type":"value","value":"#ff7270"},"hatch_alpha":{"type":"value","value":0.1}}},"muted_glyph":{"type":"object","name":"Scatter","id":"p252378","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"line_alpha":{"type":"value","value":0.2},"fill_color":{"type":"value","value":"#ff7270"},"fill_alpha":{"type":"value","value":0.2},"hatch_color":{"type":"value","value":"#ff7270"},"hatch_alpha":{"type":"value","value":0.2}}}}},{"type":"object","name":"GlyphRenderer","id":"p252389","attributes":{"visible":false,"data_source":{"type":"object","name":"ColumnDataSource","id":"p252383","attributes":{"selected":{"type":"object","name":"Selection","id":"p252384","attributes":{"indices":[],"line_indices":[]}},"selection_policy":{"type":"object","name":"UnionRenderers","id":"p252385"},"data":{"type":"map","entries":[["x",[0]],["y",[0]]]}}},"view":{"type":"object","name":"CDSView","id":"p252390","attributes":{"filter":{"type":"object","name":"AllIndices","id":"p252391"}}},"glyph":{"type":"object","name":"Scatter","id":"p252386","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"fill_color":{"type":"value","value":"#70b1ff"},"hatch_color":{"type":"value","value":"#70b1ff"}}},"nonselection_glyph":{"type":"object","name":"Scatter","id":"p252387","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"line_alpha":{"type":"value","value":0.1},"fill_color":{"type":"value","value":"#70b1ff"},"fill_alpha":{"type":"value","value":0.1},"hatch_color":{"type":"value","value":"#70b1ff"},"hatch_alpha":{"type":"value","value":0.1}}},"muted_glyph":{"type":"object","name":"Scatter","id":"p252388","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"line_alpha":{"type":"value","value":0.2},"fill_color":{"type":"value","value":"#70b1ff"},"fill_alpha":{"type":"value","value":0.2},"hatch_color":{"type":"value","value":"#70b1ff"},"hatch_alpha":{"type":"value","value":0.2}}}}},{"type":"object","name":"GlyphRenderer","id":"p252400","attributes":{"visible":false,"data_source":{"type":"object","name":"ColumnDataSource","id":"p252394","attributes":{"selected":{"type":"object","name":"Selection","id":"p252395","attributes":{"indices":[],"line_indices":[]}},"selection_policy":{"type":"object","name":"UnionRenderers","id":"p252396"},"data":{"type":"map","entries":[["x",[0]],["y",[0]]]}}},"view":{"type":"object","name":"CDSView","id":"p252401","attributes":{"filter":{"type":"object","name":"AllIndices","id":"p252402"}}},"glyph":{"type":"object","name":"Scatter","id":"p252397","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"fill_color":{"type":"value","value":"grey"},"hatch_color":{"type":"value","value":"grey"}}},"nonselection_glyph":{"type":"object","name":"Scatter","id":"p252398","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"line_alpha":{"type":"value","value":0.1},"fill_color":{"type":"value","value":"grey"},"fill_alpha":{"type":"value","value":0.1},"hatch_color":{"type":"value","value":"grey"},"hatch_alpha":{"type":"value","value":0.1}}},"muted_glyph":{"type":"object","name":"Scatter","id":"p252399","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"line_alpha":{"type":"value","value":0.2},"fill_color":{"type":"value","value":"grey"},"fill_alpha":{"type":"value","value":0.2},"hatch_color":{"type":"value","value":"grey"},"hatch_alpha":{"type":"value","value":0.2}}}}},{"type":"object","name":"GlyphRenderer","id":"p252410","attributes":{"visible":false,"data_source":{"type":"object","name":"ColumnDataSource","id":"p252404","attributes":{"selected":{"type":"object","name":"Selection","id":"p252405","attributes":{"indices":[],"line_indices":[]}},"selection_policy":{"type":"object","name":"UnionRenderers","id":"p252406"},"data":{"type":"map","entries":[["x",[0]],["y",[0]]]}}},"view":{"type":"object","name":"CDSView","id":"p252411","attributes":{"filter":{"type":"object","name":"AllIndices","id":"p252412"}}},"glyph":{"type":"object","name":"Scatter","id":"p252407","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"fill_color":{"type":"value","value":"grey"},"hatch_color":{"type":"value","value":"grey"},"marker":{"type":"value","value":"diamond"}}},"nonselection_glyph":{"type":"object","name":"Scatter","id":"p252408","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"line_alpha":{"type":"value","value":0.1},"fill_color":{"type":"value","value":"grey"},"fill_alpha":{"type":"value","value":0.1},"hatch_color":{"type":"value","value":"grey"},"hatch_alpha":{"type":"value","value":0.1},"marker":{"type":"value","value":"diamond"}}},"muted_glyph":{"type":"object","name":"Scatter","id":"p252409","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"line_alpha":{"type":"value","value":0.2},"fill_color":{"type":"value","value":"grey"},"fill_alpha":{"type":"value","value":0.2},"hatch_color":{"type":"value","value":"grey"},"hatch_alpha":{"type":"value","value":0.2},"marker":{"type":"value","value":"diamond"}}}}},{"type":"object","name":"GlyphRenderer","id":"p252420","attributes":{"visible":false,"data_source":{"type":"object","name":"ColumnDataSource","id":"p252414","attributes":{"selected":{"type":"object","name":"Selection","id":"p252415","attributes":{"indices":[],"line_indices":[]}},"selection_policy":{"type":"object","name":"UnionRenderers","id":"p252416"},"data":{"type":"map","entries":[["x",[0]],["y",[0]]]}}},"view":{"type":"object","name":"CDSView","id":"p252421","attributes":{"filter":{"type":"object","name":"AllIndices","id":"p252422"}}},"glyph":{"type":"object","name":"Scatter","id":"p252417","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"fill_color":{"type":"value","value":"grey"},"hatch_color":{"type":"value","value":"grey"},"marker":{"type":"value","value":"triangle"}}},"nonselection_glyph":{"type":"object","name":"Scatter","id":"p252418","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"line_alpha":{"type":"value","value":0.1},"fill_color":{"type":"value","value":"grey"},"fill_alpha":{"type":"value","value":0.1},"hatch_color":{"type":"value","value":"grey"},"hatch_alpha":{"type":"value","value":0.1},"marker":{"type":"value","value":"triangle"}}},"muted_glyph":{"type":"object","name":"Scatter","id":"p252419","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"line_alpha":{"type":"value","value":0.2},"fill_color":{"type":"value","value":"grey"},"fill_alpha":{"type":"value","value":0.2},"hatch_color":{"type":"value","value":"grey"},"hatch_alpha":{"type":"value","value":0.2},"marker":{"type":"value","value":"triangle"}}}}}],"toolbar":{"type":"object","name":"Toolbar","id":"p252244","attributes":{"tools":[{"type":"object","name":"PanTool","id":"p252269"},{"type":"object","name":"WheelZoomTool","id":"p252270"},{"type":"object","name":"BoxZoomTool","id":"p252271","attributes":{"overlay":{"type":"object","name":"BoxAnnotation","id":"p252272","attributes":{"syncable":false,"level":"overlay","visible":false,"left_units":"canvas","right_units":"canvas","bottom_units":"canvas","top_units":"canvas","line_color":"black","line_alpha":1.0,"line_width":2,"line_dash":[4,4],"fill_color":"lightgrey","fill_alpha":0.5}}}},{"type":"object","name":"SaveTool","id":"p252273"},{"type":"object","name":"ResetTool","id":"p252274"},{"type":"object","name":"HelpTool","id":"p252275"},{"type":"object","name":"HoverTool","id":"p243513","attributes":{"renderers":"auto","tooltips":[["Title","@{Paper name}"],["Nb params","@{Nb params}"],["SPIDEr","@{SPIDEr}"],["Train data","@{Train data}"],["Pretrained","@{Pretrained}"]]}}]}},"left":[{"type":"object","name":"LinearAxis","id":"p252262","attributes":{"ticker":{"type":"object","name":"BasicTicker","id":"p252265","attributes":{"mantissas":[1,2,5]}},"formatter":{"type":"object","name":"BasicTickFormatter","id":"p252264"},"axis_label":"SPIDEr","axis_label_text_font_size":"20px","major_label_policy":{"type":"object","name":"AllLabels","id":"p252263"},"major_label_text_font_size":"20px"}}],"below":[{"type":"object","name":"LinearAxis","id":"p252255","attributes":{"ticker":{"type":"object","name":"FixedTicker","id":"p252369","attributes":{"ticks":[2020,2021,2022,2023],"minor_ticks":[]}},"formatter":{"type":"object","name":"BasicTickFormatter","id":"p252257"},"axis_label":"Year","axis_label_text_font_size":"20px","major_label_policy":{"type":"object","name":"AllLabels","id":"p252256"},"major_label_text_font_size":"20px"}}],"center":[{"type":"object","name":"Grid","id":"p252261","attributes":{"visible":false,"axis":{"id":"p252255"}}},{"type":"object","name":"Grid","id":"p252268","attributes":{"visible":false,"dimension":1,"axis":{"id":"p252262"}}},{"type":"object","name":"Legend","id":"p252393","attributes":{"location":[285,115],"title":"External data","title_text_font_size":"14px","label_text_font_size":"14px","padding":5,"items":[{"type":"object","name":"LegendItem","id":"p252382","attributes":{"label":{"type":"value","value":"yes"},"renderers":[{"id":"p252379"}],"index":0}},{"type":"object","name":"LegendItem","id":"p252392","attributes":{"label":{"type":"value","value":"no"},"renderers":[{"id":"p252389"}],"index":2}}]}},{"type":"object","name":"Legend","id":"p252424","attributes":{"location":"bottom_right","title":"Pretrained","title_text_font_size":"14px","label_text_font_size":"14px","padding":5,"items":[{"type":"object","name":"LegendItem","id":"p252403","attributes":{"label":{"type":"value","value":"audio+text"},"renderers":[{"id":"p252400"}],"index":0}},{"type":"object","name":"LegendItem","id":"p252413","attributes":{"label":{"type":"value","value":"audio"},"renderers":[{"id":"p252410"}],"index":4}},{"type":"object","name":"LegendItem","id":"p252423","attributes":{"label":{"type":"value","value":"none"},"renderers":[{"id":"p252420"}],"index":5}}]}}],"background_fill_color":null,"border_fill_color":null,"output_backend":"svg"}}],"callbacks":{"type":"map"}}}
    </script>
    <script type="text/javascript">
    (function() {
        const fn = function() {
        Bokeh.safely(function() {
            (function(root) {
            function embed_document(root) {
            const docs_json = document.getElementById('p262737').textContent;
            const render_items = [{"docid":"7206771e-8b4a-412f-b975-efe307b53dbf","roots":{"p252238":"b430d07d-b027-46a9-8da8-b7db3b71398f"},"root_ids":["p252238"]}];
            root.Bokeh.embed.embed_items(docs_json, render_items);
            }
            if (root.Bokeh !== undefined) {
                embed_document(root);
            } else {
                let attempts = 0;
                const timer = setInterval(function(root) {
                if (root.Bokeh !== undefined) {
                    clearInterval(timer);
                    embed_document(root);
                } else {
                    attempts++;
                    if (attempts > 100) {
                    clearInterval(timer);
                    console.log("Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing");
                    }
                }
                }, 10, root)
            }
            })(window);
        });
        };
        if (document.readyState != "loading") fn();
        else document.addEventListener("DOMContentLoaded", fn);
    })();
    </script>

    </center>
  </body>
</html>