% SSL methods (EURASIP version)
@article{Cances2022,
	title        = {Comparison of semi-supervised deep learning algorithms for audio classification},
	author       = {Cances, L{\'e}o and Labb{\'e}, Etienne and Pellegrini, Thomas},
	year         = 2022,
	month        = {Sep},
	day          = 19,
	journal      = {EURASIP Journal on Audio, Speech, and Music Processing},
	volume       = 2022,
	number       = 1,
	pages        = 23,
	doi          = {10.1186/s13636-022-00255-6},
	issn         = {1687-4722},
	url          = {https://doi.org/10.1186/s13636-022-00255-6},
	abstract     = {In this article, we adapted five recent SSL methods to the task of audio classification. The first two methods, namely Deep Co-Training (DCT) and Mean Teacher (MT), involve two collaborative neural networks. The three other algorithms, called MixMatch (MM), ReMixMatch (RMM), and FixMatch (FM), are single-model methods that rely primarily on data augmentation strategies. Using the Wide-ResNet-28-2 architecture in all our experiments, 10{\%} of labeled data and the remaining 90{\%} as unlabeled data for training, we first compare the error rates of the five methods on three standard benchmark audio datasets: Environmental Sound Classification (ESC-10), UrbanSound8K (UBS8K), and Google Speech Commands (GSC). In all but one cases, MM, RMM, and FM outperformed MT and DCT significantly, MM and RMM being the best methods in most experiments. On UBS8K and GSC, MM achieved 18.02{\%} and 3.25{\%} error rate (ER), respectively, outperforming models trained with 100{\%} of the available labeled data, which reached 23.29{\%} and 4.94{\%}, respectively. RMM achieved the best results on ESC-10 (12.00{\%} ER), followed by FM which reached 13.33{\%}. Second, we explored adding the mixup augmentation, used in MM and RMM, to DCT, MT, and FM. In almost all cases, mixup brought consistent gains. For instance, on GSC, FM reached 4.44{\%} and 3.31{\%} ER without and with mixup. Our PyTorch code will be made available upon paper acceptance at https://github.com/Labbeti/SSLH.}
}
% DCASE2022 Workshop - paper 1/2 - SPIDEr-max
@inproceedings{Labbe2022,
	title        = {Is my Automatic Audio Captioning System so Bad? SPIDEr-max: A Metric to Consider Several Caption Candidates},
	author       = {Labb\'{e}, Etienne and Pellegrini, Thomas and Pinquier, Julien},
	year         = 2022,
	month        = {November},
	booktitle    = {Proceedings of the 7th Detection and Classification of Acoustic Scenes and Events 2022 Workshop (DCASE2022)},
	address      = {Nancy, France},
	url          = {https://dcase.community/documents/workshop2022/proceedings/DCASE2022Workshop_Labbe_46.pdf},
	abstract     = {Automatic Audio Captioning (AAC) is the task that aims to describe an audio signal using natural language. AAC systems take as input an audio signal and output a free-form text sentence, called a caption. Evaluating such systems is not trivial, since there are many ways to express the same idea. For this reason, several complementary metrics, such as BLEU, CIDEr, SPICE and SPIDEr, are used to compare a single automatic caption to one or several captions of reference, produced by a human annotator. Nevertheless, an automatic system can produce several caption candidates, either using some randomness in the sentence generation process, or by considering the various competing hypothesized captions during decoding with beam-search, for instance. If we consider an end-user of an AAC system, presenting several captions instead of a single one seems relevant to provide some diversity, similarly to information retrieval systems. In this work, we explore the possibility to consider several predicted captions in the evaluation process instead of one. For this purpose, we propose SPIDEr-max, a metric that takes the maximum SPIDEr value among the scores of several caption candidates. To advocate for our metric, we report experiments on Clotho v2.1 and AudioCaps, with a transformed-based system. On AudioCaps for example, this system reached a SPIDEr-max value (with 5 candidates) close to the SPIDEr human score of reference.}
}
% SER Loss (arxiv version)
@misc{labbe2023multitask,
	title        = {Multitask learning in Audio Captioning: a sentence embedding regression loss acts as a regularizer},
	author       = {Etienne Labbé and Julien Pinquier and Thomas Pellegrini},
	year         = 2023,
	url          = {https://arxiv.org/pdf/2305.01482.pdf},
	eprint       = {2305.01482},
	archiveprefix = {arXiv},
	primaryclass = {cs.SD}
}
% Interspeech CNext (arxiv version)
misc{pellegrini2023adapting,
	title        = {Adapting a ConvNeXt model to audio classification on AudioSet},
	author       = {Thomas Pellegrini and Ismail Khalfaoui-Hassani and Etienne Labbé and Timothée Masquelier},
	year         = 2023,
	eprint       = {2306.00830},
	archiveprefix = {arXiv},
	primaryclass = {cs.SD}
}
% Interspeech CNext (HAL version)
@inproceedings{pellegrini2023adapting,
	title        = {{Adapting a ConvNeXt model to audio classification on AudioSet}},
	author       = {Pellegrini, Thomas and Khalfaoui-Hassani, Ismail and Labb{\'e}, Etienne and Masquelier, Timoth{\'e}e},
	year         = 2023,
	month        = Aug,
	booktitle    = {{24th INTERSPEECH Conference (INTERSPEECH 2023)}},
	address      = {Dublin, Ireland},
	pages        = {{\`a} para{\^i}tre},
	url          = {https://ut3-toulouseinp.hal.science/hal-04114822},
	keywords     = {audio classification ; automatic audio captioning ; convolutional neural networks ; AudioSet ; ConvNeXt ; language based audio retrieval},
	hal_id       = {hal-04114822},
	hal_version  = {v1}
}
% DCASE my retrieval (arxiv version)
misc{labbé2023killing,
	title        = {Killing two birds with one stone: Can an audio captioning system also be used for audio-text retrieval?},
	author       = {Etienne Labbé and Thomas Pellegrini and Julien Pinquier},
	year         = 2023,
	eprint       = {2308.15090},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
% DCASE my retrieval (DCASE version)
@inproceedings{Labba2023,
	title        = {Killing Two Birds with One Stone: Can an Audio Captioning System Also Be Used for Audio-Text Retrieval?},
	author       = {Labbé, Étienne and Pellegrini, Thomas and Pinquier, Julien},
	year         = 2023,
	month        = {September},
	booktitle    = {Proceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)},
	address      = {Tampere, Finland},
	pages        = {96--100},
	url          = {https://dcase.community/documents/workshop2023/proceedings/DCASE2023Workshop_Labb%C3%A9_43.pdf},
	abstract     = {Automated Audio Captioning (AAC) aims to develop systems capable of describing an audio recording using a textual sentence. In contrast, Audio-Text Retrieval (ATR) systems seek to find the best matching audio recording(s) for a given textual query (Text-to-Audio) or vice versa (Audio-to-Text). These tasks require different types of systems: AAC employs a sequence-to-sequence model, while ATR utilizes a ranking model that compares audio and text representations within a shared projection subspace. However, this work investigates the relationship between AAC and ATR by exploring the ATR capabilities of an unmodified AAC system, without fine-tuning for the new task. Our AAC system consists of an audio encoder (ConvNeXt-Tiny) trained on AudioSet for audio tagging, and a transformer decoder responsible for generating sentences. For AAC, it achieves a high SPIDEr-FL score of 0.298 on Clotho and 0.472 on AudioCaps on average. For ATR, we propose using the standard Cross-Entropy loss values obtained for any audio/caption pair. Experimental results on the Clotho and AudioCaps datasets demonstrate decent recall values using this simple approach. For instance, we obtained a Text-to-Audio R@1 value of 0.382 for AudioCaps, which is above the current state-of-the-art method without external data. Interestingly, we observe that normalizing the loss values was necessary for Audio-to-Text retrieval.}
}
% Conette (arxiv version)
@article{labbé2023conette,
	title        = {CoNeTTE: An efficient Audio Captioning system leveraging multiple datasets with Task Embedding},
	author       = {Étienne Labbé and Thomas Pellegrini and Julien Pinquier},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2309.00454},
	url          = {https://arxiv.org/pdf/2309.00454.pdf},
	eprint       = {2309.00454},
	archiveprefix = {arXiv},
	primaryclass = {cs.SD}
}
% Multilingual AAC (arxiv version)
@article{cousin2023multilingual,
	title        = {Multilingual Audio Captioning using machine translated data},
	author       = {Cousin, Mat{\'e}o and Labb{\'e}, {\'E}tienne and Pellegrini, Thomas},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2309.07615},
	url          = {https://arxiv.org/pdf/2309.07615.pdf}
}
